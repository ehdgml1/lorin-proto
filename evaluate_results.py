#!/usr/bin/env python3
"""
LORIN Evaluation Script
=======================
Calculates Coverage and Reduction metrics by comparing
system-recommended line ranges with ground truth (GT).

Coverage = |R_hat âˆ© R*| / |R*| Ã— 100
Reduction = (1 - |R_hat| / |OriginalLog|) Ã— 100

Usage:
    poetry run python evaluate_results.py --results results/stage2_results.json
"""

import argparse
import json
import re
from pathlib import Path
from typing import List, Dict, Set, Tuple


def parse_gt_file(gt_path: Path) -> List[Tuple[int, int]]:
    """
    GT íŒŒì¼ì—ì„œ ë¼ì¸ ë²”ìœ„ íŒŒì‹±

    í˜•ì‹: line[start~end] ë˜ëŠ” line[start-end]
    """
    ranges = []

    if not gt_path.exists():
        return ranges

    with open(gt_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # line[a~b] ë˜ëŠ” line[a-b] íŒ¨í„´ ì°¾ê¸°
    pattern = r'line\s*\[\s*(\d+)\s*[~\-]\s*(\d+)\s*\]'
    matches = re.findall(pattern, content, re.IGNORECASE)

    for match in matches:
        start, end = int(match[0]), int(match[1])
        if start <= end:
            ranges.append((start, end))

    return ranges


def merge_ranges(ranges: List[Tuple[int, int]]) -> List[Tuple[int, int]]:
    """
    ê²¹ì¹˜ëŠ” ë²”ìœ„ ë³‘í•©

    ì˜ˆ: [(1, 100), (50, 150)] -> [(1, 150)]
    """
    if not ranges:
        return []

    # ì‹œì‘ì  ê¸°ì¤€ ì •ë ¬
    sorted_ranges = sorted(ranges, key=lambda x: x[0])
    merged = [sorted_ranges[0]]

    for current in sorted_ranges[1:]:
        last = merged[-1]

        # ê²¹ì¹˜ê±°ë‚˜ ì—°ì†ë˜ë©´ ë³‘í•©
        if current[0] <= last[1] + 1:
            merged[-1] = (last[0], max(last[1], current[1]))
        else:
            merged.append(current)

    return merged


def ranges_to_set(ranges: List[Tuple[int, int]]) -> Set[int]:
    """ë²”ìœ„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¼ì¸ ë²ˆí˜¸ ì§‘í•©ìœ¼ë¡œ ë³€í™˜"""
    line_set = set()
    for start, end in ranges:
        line_set.update(range(start, end + 1))
    return line_set


def count_lines_in_ranges(ranges: List[Tuple[int, int]]) -> int:
    """ë²”ìœ„ì˜ ì´ ë¼ì¸ ìˆ˜ ê³„ì‚° (ê²¹ì¹˜ëŠ” ë¶€ë¶„ ì œì™¸)"""
    merged = merge_ranges(ranges)
    total = 0
    for start, end in merged:
        total += end - start + 1
    return total


def calculate_metrics(
    recommended_ranges: List[Dict],
    gt_ranges: List[Tuple[int, int]],
    original_log_lines: int
) -> Dict:
    """
    Coverageì™€ Reduction ê³„ì‚°

    Args:
        recommended_ranges: ì‹œìŠ¤í…œ ì¶”ì²œ ë²”ìœ„ [{"start": a, "end": b}, ...]
        gt_ranges: GT ë²”ìœ„ [(start, end), ...]
        original_log_lines: ì›ë³¸ ë¡œê·¸ ë¼ì¸ ìˆ˜

    Returns:
        {"coverage": float, "reduction": float, ...}
    """
    # ì¶”ì²œ ë²”ìœ„ë¥¼ íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    rec_tuples = [(r["start"], r["end"]) for r in recommended_ranges]

    # ê²¹ì¹˜ëŠ” ë¶€ë¶„ ë³‘í•©
    rec_merged = merge_ranges(rec_tuples)
    gt_merged = merge_ranges(gt_ranges)

    # ë¼ì¸ ì§‘í•©ìœ¼ë¡œ ë³€í™˜
    rec_set = ranges_to_set(rec_merged)
    gt_set = ranges_to_set(gt_merged)

    # êµì§‘í•© (ê²¹ì¹˜ëŠ” ë¼ì¸)
    overlap_set = rec_set & gt_set

    # ê° ì§‘í•© í¬ê¸°
    rec_count = len(rec_set)
    gt_count = len(gt_set)
    overlap_count = len(overlap_set)

    # Coverage = |R_hat âˆ© R*| / |R*| Ã— 100
    coverage = (overlap_count / gt_count * 100) if gt_count > 0 else 0.0

    # Reduction = (1 - |R_hat| / |OriginalLog|) Ã— 100
    reduction = (1 - rec_count / original_log_lines) * 100 if original_log_lines > 0 else 0.0

    return {
        "coverage": round(coverage, 2),
        "reduction": round(reduction, 2),
        "recommended_lines": rec_count,
        "gt_lines": gt_count,
        "overlap_lines": overlap_count,
        "original_lines": original_log_lines,
        "recommended_ranges_merged": rec_merged,
        "gt_ranges_merged": gt_merged
    }


def get_csv_line_count(csv_dir: Path, case_id: int) -> int:
    """CSV íŒŒì¼ì˜ ë¼ì¸ ìˆ˜ (í—¤ë” ì œì™¸)"""
    csv_path = csv_dir / f"case_{case_id}.csv"
    if not csv_path.exists():
        return 0

    with open(csv_path, 'r', encoding='utf-8') as f:
        # í—¤ë” ì œì™¸
        return sum(1 for _ in f) - 1


def evaluate_all_cases(
    results_path: Path,
    gt_dir: Path,
    csv_dir: Path
) -> Dict:
    """ëª¨ë“  ì¼€ì´ìŠ¤ í‰ê°€"""

    # Stage2 ê²°ê³¼ ë¡œë“œ
    with open(results_path, 'r', encoding='utf-8') as f:
        results = json.load(f)

    evaluation = {
        "summary": {
            "total_cases": 0,
            "evaluated_cases": 0,
            "avg_coverage": 0.0,
            "avg_reduction": 0.0
        },
        "cases": {}
    }

    coverage_sum = 0.0
    reduction_sum = 0.0
    evaluated_count = 0

    for case_key, case_data in results.get("cases", {}).items():
        case_id = case_data.get("case_id")
        if case_id is None:
            continue

        evaluation["summary"]["total_cases"] += 1

        # GT íŒŒì¼ ë¡œë“œ
        gt_path = gt_dir / f"case_{case_id}gt.txt"
        gt_ranges = parse_gt_file(gt_path)

        if not gt_ranges:
            evaluation["cases"][case_key] = {
                "case_id": case_id,
                "error": "GT file not found or empty",
                "coverage": None,
                "reduction": None
            }
            continue

        # ì¶”ì²œ ë²”ìœ„
        recommended_ranges = case_data.get("recommended_ranges", [])

        if not recommended_ranges:
            evaluation["cases"][case_key] = {
                "case_id": case_id,
                "error": "No recommended ranges",
                "coverage": 0.0,
                "reduction": 100.0,
                "gt_lines": count_lines_in_ranges(gt_ranges)
            }
            continue

        # ì›ë³¸ ë¡œê·¸ ë¼ì¸ ìˆ˜
        original_lines = get_csv_line_count(csv_dir, case_id)

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = calculate_metrics(recommended_ranges, gt_ranges, original_lines)

        evaluation["cases"][case_key] = {
            "case_id": case_id,
            "coverage": metrics["coverage"],
            "reduction": metrics["reduction"],
            "recommended_lines": metrics["recommended_lines"],
            "gt_lines": metrics["gt_lines"],
            "overlap_lines": metrics["overlap_lines"],
            "original_lines": metrics["original_lines"],
            "success": case_data.get("success", False)
        }

        coverage_sum += metrics["coverage"]
        reduction_sum += metrics["reduction"]
        evaluated_count += 1

    # í‰ê·  ê³„ì‚°
    if evaluated_count > 0:
        evaluation["summary"]["evaluated_cases"] = evaluated_count
        evaluation["summary"]["avg_coverage"] = round(coverage_sum / evaluated_count, 2)
        evaluation["summary"]["avg_reduction"] = round(reduction_sum / evaluated_count, 2)

    return evaluation


def print_evaluation_table(evaluation: Dict):
    """í‰ê°€ ê²°ê³¼ í…Œì´ë¸” ì¶œë ¥"""
    print("\n" + "=" * 100)
    print("EVALUATION RESULTS")
    print("=" * 100)
    print(f"{'Case':<10} {'Coverage(%)':<12} {'Reduction(%)':<12} {'Overlap':<10} {'GT Lines':<10} {'Rec Lines':<10} {'Original':<10}")
    print("-" * 100)

    for case_key in sorted(evaluation["cases"].keys(), key=lambda x: int(x.split('_')[1])):
        case = evaluation["cases"][case_key]
        case_id = f"case_{case['case_id']}"

        if case.get("error"):
            print(f"{case_id:<10} {'N/A':<12} {'N/A':<12} {'-':<10} {'-':<10} {'-':<10} {case.get('error', '')}")
        else:
            coverage = f"{case['coverage']:.1f}"
            reduction = f"{case['reduction']:.1f}"
            overlap = str(case.get('overlap_lines', '-'))
            gt_lines = str(case.get('gt_lines', '-'))
            rec_lines = str(case.get('recommended_lines', '-'))
            original = str(case.get('original_lines', '-'))

            print(f"{case_id:<10} {coverage:<12} {reduction:<12} {overlap:<10} {gt_lines:<10} {rec_lines:<10} {original:<10}")

    print("-" * 100)
    summary = evaluation["summary"]
    print(f"{'Average':<10} {summary['avg_coverage']:<12.1f} {summary['avg_reduction']:<12.1f}")
    print("=" * 100)
    print(f"\nEvaluated: {summary['evaluated_cases']} / {summary['total_cases']} cases")


def main():
    parser = argparse.ArgumentParser(description="Evaluate LORIN Stage2 results")
    parser.add_argument(
        "--results",
        type=Path,
        default=Path("/home/bigdata/1113/lorin-proto/results/stage2_results.json"),
        help="Path to stage2_results.json"
    )
    parser.add_argument(
        "--gt-dir",
        type=Path,
        default=Path("/home/bigdata/1113/lorin-proto/data/gt"),
        help="Directory containing GT files"
    )
    parser.add_argument(
        "--csv-dir",
        type=Path,
        default=Path("/home/bigdata/1113/lorin-proto/data/logs"),
        help="Directory containing case CSV files"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("/home/bigdata/1113/lorin-proto/results/evaluation_results.json"),
        help="Output JSON file path"
    )

    args = parser.parse_args()

    if not args.results.exists():
        print(f"âŒ Results file not found: {args.results}")
        print("   Run stage2 first: poetry run python run_stage2.py")
        return

    print("ğŸ” LORIN Evaluation")
    print("=" * 60)
    print(f"Results: {args.results}")
    print(f"GT dir: {args.gt_dir}")
    print(f"CSV dir: {args.csv_dir}")

    # í‰ê°€ ì‹¤í–‰
    evaluation = evaluate_all_cases(
        results_path=args.results,
        gt_dir=args.gt_dir,
        csv_dir=args.csv_dir
    )

    # ê²°ê³¼ ì €ì¥
    args.output.parent.mkdir(parents=True, exist_ok=True)
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(evaluation, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“„ Evaluation saved to: {args.output}")

    # í…Œì´ë¸” ì¶œë ¥
    print_evaluation_table(evaluation)


if __name__ == "__main__":
    main()
