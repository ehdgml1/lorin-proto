from __future__ import annotations
import asyncio
import os
import re
from typing import Any, Dict, List, Optional, Tuple

from langchain_core.messages import AIMessage, BaseMessage, HumanMessage

from ...llm import Chatbot
from ...logger.logger import get_logger
from ..state import AgentState
from ..schema import (
    MetadataManager,
    LoopPreventionStatus,
    create_answer_metadata,
    get_search_results,
    get_quality_filtered_search_results
)
from ...prompt.template_loader import PromptTemplateLoader

logger = get_logger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê³µí†µ ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _after_thought(output: str) -> str:
    """ ì‘ë‹µ ë¬¸ìì—´ì—ì„œ ë§ˆì§€ë§‰ </thought> ì´í›„ ë¶€ë¶„ë§Œ ë°˜í™˜.
    - íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì›ë¬¸ strip ë°˜í™˜
    - ëŒ€ì†Œë¬¸ì/ê°œí–‰/ê³µë°± í—ˆìš©: </thought>, </Thought > ë“±
    """
    if not output:
        return ""
    m = list(re.finditer(r"(?is)</\s*thought\s*>", output))
    if not m:
        return output.strip()
    end = m[-1].end()
    return output[end:].strip()


def _get_last_user_utterance(messages: List[BaseMessage]) -> Optional[str]:
    """ ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë§ˆì§€ë§‰ ì‚¬ìš©ì ë°œí™”ë¥¼ ì°¾ì•„ ë°˜í™˜
    (HumanMessage, type='human', role='user' ìˆœìœ¼ë¡œ íƒìƒ‰)
    """
    if not messages:
        return None
    for m in reversed(messages):
        if isinstance(m, HumanMessage) or getattr(m, "type", "") == "human" or getattr(m, "role", "") == "user":
            content = getattr(m, "content", None)
            if content:
                return str(content)
    return str(getattr(messages[-1], "content", "") or "")


def _kv(meta: Dict[str, Any], *keys: str, default=None):
    """ë©”íƒ€ ì‚¬ì „ì—ì„œ ëŒ€ì†Œë¬¸ì ë¬´ì‹œ í‚¤ ì¡°íšŒ."""
    if not meta:
        return default
    lowers = {k.lower(): k for k in meta.keys()}
    for k in keys:
        lk = k.lower()
        if lk in lowers:
            return meta[lowers[lk]]
    return default


def _basename(p: str) -> str:
    try:
        import os as _os
        return _os.path.basename(p) or p
    except Exception:
        return str(p)


def _make_label_or_none(meta: Dict[str, Any]) -> Optional[str]:
    """ âš ï¸ doc1/doc2 ê°™ì€ ê°€ì§œ ë¼ë²¨ì€ ìƒì„±í•˜ì§€ ì•ŠìŒ.
    ë¼ë²¨ í›„ë³´ê°€ ì—†ìœ¼ë©´ Noneì„ ë°˜í™˜í•˜ì—¬ ì¶œë ¥ì—ì„œ ë¼ë²¨ì„ ìƒëµí•œë‹¤.
    ìš°ì„ ìˆœìœ„: url â†’ path/file â†’ source â†’ title
    """
    url = _kv(meta, "url", "URI", "href")
    if url:
        return str(url)
    path = _kv(meta, "path", "filepath", "file_path", "fullpath")
    file_ = _kv(meta, "file", "filename", "name")
    if path:
        return _basename(str(path))
    if file_:
        return _basename(str(file_))
    source = _kv(meta, "source")
    if source:
        return str(source)
    title = _kv(meta, "title")
    if title:
        return str(title)
    return None  # â¬…ï¸ ê°€ì§œ ë¼ë²¨ ê¸ˆì§€


def _parse_int(v) -> Optional[int]:
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        return None


# ============================================
# ë£¨í”„ ë°©ì§€ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ============================================

def _has_partial_evaluation_results(state: AgentState) -> bool:
    """ë¶€ë¶„ í‰ê°€ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸

    Args:
        state: AgentState ê°ì²´

    Returns:
        bool: ë¶€ë¶„ì ìœ¼ë¡œ ì„±ê³µí•œ ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´ True
    """
    evaluator_data = MetadataManager.safe_get_metadata(state, "quality_evaluator")
    if not evaluator_data:
        return False

    cumulative_results = evaluator_data.get("cumulative_evaluation_results", {})
    if not cumulative_results:
        return False

    successful_count = sum(
        1 for result in cumulative_results.values()
        if isinstance(result, dict) and result.get("is_relevant", False)
    )

    return successful_count > 0


def _check_loop_prevention_status(state: AgentState) -> LoopPreventionStatus:
    """ë£¨í”„ ë°©ì§€ ìƒíƒœ í™•ì¸

    MetadataManagerë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ë£¨í”„ ë°©ì§€ ê´€ë ¨ ë©”íƒ€ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    Args:
        state: AgentState ê°ì²´

    Returns:
        LoopPreventionStatus: ë£¨í”„ ë°©ì§€ ìƒíƒœ ì •ë³´ (TypedDict)
    """
    loop_data = MetadataManager.safe_get_metadata(state, "loop_prevention") or {}
    last_decision = MetadataManager.safe_get_metadata(state, "last_routing_decision") or {}

    # ê°•ì œ ì¢…ë£Œ ì¡°ê±´ë“¤ í™•ì¸
    global_iterations = loop_data.get("global_iteration_count", 0)
    forced_terminations = loop_data.get("forced_terminations", 0)

    # ì¢…ë£Œ ì—¬ë¶€ íŒë‹¨
    terminated = False
    reason = ""

    if global_iterations >= 25:
        terminated = True
        reason = "Maximum global iterations (25) reached"
    elif forced_terminations > 0:
        terminated = True
        reason = last_decision.get("reason", "Loop prevention triggered")
    elif last_decision.get("strategy") in [
        "global_limit_prevention",
        "oscillation_prevention",
        "conservative_termination"
    ]:
        terminated = True
        reason = last_decision.get("reason", "Loop prevention strategy activated")

    return LoopPreventionStatus(
        terminated=terminated,
        reason=reason,
        global_iterations=global_iterations,
        forced_terminations=forced_terminations,
        routing_history=loop_data.get("routing_history", [])[-5:],
        strategy=last_decision.get("strategy", ""),
        has_partial_results=_has_partial_evaluation_results(state)
    )

def _create_loop_termination_message(
    state: AgentState,
    reason: str,
    loop_status: Dict[str, Any]
) -> str:
    """ë£¨í”„ ì¢…ë£Œ ì‹œ ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„±"""
    from .quality_evaluator_node import create_graceful_termination_message

    return create_graceful_termination_message(state, reason, loop_status)

def _build_termination_response(
    termination_message: str,
    qids: List[str],
    subanswers_by_qid: Dict[str, Dict[str, Any]],
    loop_status: Dict[str, Any]
) -> str:
    """ì¢…ë£Œ ìƒí™©ì—ì„œì˜ ìµœì¢… ì‘ë‹µ êµ¬ì„±"""
    response_parts = [termination_message]

    # ë¶€ë¶„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í¬í•¨
    if loop_status.get("has_partial_results", False) and subanswers_by_qid:
        response_parts.append("\n## ğŸ“ **Available Partial Results**\n")

        # ì„œë¸Œì¿¼ë¦¬ ì‘ë‹µì´ ìˆìœ¼ë©´ ê°„ëµí•˜ê²Œ í¬í•¨
        for qid in qids:
            subans = subanswers_by_qid.get(qid)
            if subans and subans.get("output"):
                query_text = subans.get("query", f"Query {qid}")
                output = subans.get("output", "")

                # ê¸´ ì‘ë‹µì€ ìš”ì•½
                if len(output) > 300:
                    output = output[:300] + "..."

                response_parts.append(f"**{query_text}**: {output}\n")

    return "\n".join(response_parts)


def _extract_line_range(meta: Dict[str, Any]) -> Tuple[Optional[int], Optional[int]]:
    """ ë©”íƒ€ì—ì„œ ë¼ì¸ ë²”ìœ„ë¥¼ ì¶”ì¶œ.
    - ì˜¤ì§ "Range"/"range" í‚¤ë§Œ ì§€ì›
    - ë¦¬ìŠ¤íŠ¸/íŠœí”Œ: [start, end]
    - ë¬¸ìì—´: "[123,456]", "123-456", "123~456", "123 456"
    - ì‹¤íŒ¨ ì‹œ (None, None)
    """
    rng = _kv(meta, "Range", "range")
    if rng is None:
        return None, None
    if isinstance(rng, (list, tuple)) and len(rng) >= 2:
        ls = _parse_int(rng[0]); le = _parse_int(rng[1])
        return ls, le
    if isinstance(rng, str):
        s = rng.strip()
        s = re.sub(r"^[\[\(\{]\s*|\s*[\]\)\}]$", "", s)  # ì–‘ ë ê´„í˜¸ ì œê±°
        m = re.match(r"\s*(\d+)\s*[,~\-]\s*(\d+)\s*$", s)  # 123-456, 123~456, 123,456
        if m:
            return _parse_int(m.group(1)), _parse_int(m.group(2))
        m = re.match(r"\s*(\d+)\s+(\d+)\s*$", s)  # 123 456
        if m:
            return _parse_int(m.group(1)), _parse_int(m.group(2))
    return None, None


def _format_meta_all(meta: Dict[str, Any], extra: Dict[str, Any] | None = None) -> str:
    """ë””ë²„ê¹…/ì¶”ì ìš© ë©”íƒ€ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í•œ ì¤„ ë¬¸ìì—´ë¡œ."""
    merged = dict(meta or {})
    if extra:
        for k, v in extra.items():
            if v is not None and k not in merged:
                merged[k] = v

    priority = [
        "url", "path", "file", "source", "title", "Range", "range",
        "line_start", "line_end", "score", "rerank_score"
    ]

    def sort_key(item):
        k = item[0]
        try:
            idx = priority.index(k)
        except ValueError:
            idx = 999
        return (idx, k)

    items = sorted(merged.items(), key=sort_key)
    return " | ".join(f"{k}={v}" for k, v in items)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¦ê±° ìˆ˜ì§‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _qid_sort_key(qid: str) -> Tuple[int, str]:
    """QID ì •ë ¬ í‚¤. 'Q1','Q2' ìˆ«ìë¥¼ ìš°ì„  ì •ë ¬, ê·¸ ì™¸ëŠ” ì‚¬ì „ìˆœ."""
    m = re.match(r"[Qq](\d+)", qid or "")
    if m:
        return (int(m.group(1)), qid)
    return (10**9, qid or "")


def _collect_evidence(state: AgentState, results_by_qid: Dict[str, List[Dict[str, Any]]]) -> Tuple[str, Dict[str, Any]]:
    """ëª¨ë“  QID ê²°ê³¼ë¥¼ í•œë° ëª¨ì•„ evidence_block êµ¬ì„± (í’ˆì§ˆ í•„í„°ë§ ì ìš©).

    Args:
        state: AgentState ê°ì²´
        results_by_qid: QIDë³„ ê²€ìƒ‰ ê²°ê³¼

    Returns:
        Tuple[str, Dict[str, Any]]: (evidence_block ë¬¸ìì—´, label_map)
    """
    logger.info(f"[_collect_evidence] Using quality-filtered results: {len(results_by_qid)} QIDs")

    # MetadataManagerë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ ë§¤í•‘ ì¡°íšŒ
    retr_data = MetadataManager.safe_get_metadata(state, "faiss_retriever")
    queries_by_qid: Dict[str, str] = retr_data.get("queries_by_qid", {}) if retr_data else {}
    logger.info(f"[_collect_evidence] queries_by_qid: {len(queries_by_qid)} queries")
    lines: List[str] = []
    label_map: Dict[str, Any] = {}
    anon_counter = 0

    qids = sorted(results_by_qid.keys(), key=_qid_sort_key)
    logger.info(f"[_collect_evidence] Processing {len(qids)} QIDs")

    for qid in qids:
        used_query = (queries_by_qid or {}).get(qid, "")
        docs = results_by_qid.get(qid) or []
        logger.info(f"[_collect_evidence] QID {qid}: {len(docs)} docs")

        if not docs:
            continue

        # âœ¨ Sort documents by line_start for chronological interpretation
        def get_line_start(doc):
            meta = doc.get("metadata") or {}
            ls, _ = _extract_line_range(meta)
            return ls if ls is not None else float('inf')

        sorted_docs = sorted(docs, key=get_line_start)
        logger.info(f"[_collect_evidence] QID {qid}: Sorted {len(sorted_docs)} docs by line position")

        for i, d in enumerate(sorted_docs):
            logger.info(f"[_collect_evidence] QID {qid}, doc {i}: type={type(d)}")
            try:
                meta = d.get("metadata") or {}
                logger.info(f"[_collect_evidence] QID {qid}, doc {i}: metadata success")
            except Exception as e:
                logger.error(f"[_collect_evidence] QID {qid}, doc {i}: metadata error: {e}")
                raise
            score = d.get("score")
            rscore = d.get("rerank_score")
            ls, le = _extract_line_range(meta)
            label = _make_label_or_none(meta)
            if label is None:
                anon_counter += 1
                key = f"anon_{anon_counter}"
            else:
                key = f"label_{hash(label)}"

            header_parts = []
            header_parts.append(f"### {label}" if label else "### (ì†ŒìŠ¤ ë¼ë²¨ ì—†ìŒ)")
            if ls is not None or le is not None:
                header_parts.append(f"(lines {ls if ls is not None else '?'}~{le if le is not None else '?'})")
            if used_query:
                header_parts.append(f"input_query: {used_query}")
            lines.append(" ".join(header_parts))

            meta_str = _format_meta_all(meta, {"score": score, "rerank_score": rscore})
            if meta_str:
                lines.append(f"META: {meta_str}")

            # ğŸ”§ FIX: SearchResult uses "content" field, not "text"
            txt = str(d.get("content", "") or "").strip()
            if txt:
                lines.append(txt)
            lines.append("")

            label_map[key] = {
                "label": label,
                "meta": meta,
                "score": score,
                "rerank_score": rscore,
                "line_start": ls,
                "line_end": le,
                "input_query": used_query or None,
                "input_querry": used_query or None,
            }

    context = "\n".join(lines).strip()
    return context, label_map


def _collect_evidence_for_qid(state: AgentState, qid: str, results_by_qid: Dict[str, List[Dict[str, Any]]]) -> Tuple[str, Dict[str, Any]]:
    """íŠ¹ì • QIDì˜ ê²°ê³¼ë§Œ evidence_blockìœ¼ë¡œ êµ¬ì„± (í’ˆì§ˆ í•„í„°ë§ ì ìš©).

    Args:
        state: AgentState ê°ì²´
        qid: ëŒ€ìƒ ì¿¼ë¦¬ ID
        results_by_qid: QIDë³„ ê²€ìƒ‰ ê²°ê³¼

    Returns:
        Tuple[str, Dict[str, Any]]: (evidence_block ë¬¸ìì—´, label_map)
    """
    # MetadataManagerë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ ë§¤í•‘ ì¡°íšŒ
    retr_data = MetadataManager.safe_get_metadata(state, "faiss_retriever")
    queries_by_qid: Dict[str, str] = retr_data.get("queries_by_qid", {}) if retr_data else {}

    docs = results_by_qid.get(qid) or []
    used_query = (queries_by_qid or {}).get(qid, "")
    lines: List[str] = []
    label_map: Dict[str, Any] = {}
    anon_counter = 0

    if not docs:
        return "", {}

    # âœ¨ Sort documents by line_start for chronological interpretation
    def get_line_start(doc):
        meta = doc.get("metadata") or {}
        ls, _ = _extract_line_range(meta)
        return ls if ls is not None else float('inf')

    sorted_docs = sorted(docs, key=get_line_start)

    for d in sorted_docs:
        meta = d.get("metadata") or {}
        score = d.get("score")
        rscore = d.get("rerank_score")
        ls, le = _extract_line_range(meta)
        label = _make_label_or_none(meta)
        if label is None:
            anon_counter += 1
            key = f"anon_{anon_counter}"
        else:
            key = f"label_{hash(label)}"

        header_parts = []
        header_parts.append(f"### {label}" if label else "### (ì†ŒìŠ¤ ë¼ë²¨ ì—†ìŒ)")
        if ls is not None or le is not None:
            header_parts.append(f"(lines {ls if ls is not None else '?'}~{le if le is not None else '?'})")
        if used_query:
            header_parts.append(f"input_query: {used_query}")
        lines.append(" ".join(header_parts))

        meta_str = _format_meta_all(meta, {"score": score, "rerank_score": rscore})
        if meta_str:
            lines.append(f"META: {meta_str}")

        # ğŸ”§ FIX: SearchResult uses "content" field, not "text"
        txt = str(d.get("content", "") or "").strip()
        if txt:
            lines.append(txt)
        lines.append("")

        label_map[key] = {
            "label": label,
            "meta": meta,
            "score": score,
            "rerank_score": rscore,
            "line_start": ls,
            "line_end": le,
            "input_query": used_query or None,
            "input_querry": used_query or None,
        }

    context = "\n".join(lines).strip()
    return context, label_map


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í”„ë¡¬í”„íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _build_sub_prompt(user_or_subquery: str, evidence_block: str, intent: str = "debug", query_type: str = "cause") -> str:
    """ ì„œë¸Œì¿¼ë¦¬ìš© í”„ë¡¬í”„íŠ¸ (Jinja2 template ì‚¬ìš© with intent-based template selection).
    - evidenceë¥¼ ì°¸ê³ í•´ ì§ˆë¬¸(ë˜ëŠ” ì„œë¸Œì¿¼ë¦¬)ì— ëŒ€í•œ ê°„ê²°í•œ ë‹µë³€ì„ ì‘ì„±.
    - ì¶”ê°€ ì •ë³´ ìš”ì²­ ê¸ˆì§€(í•­ìƒ ë‹µë³€ ì‹œë„).

    Args:
        user_or_subquery: User question or subquery text
        evidence_block: Retrieved evidence
        intent: Query intent - "debug" or "analysis" (default: "debug")
        query_type: Query type - "cause" or "effect" (default: "cause")
    """
    template_loader = PromptTemplateLoader()

    # Select template based on intent
    if intent == "analysis":
        template_name = "answer_activity.j2"
    else:  # debug (default)
        template_name = "answer_sub.j2"

    return template_loader.render_template(
        template_name,
        query=user_or_subquery.strip(),
        evidence_block=evidence_block if evidence_block else "(No retriever evidence)",
        query_type=query_type  # âœ¨ Pass query type for perspective instruction
    )


def _build_final_prompt(user_question: str, ordered_qids: List[str], subanswers_by_qid: Dict[str, Dict[str, Any]], available_ranges_by_qid: Dict[str, List[str]] = None) -> str:
    """ ìµœì¢… í”„ë¡¬í”„íŠ¸ (Jinja2 template ì‚¬ìš©):
    â–¶ ìµœì¢… ì…ë ¥ = (ì‚¬ìš©ì ì§ˆë¬¸) + (ê° ì„œë¸Œì¿¼ë¦¬ë³„ 'ê·¸ëŒ€ë¡œì˜' ë‹µë³€ë“¤) + (available ranges for verification) + (ìµœì¢… ì§€ì¹¨ í”„ë¡¬í”„íŠ¸)
    """
    template_loader = PromptTemplateLoader()

    return template_loader.render_template(
        "answer_final.j2",
        user_question=user_question.strip() or '(Empty question)',
        ordered_qids=ordered_qids,
        subanswers_by_qid=subanswers_by_qid,
        available_ranges_by_qid=available_ranges_by_qid or {}  # Token-efficient: only ranges, not full evidence
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LLM í˜¸ì¶œ (3íšŒ ì¬ì‹œë„)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _ask_with_retries(llm: Chatbot, prompt: str, retries: int = 2) -> Tuple[str, str]:
    """ LLM í˜¸ì¶œì„ ìµœëŒ€ retriesíšŒ ì¬ì‹œë„ (ê¸°ë³¸ 2íšŒë¡œ ìµœì í™”).
    ë°˜í™˜: (final_text_after_thought, raw_text_from_llm)
    ì‹¤íŒ¨ ì‹œ í´ë°± ë©”ì‹œì§€ ë°˜í™˜.
    """
    last_err: Optional[Exception] = None
    raw_text: str = ""
    # ì¬ì‹œë„ íšŸìˆ˜ ìµœì í™”: ê¸°ë³¸ 3â†’2
    optimized_retries = min(max(1, retries), 2)

    for i in range(1, optimized_retries + 1):
        try:
            res = await llm.ask(prompt)
            raw_text = getattr(res, "content", None) or str(res) or ""
            final_text = _after_thought(raw_text)
            if final_text.strip():
                return final_text, raw_text
            logger.warning("[answer_node] empty LLM content (attempt %d/%d)", i, optimized_retries)
        except Exception as e:
            last_err = e
            logger.warning("[answer_node] LLM call failed (attempt %d/%d): %s", i, optimized_retries, e)

            # Exponential backoff ì ìš©
            if i < optimized_retries:
                await asyncio.sleep(2 ** (i - 1))

    if last_err:
        logger.warning("[answer_node] All retry attempts failed: %s", last_err)

    fallback = (
        "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n"
        "ê·¼ê±°: ì—†ìŒ"
    )
    return fallback, raw_text


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í¼ë¸”ë¦­ ì—”íŠ¸ë¦¬: answer_node
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def answer_node(
    *,
    answer_llm: Optional[Chatbot] = None,
    agent_name: str = "answer",
) -> callable:
    if answer_llm is None:
        from ...utils import create_chatbot_from_env
        answer_llm = create_chatbot_from_env(
            temperature=0.1
            # max_tokensëŠ” ê¸°ë³¸ê°’(4096) ì‚¬ìš©
        )

    async def node_function(state: AgentState) -> AgentState:
        logger.info(f"[answer_node] Called with state type: {type(state)}")

        # ğŸ›¡ï¸ State íƒ€ì… ê²€ì¦ (LangGraph ìƒíƒœ ì „ë‹¬ ì²´ì¸ ë³´í˜¸)
        if isinstance(state, str):
            logger.error(f"[answer_node] Received string state instead of AgentState: {state[:200]}...")
            raise TypeError(f"Expected AgentState but received string: {type(state)}")

        if not hasattr(state, 'get'):
            logger.error(f"[answer_node] State object missing 'get' method: {type(state)}")
            raise TypeError(f"Invalid state object type: {type(state)}")

        try:
            messages: List[BaseMessage] = state.get("messages", []) or []
            logger.info(f"[answer_node] Successfully got messages: {len(messages)}")
        except Exception as e:
            logger.error(f"[answer_node] Error getting messages: {e}")
            raise

        try:
            md: Dict[str, Any] = state.get("metadata", {}) or {}
            logger.info(f"[answer_node] Successfully got metadata: {type(md)}")
        except Exception as e:
            logger.error(f"[answer_node] Error getting metadata: {e}")
            raise

        try:
            user_utter = _get_last_user_utterance(messages) or ""
            logger.info(f"[answer_node] Successfully got user utterance")
        except Exception as e:
            logger.error(f"[answer_node] Error getting user utterance: {e}")
            raise

        # ============================================
        # í’ˆì§ˆ í•„í„°ë§ëœ ê²°ê³¼ í•œ ë²ˆë§Œ ì¡°íšŒ (ì¤‘ë³µ í˜¸ì¶œ ìµœì í™”)
        # ============================================
        try:
            logger.info(f"[answer_node] Getting quality-filtered search results from state")
            results_by_qid: Dict[str, List[Dict[str, Any]]] = get_quality_filtered_search_results(state)
            logger.info(f"[answer_node] Quality-filtered search results extraction successful: {len(results_by_qid)} queries")
        except Exception as e:
            logger.error(f"[answer_node] Error getting quality-filtered search results: {e}")
            raise

        # (A) í’ˆì§ˆ í•„í„°ë§ëœ ê²°ê³¼ë¡œ evidence ìˆ˜ì§‘
        try:
            logger.info(f"[answer_node] Collecting evidence from filtered results")
            evidence_block_all, label_map_all = _collect_evidence(state, results_by_qid)
            logger.info(f"[answer_node] Evidence collection successful")
        except Exception as e:
            logger.error(f"[answer_node] Error collecting evidence: {e}")
            raise

        # ============================================
        # ë£¨í”„ ë°©ì§€ ìƒíƒœ í™•ì¸ (ì„±ê³µí•œ ì¿¼ë¦¬ê°€ ìˆëŠ”ì§€ ê³ ë ¤)
        # ============================================
        try:
            logger.info(f"[answer_node] Checking loop prevention status")
            loop_prevention_active = _check_loop_prevention_status(state)
            logger.info(f"[answer_node] Loop prevention check completed: {loop_prevention_active.get('terminated', False)}")
        except Exception as e:
            logger.error(f"[answer_node] Error in loop prevention check: {e}")
            raise

        # ìŠ¤í‚µëœ ì¿¼ë¦¬ ì •ë³´ ë¡œê¹…
        skipped_queries = md.get("skipped_queries", [])
        if skipped_queries:
            logger.warning(f"[answer_node] Skipped queries (gave up after max failures): {skipped_queries}")

        # ì„±ê³µí•œ ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´ ì •ìƒ ë‹µë³€ ìƒì„±, ì—†ìœ¼ë©´ ì¢…ë£Œ ë©”ì‹œì§€ ìƒì„±
        termination_message = None
        has_successful_queries = len(results_by_qid) > 0

        if loop_prevention_active["terminated"] and not has_successful_queries:
            # ë£¨í”„ ë°©ì§€ ì¢…ë£Œ + ì„±ê³µí•œ ì¿¼ë¦¬ ì—†ìŒ â†’ ì¢…ë£Œ ë©”ì‹œì§€
            termination_message = _create_loop_termination_message(
                state,
                loop_prevention_active["reason"],
                loop_prevention_active
            )
            logger.warning(f"[answer_node] Loop prevention termination with no successful queries: {loop_prevention_active['reason']}")
        elif loop_prevention_active["terminated"] and has_successful_queries:
            # ë£¨í”„ ë°©ì§€ ì¢…ë£Œ + ì„±ê³µí•œ ì¿¼ë¦¬ ìˆìŒ â†’ ì •ìƒ ë‹µë³€ ìƒì„±
            logger.info(f"[answer_node] Loop prevention active but proceeding with {len(results_by_qid)} successful queries")

        # ì¿¼ë¦¬ ë§¤í•‘ ì¶”ì¶œ (í˜¸í™˜ì„± ì§€ì›)
        retr_data = MetadataManager.safe_get_metadata(state, "faiss_retriever")
        queries_by_qid: Dict[str, str] = retr_data.get("queries_by_qid", {}) if retr_data else {}

        # âœ¨ Extract intent from planner metadata
        planner_data = MetadataManager.safe_get_metadata(state, "planner")
        intent = planner_data.get("intent", "debug") if planner_data else "debug"
        logger.info(f"[answer_node] Using intent: '{intent}' for answer generation")

        # âœ¨ Extract query types from planner metadata (expansion queries)
        plan_json = planner_data.get("last_plan_json", {}) if planner_data else {}
        subqueries = plan_json.get("subqueries", [])
        query_type_map = {sq.get("id"): sq.get("type", "cause") for sq in subqueries}
        logger.info(f"[answer_node] Query type mapping: {query_type_map}")

        # ğŸ”§ ì„±ê³µí•œ ì¿¼ë¦¬ë§Œ ì‚¬ìš© (ì‹¤íŒ¨ ì¿¼ë¦¬ ì œì™¸)
        qid_set = set(results_by_qid.keys())
        qids = sorted(list(qid_set), key=_qid_sort_key)

        # (C) ì„œë¸Œì¿¼ë¦¬ë³„ ì²˜ë¦¬: evidence ìˆ˜ì§‘ â†’ ë³‘ë ¬ ì‘ë‹µ ìƒì„± âš¡
        subanswers_by_qid: Dict[str, Dict[str, Any]] = {}
        evidence_by_qid: Dict[str, Any] = {}

        if qids:
            # ğŸš€ ë³‘ë ¬í™”: ëª¨ë“  ì„œë¸Œì¿¼ë¦¬ ë‹µë³€ì„ ë™ì‹œì— ìƒì„±
            async def process_single_qid(qid: str):
                """ë‹¨ì¼ ì„œë¸Œì¿¼ë¦¬ ì²˜ë¦¬ (ë³‘ë ¬ ì‹¤í–‰ìš©)"""
                used_query = (queries_by_qid or {}).get(qid, "") or f"(QID:{qid})"
                ev_block_qid, label_map_qid = _collect_evidence_for_qid(state, qid, results_by_qid)

                # ğŸ” ë””ë²„ê¹…: Evidence ê¸¸ì´ í™•ì¸
                logger.info(f"[answer_node] {qid} evidence length: {len(ev_block_qid)} chars, docs: {len(results_by_qid.get(qid, []))}")
                if len(ev_block_qid) < 100:
                    logger.warning(f"[answer_node] {qid} has very short evidence: {ev_block_qid[:200]}")

                # âœ¨ Get query type for this QID
                query_type = query_type_map.get(qid, "cause")
                logger.info(f"[answer_node] {qid} using query_type: '{query_type}'")

                sub_prompt = _build_sub_prompt(used_query, ev_block_qid, intent=intent, query_type=query_type)
                sub_final_text, sub_raw_text = await _ask_with_retries(answer_llm, sub_prompt, retries=2)
                return qid, {
                    "query": used_query,
                    "output": sub_final_text.strip(),
                    "raw": sub_raw_text,
                }, label_map_qid

            # ğŸ”§ Provider ê¸°ë°˜ ë™ì‹œì„± ì œí•œ (EXAONEì€ ìˆœì°¨ ì²˜ë¦¬)
            llm_provider = md.get("llm_provider", "").lower()

            if llm_provider == "exaone":
                # EXAONE: ìˆœì°¨ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì•ˆì „)
                max_concurrent = 1
                logger.info(f"[answer_node] Sequential processing for EXAONE (max_concurrent=1)")
            else:
                # API ëª¨ë¸: GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬
                max_concurrent = 5  # ê¸°ë³¸ ë™ì‹œ ì‹¤í–‰ ìˆ˜

                # GPU ë©”ëª¨ë¦¬ ì—¬ìœ  í™•ì¸ ë° ë™ì  ì¡°ì •
                import torch
                if torch.cuda.is_available():
                    free_memory_gb = torch.cuda.mem_get_info(0)[0] / 1e9
                    if free_memory_gb < 4.0:
                        logger.warning(f"[answer_node] Low GPU memory ({free_memory_gb:.1f}GB), reducing max_concurrent to 2")
                        max_concurrent = 2
                    elif free_memory_gb < 6.0:
                        logger.warning(f"[answer_node] Moderate GPU memory ({free_memory_gb:.1f}GB), limiting max_concurrent to 3")
                        max_concurrent = 3
                    logger.info(f"[answer_node] Parallel processing with max_concurrent={max_concurrent} (free_memory={free_memory_gb:.1f}GB)")
                else:
                    logger.info(f"[answer_node] Parallel processing with max_concurrent={max_concurrent} (no GPU)")

            # Chunk ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬
            all_results = []
            for chunk_idx in range(0, len(qids), max_concurrent):
                chunk = qids[chunk_idx:chunk_idx + max_concurrent]
                logger.info(f"[answer_node] Processing chunk {chunk_idx//max_concurrent + 1}/{(len(qids) + max_concurrent - 1)//max_concurrent} ({len(chunk)} queries)")

                tasks = [process_single_qid(qid) for qid in chunk]
                chunk_results = await asyncio.gather(*tasks, return_exceptions=True)
                all_results.extend(chunk_results)

                # ì²­í¬ ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
                if torch.cuda.is_available() and chunk_idx + max_concurrent < len(qids):
                    torch.cuda.empty_cache()

            # ê²°ê³¼ ìˆ˜ì§‘
            for result in all_results:
                if isinstance(result, Exception):
                    logger.error(f"[answer_node] Subquery processing failed: {result}")
                    continue
                qid, subanswer, label_map = result
                subanswers_by_qid[qid] = subanswer
                evidence_by_qid[qid] = label_map
        else:
            # ì„œë¸Œì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ì „ì²´ evidenceë¡œ ë‹¨ë°œ ì²˜ë¦¬(í´ë°±)
            # Fallback uses default query_type="cause"
            sub_prompt = _build_sub_prompt(user_utter, evidence_block_all, intent=intent, query_type="cause")
            sub_final_text, sub_raw_text = await _ask_with_retries(answer_llm, sub_prompt, retries=3)
            subanswers_by_qid["Q0"] = {
                "query": user_utter,
                "output": sub_final_text.strip(),
                "raw": sub_raw_text,
            }
            evidence_by_qid["Q0"] = label_map_all
            qids = ["Q0"]

        # (D) ìµœì¢… ê²°í•©: ì‚¬ìš©ì ì§ˆë¬¸ + ì„œë¸Œë‹µë³€(ê·¸ëŒ€ë¡œ) + ìµœì¢… ì§€ì¹¨ í”„ë¡¬í”„íŠ¸
        # âœ¨ Extract available ranges from evidence (token-efficient)
        available_ranges_by_qid = {}
        for qid, label_map in evidence_by_qid.items():
            ranges = []
            for key, data in label_map.items():
                if isinstance(data, dict) and "meta" in data:
                    line_range = data["meta"].get("line_range", "")
                    if line_range:
                        ranges.append(line_range)
            available_ranges_by_qid[qid] = ranges

        if termination_message:
            # ë£¨í”„ ë°©ì§€ ì¢…ë£Œ ì‹œ íŠ¹ë³„ ë©”ì‹œì§€ + ë¶€ë¶„ ê²°ê³¼ ì²˜ë¦¬
            final_text = _build_termination_response(
                termination_message,
                qids,
                subanswers_by_qid,
                loop_prevention_active
            )
            final_raw_text = final_text
            final_prompt = "Loop prevention termination - no LLM call needed"
        else:
            # ì •ìƒ ì²˜ë¦¬ - Pass available ranges for verification
            # Build final prompt with all subquery document summaries
            final_prompt = _build_final_prompt(user_utter, qids, subanswers_by_qid, available_ranges_by_qid)

            # Call LLM to synthesize final answer to user's question
            final_text, final_raw_text = await _ask_with_retries(answer_llm, final_prompt, retries=3)

        # (E) ì¶”ì  ë©”íƒ€ ë³´ì¡´ - MetadataManagerë¥¼ ì‚¬ìš©í•œ ì•ˆì „í•œ ì €ì¥
        existing_answer_data = MetadataManager.safe_get_metadata(state, "answer") or {}

        # ìƒˆë¡œìš´ answer ë©”íƒ€ë°ì´í„° êµ¬ì„±
        answer_metadata = {
            **existing_answer_data,  # ê¸°ì¡´ ë°ì´í„° ë³´ì¡´
            # ìµœì í™”: ì „ì²´ evidenceëŠ” faiss_retrieverì—ì„œ ì°¸ì¡°
            "evidence_ref": "metadata.faiss_retriever.results_by_qid",
            "used_queries_ref": "metadata.faiss_retriever.queries_by_qid",
            # QIDë³„ ì„œë¸Œ ì‘ë‹µ (ì‹¤ì œ ë°ì´í„°)
            "subanswers_by_qid": subanswers_by_qid,
            # ìµœì¢… ì‘ë‹µ (ì••ì¶•)
            "final": {
                "output": final_text.strip(),
                # ë””ë²„ê¹… ì‹œì—ë§Œ ì €ì¥
                "_debug": {
                    "prompt_length": len(final_prompt),
                    "raw_length": len(final_raw_text)
                } if os.getenv("DEBUG_MODE") == "1" else {}
            },
        }

        # MetadataManagerë¥¼ í†µí•´ ì•ˆì „í•˜ê²Œ ì €ì¥
        MetadataManager.safe_set_metadata(state, "answer", answer_metadata)

        # (F) ì‹¤í—˜ìš© ì¶œë ¥ íŒŒì‹± - ìµœì¢… ë‹µë³€ì—ì„œ line ë²”ìœ„ ì¶”ì¶œ (ë‹¤ì¤‘ ë²”ìœ„ ì§€ì›)
        experiment_output = None

        # ğŸ“Š [NEW] ì „ì²´ ë¡œê·¸ ë²”ìœ„ ê³„ì‚° (ëª¨ë“  ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ì¶”ì¶œ)
        total_log_start = float('inf')
        total_log_end = 0
        overall_total_lines = 0
        all_doc_lines = []

        for qid, label_map in evidence_by_qid.items():
            for key, data in label_map.items():
                if isinstance(data, dict):
                    meta = data.get("meta") or {}
                    total_lines_meta = _parse_int((meta or {}).get("total_lines"))
                    if total_lines_meta:
                        overall_total_lines = max(overall_total_lines, total_lines_meta)

                    line_start = data.get("line_start")
                    line_end = data.get("line_end")
                    if line_start is not None and line_end is not None:
                        total_log_start = min(total_log_start, line_start)
                        total_log_end = max(total_log_end, line_end)
                        all_doc_lines.extend(range(line_start, line_end + 1))

        # ì „ì²´ ë¡œê·¸ ë²”ìœ„ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        if overall_total_lines > 0:
            total_log_start = 1
            total_log_end = overall_total_lines
        elif total_log_start == float('inf'):
            total_log_start = 1
            total_log_end = 1

        # ì¤‘ë³µ ì œê±°í•˜ì—¬ ì‹¤ì œ ê²€ìƒ‰ëœ ë¡œê·¸ ë¼ì¸ ìˆ˜ ê³„ì‚°
        unique_doc_lines = sorted(list(set(all_doc_lines))) if all_doc_lines else []
        total_line_span = max(total_log_end - total_log_start + 1, 1)

        if final_text:
            # ë‹¤ì¤‘ line ë²”ìœ„ íŒ¨í„´ íŒŒì‹±
            line_patterns = [
                r'line\[(\d+)~(\d+)\]',              # "line[100~200]" (ê¶Œì¥ í˜•ì‹)
                r'lines?\s+(\d+)\s*[-~]\s*(\d+)',    # "line 100-200", "lines 100~200"
                r'\[(\d+)\s*[-~]\s*(\d+)\]',          # "[100-200]", "[100~200]"
                r'(\d+)\s*[-~]\s*(\d+)\s+lines?',    # "100-200 lines"
            ]

            # ëª¨ë“  ë§¤ì¹­ ë²”ìœ„ë¥¼ ìˆ˜ì§‘
            all_ranges = []
            for pattern in line_patterns:
                matches = re.finditer(pattern, final_text, re.IGNORECASE)
                for match in matches:
                    start = int(match.group(1))
                    end = int(match.group(2))
                    if start <= end:  # ìœ íš¨í•œ ë²”ìœ„ë§Œ
                        all_ranges.append((start, end))
                        logger.info(f"[answer_node] Extracted line range: {start}-{end}")

            if all_ranges:
                # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
                all_ranges = sorted(list(set(all_ranges)))

                # ëª¨ë“  ë²”ìœ„ì˜ ë¼ì¸ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³‘í•©
                recommended_lines = []
                for start, end in all_ranges:
                    recommended_lines.extend(range(start, end + 1))

                # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
                recommended_lines = sorted(list(set(recommended_lines)))

                # ì „ì²´ ë²”ìœ„ ê³„ì‚° (ì²« ë²ˆì§¸ ë¼ì¸ ~ ë§ˆì§€ë§‰ ë¼ì¸)
                overall_start = recommended_lines[0]
                overall_end = recommended_lines[-1]

                experiment_output = {
                    # ê¸°ì¡´ í•„ë“œ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
                    "recommended_lines": recommended_lines,
                    "start_line": overall_start,
                    "end_line": overall_end,
                    "line_count": len(recommended_lines),
                    "ranges": all_ranges,  # ê°œë³„ ë²”ìœ„ ì •ë³´ ë³´ì¡´
                    # ğŸ”¬ ì‹¤í—˜ìš© ë©”íŠ¸ë¦­
                    "faiss_calls": md.get("faiss_calls", 0),
                    "planner_iterations": md.get("planner_iterations", 0),
                    # ğŸ“Š [NEW] ì „ì²´ ë¡œê·¸ ë²”ìœ„ ì •ë³´ ì¶”ê°€
                    "total_log_range": {
                        "start": total_log_start,
                        "end": total_log_end,
                        "count": total_line_span,
                        "retrieved_lines": len(unique_doc_lines),
                        "retrieved_coverage": f"{len(unique_doc_lines) / total_line_span * 100:.1f}%" if total_line_span > 0 else "0%"
                    },
                    # ğŸ“Š [NEW] ì¶”ì²œ ë²”ìœ„ ìƒì„¸ ì •ë³´ ì¶”ê°€
                    "recommended_range": {
                        "start": overall_start,
                        "end": overall_end,
                        "count": len(recommended_lines),
                        "coverage": f"{len(recommended_lines) / total_line_span * 100:.1f}%" if total_line_span > 0 else "0%"
                    }
                }
                logger.info(f"[answer_node] Total {len(all_ranges)} ranges found, {len(recommended_lines)} total lines")
                logger.info(f"[answer_node] Total log range: {total_log_start}-{total_log_end} ({total_line_span} lines), Coverage: {experiment_output['recommended_range']['coverage']}")
            else:
                # ğŸ“Š [NEW] ì¶”ì²œ ë²”ìœ„ê°€ ì—†ì–´ë„ ì „ì²´ ë¡œê·¸ ë²”ìœ„ëŠ” ì œê³µ
                experiment_output = {
                    # ğŸ”¬ ì‹¤í—˜ìš© ë©”íŠ¸ë¦­
                    "faiss_calls": md.get("faiss_calls", 0),
                    "planner_iterations": md.get("planner_iterations", 0),
                    # ğŸ“Š [NEW] ì „ì²´ ë¡œê·¸ ë²”ìœ„ ì •ë³´
                    "total_log_range": {
                        "start": total_log_start,
                        "end": total_log_end,
                        "count": total_line_span,
                        "retrieved_lines": len(unique_doc_lines),
                        "retrieved_coverage": f"{len(unique_doc_lines) / total_line_span * 100:.1f}%" if total_line_span > 0 else "0%"
                    },
                    "recommended_range": None  # ì¶”ì²œ ë²”ìœ„ ì—†ìŒ
                }
                logger.warning(f"[answer_node] No line range found in final answer, but total range available: {total_log_start}-{total_log_end}")

        # (G) ë©”ì‹œì§€ ëˆ„ì 
        ai_msg = AIMessage(
            content=final_text.strip(),
            additional_kwargs={"agent_name": agent_name},
        )
        new_messages = (messages or []) + [ai_msg]

        return type(state)(
            messages=new_messages,
            context_messages=state.get("context_messages", []),
            metadata=md,
            current_agent=agent_name,
            session_id=state.get("session_id", ""),
            experiment_output=experiment_output,  # ì‹¤í—˜ìš© êµ¬ì¡°í™”ëœ ì¶œë ¥
        )

    return node_function
