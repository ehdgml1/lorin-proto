"""
Quality Evaluator Node - LangGraph Integration Module
=====================================================

LORIN ì‹œìŠ¤í…œì˜ LangGraph workflowì— í†µí•©ë˜ëŠ” í’ˆì§ˆ í‰ê°€ ë…¸ë“œì…ë‹ˆë‹¤.
FAISS ê²€ìƒ‰ ê²°ê³¼ë¥¼ EXAONE ëª¨ë¸ë¡œ í‰ê°€í•˜ì—¬ ê´€ë ¨ì„±ì„ íŒë‹¨í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- state.metadata.faiss_retriever ë°ì´í„° ì²˜ë¦¬
- ì„œë¸Œì¿¼ë¦¬ë³„ ê´€ë ¨ì„± í‰ê°€ ì‹¤í–‰
- state.metadata.quality_evaluatorì— ê²°ê³¼ ì €ì¥
- ê¸°ì¡´ state ë° metadata ë³´ì¡´
- ìƒì„¸í•œ í‰ê°€ í†µê³„ ë° ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì œê³µ

í†µí•© í¬ì¸íŠ¸:
- ì…ë ¥: FAISS retriever ê²°ê³¼
- ì¶œë ¥: í‰ê°€ëœ ê´€ë ¨ì„± ì •ë³´
- ì˜ì¡´ì„±: quality_evaluator.py, chatbot.py
"""

from typing import Dict, List, Any, Optional, Tuple
from dataclasses import asdict
from datetime import datetime, timezone

from langchain_core.messages import AIMessage, BaseMessage
from ...make_faiss.search_engine import SearchResult
from ...logger.logger import get_logger
from ...config.settings import get_settings
from ..state import AgentState
from .quality_evaluator import (
    QualityEvaluator,
    QueryEvaluationResult,
    EvaluationStats,
    DocumentEvaluation
)

# â”€â”€ ë¼ìš°íŒ… ê²°ì • ë° ë£¨í”„ ë°©ì§€ ëª¨ë“ˆ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from ..routing.routing_decision import (
    should_retry_routing,
    analyze_query_results as _analyze_query_results,
    identify_persistent_failures as _identify_persistent_failures,
    determine_iterative_routing_with_loop_prevention as _determine_iterative_routing_with_loop_prevention,
    determine_iterative_routing as _determine_iterative_routing,
)
from ..routing.loop_prevention import (
    initialize_loop_prevention as _initialize_loop_prevention,
    detect_routing_loop as _detect_routing_loop,
    detect_oscillation_pattern as _detect_oscillation_pattern,
    detect_consecutive_pattern as _detect_consecutive_pattern,
    detect_cycle_pattern as _detect_cycle_pattern,
    log_forced_termination as _log_forced_termination,
    update_loop_prevention_metadata as _update_loop_prevention_metadata,
)
from ..schema import MetadataManager

# DataClasses are now imported from quality_evaluator module

logger = get_logger(__name__)

# â”€â”€ ì¤‘ì•™ ì§‘ì¤‘ì‹ ì„¤ì • ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_settings = get_settings()
_qe_cfg = _settings.quality_evaluator

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utility Functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _build_query_success_timeline(
    existing_timeline: Dict[str, Any],
    current_results: Dict[str, Any],
    current_iteration: int,
    current_timestamp: str,
    search_timestamp: str
) -> Dict[str, Any]:
    """ì¿¼ë¦¬ë³„ ì„±ê³µ ì‹œì  ì¶”ì  ì‹œìŠ¤í…œ êµ¬ì¶•

    Args:
        existing_timeline: ê¸°ì¡´ ì„±ê³µ ì‹œì  ì¶”ì  ë°ì´í„°
        current_results: í˜„ì¬ í‰ê°€ ê²°ê³¼
        current_iteration: í˜„ì¬ ë°˜ë³µ íšŸìˆ˜
        current_timestamp: í˜„ì¬ í‰ê°€ ì‹œê°
        search_timestamp: ê²€ìƒ‰ ì‹¤í–‰ ì‹œê°

    Returns:
        Dict[str, Any]: ì—…ë°ì´íŠ¸ëœ ì„±ê³µ ì‹œì  ì¶”ì  ë°ì´í„°
    """
    timeline = existing_timeline.copy() if existing_timeline else {}

    for qid, result_data in current_results.items():
        is_relevant = result_data.get("is_relevant", False)

        # ì¿¼ë¦¬ê°€ ì„±ê³µí–ˆê³ , ì•„ì§ ì„±ê³µ ê¸°ë¡ì´ ì—†ëŠ” ê²½ìš°ë§Œ ìµœì´ˆ ì„±ê³µ ì‹œì  ê¸°ë¡
        if is_relevant and qid not in timeline:
            timeline[qid] = {
                "iteration": current_iteration,
                "success_timestamp": current_timestamp,
                "search_timestamp": search_timestamp,
                "relevance_score": result_data.get("relevance_score", 0.0),
                "confidence": result_data.get("confidence", 0.0),
                "first_success": True
            }
            logger.info(f"[_build_query_success_timeline] First success recorded for {qid} at iteration {current_iteration}")
        elif is_relevant and qid in timeline:
            # ì´ë¯¸ ì„±ê³µí•œ ì¿¼ë¦¬ì˜ ì¬í‰ê°€ ì‹œ latest ì •ë³´ë§Œ ì—…ë°ì´íŠ¸ (first_successëŠ” ìœ ì§€)
            timeline[qid].update({
                "latest_iteration": current_iteration,
                "latest_success_timestamp": current_timestamp,
                "latest_search_timestamp": search_timestamp,
                "latest_relevance_score": result_data.get("relevance_score", 0.0),
                "latest_confidence": result_data.get("confidence", 0.0)
            })
            logger.info(f"[_build_query_success_timeline] Updated latest success for {qid} at iteration {current_iteration}")

    return timeline

def _build_query_status_by_iteration(
    existing_status: Dict[str, Dict[str, str]],
    current_results: Dict[str, Any],
    current_iteration: int
) -> Dict[str, Dict[str, str]]:
    """ë°˜ë³µë³„ ì¿¼ë¦¬ ìƒíƒœ ì´ë ¥ êµ¬ì¶•

    Args:
        existing_status: ê¸°ì¡´ ë°˜ë³µë³„ ìƒíƒœ ë°ì´í„°
        current_results: í˜„ì¬ í‰ê°€ ê²°ê³¼
        current_iteration: í˜„ì¬ ë°˜ë³µ íšŸìˆ˜

    Returns:
        Dict[str, Dict[str, str]]: ì—…ë°ì´íŠ¸ëœ ë°˜ë³µë³„ ìƒíƒœ ë°ì´í„°
    """
    status_by_iteration = existing_status.copy() if existing_status else {}

    current_status = {}
    for qid, result_data in current_results.items():
        is_relevant = result_data.get("is_relevant", False)
        current_status[qid] = "pass" if is_relevant else "fail"

    status_by_iteration[str(current_iteration)] = current_status

    logger.info(f"[_build_query_status_by_iteration] Recorded status for iteration {current_iteration}: {current_status}")
    return status_by_iteration

def _get_search_timestamps_from_state(state: AgentState) -> Tuple[str, str]:
    """Stateì—ì„œ ê²€ìƒ‰ ì‹¤í–‰ ì‹œê° ì¶”ì¶œ

    Args:
        state: Current agent state

    Returns:
        Tuple[str, str]: (ê²€ìƒ‰ ì‹¤í–‰ ì‹œê°, í˜„ì¬ ì‹œê°)
    """
    metadata = state.get("metadata", {})

    # faiss_retrieverì˜ timestamp ì‚¬ìš©
    faiss_data = metadata.get("faiss_retriever", {})
    search_timestamp = faiss_data.get("timestamp", "")

    current_timestamp = datetime.now(timezone.utc).isoformat()

    return search_timestamp, current_timestamp

def _reconstruct_search_results(results_data: List[Dict[str, Any]]) -> List[SearchResult]:
    """ì§ë ¬í™”ëœ SearchResult ë°ì´í„°ë¥¼ ê°ì²´ë¡œ ë³µì›"""
    search_results = []

    for result_dict in results_data:
        search_result = SearchResult(
            rank=result_dict.get("rank", 0),
            score=result_dict.get("score", 0.0),
            content=result_dict.get("content", ""),
            metadata=result_dict.get("metadata", {}),
            line_range=result_dict.get("line_range", []),
            time_start=result_dict.get("time_start", ""),
            time_end=result_dict.get("time_end", "")
        )
        search_results.append(search_result)

    return search_results

def _extract_retrieval_data(state: AgentState) -> Tuple[List[Dict[str, Any]], Dict[str, List[SearchResult]]]:
    """Stateì—ì„œ FAISS ê²€ìƒ‰ ë°ì´í„° ì¶”ì¶œ"""
    metadata = state.get("metadata", {})
    faiss_data = metadata.get("faiss_retriever", {})

    if not faiss_data.get("success", False):
        logger.warning("[quality_evaluator_node] FAISS retrieval data not found or unsuccessful")
        return [], {}

    # ì„œë¸Œì¿¼ë¦¬ ì •ë³´ ì¶”ì¶œ
    planner_data = metadata.get("planner", {})
    plan_json = planner_data.get("last_plan_json", {})
    subqueries = plan_json.get("subqueries", [])

    # ë””ë²„ê¹…: Planner ì„œë¸Œì¿¼ë¦¬ í™•ì¸
    logger.info(f"ğŸ” [_extract_retrieval_data] Planner subqueries extraction:")
    logger.info(f"  - Total subqueries: {len(subqueries)}")
    for i, sq in enumerate(subqueries):
        query_id = sq.get("id", "unknown")
        query_text = sq.get("text", "")
        logger.info(f"  - [{query_id}] text='{query_text[:80]}...' (length: {len(query_text)})")

    if not subqueries:
        logger.warning("[quality_evaluator_node] No subqueries found in planner data")
        return [], {}

    # ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ ë° ë³µì›
    results_by_qid = faiss_data.get("results_by_qid", {})
    documents_by_query = {}

    for qid, result_data in results_by_qid.items():
        raw_results = result_data.get("results", [])
        documents_by_query[qid] = _reconstruct_search_results(raw_results)

    logger.info(f"[quality_evaluator_node] Extracted {len(subqueries)} subqueries and "
               f"{len(documents_by_query)} result sets")

    return subqueries, documents_by_query

def _prepare_evaluation_batch(
    subqueries: List[Dict[str, Any]],
    documents_by_query: Dict[str, List[SearchResult]]
) -> List[Tuple[Dict[str, Any], List[SearchResult]]]:
    """í‰ê°€ìš© ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„"""
    batch_data = []

    for subquery in subqueries:
        query_id = subquery.get("id", "unknown")
        documents = documents_by_query.get(query_id, [])

        if documents:  # ë¬¸ì„œê°€ ìˆëŠ” ì¿¼ë¦¬ë§Œ í‰ê°€
            batch_data.append((subquery, documents))
        else:
            logger.warning(f"[quality_evaluator_node] No documents found for query {query_id}")

    return batch_data

def _create_evaluation_summary(
    evaluation_results: Dict[str, QueryEvaluationResult],
    stats: EvaluationStats
) -> str:
    """í‰ê°€ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
    if not evaluation_results:
        return "Quality Evaluation: No new queries evaluated (all previously successful)"

    relevant_queries = sum(1 for result in evaluation_results.values() if result.is_relevant)
    total_queries = len(evaluation_results)

    avg_score = stats.avg_relevance_score
    avg_confidence = stats.avg_confidence

    summary_lines = [
        "Quality Evaluation Complete",
        f"Relevant Queries: {relevant_queries}/{total_queries} ({relevant_queries/total_queries*100:.1f}%)",
        f"Average Relevance Score: {avg_score:.3f}",
        f"Average Confidence: {avg_confidence:.3f}",
        f"Total Documents Evaluated: {stats.total_documents_evaluated}",
        f"Relevant Documents Found: {stats.total_relevant_documents}",
        f"Evaluation Time: {stats.evaluation_time:.2f}s",
        f"Tokens Used: {stats.total_tokens_used}",
        f"API Calls: {stats.api_calls_made}",
        f"Cache Hits: {stats.cache_hits}"
    ]

    if stats.fallback_count > 0:
        summary_lines.append(f"Fallback Evaluations: {stats.fallback_count}")

    return "\n".join(summary_lines)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main Quality Evaluator Node
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def quality_evaluator_node(chatbot):
    """Factory function for LangGraph-compatible quality evaluation node

    Args:
        chatbot: ê³µìœ  Chatbot ì¸ìŠ¤í„´ìŠ¤ (Together API EXAONE ì‚¬ìš©)

    Returns:
        Async function that processes AgentState
    """

    async def _quality_evaluator_node_impl(state: AgentState) -> AgentState:
        """LangGraph-compatible quality evaluation node

        ì´ í•¨ìˆ˜ëŠ”:
        1. state.metadata.faiss_retrieverì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ
        2. ê° ì„œë¸Œì¿¼ë¦¬ì™€ ë¬¸ì„œë“¤ì˜ ê´€ë ¨ì„±ì„ EXAONEìœ¼ë¡œ í‰ê°€
        3. í‰ê°€ ê²°ê³¼ë¥¼ state.metadata.quality_evaluatorì— ì €ì¥
        4. ëª¨ë“  ê¸°ì¡´ stateì™€ metadata ë³´ì¡´
        5. í‰ê°€ ìš”ì•½ ë©”ì‹œì§€ë¥¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€

        Args:
            state: Current agent state from LangGraph

        Returns:
            AgentState: Updated state with quality evaluation results
        """
        logger.info("[quality_evaluator_node] Starting quality evaluation processing")

        # ê¸°ì¡´ ë°ì´í„° ì¶”ì¶œ
        messages: List[BaseMessage] = state.get("messages", [])
        metadata: Dict[str, Any] = state.get("metadata", {})

        try:
            # FAISS ê²€ìƒ‰ ë°ì´í„° ì¶”ì¶œ
            subqueries, documents_by_query = _extract_retrieval_data(state)

            if not subqueries or not documents_by_query:
                logger.warning("[quality_evaluator_node] No data available for evaluation")
                return _create_error_state(
                    state, messages, metadata,
                    "No retrieval data available for quality evaluation"
                )

            # ğŸ”§ ì„ íƒì  ì¬í‰ê°€: ê¸°ì¡´ ì„±ê³µí•œ ì¿¼ë¦¬ëŠ” ì¬í‰ê°€í•˜ì§€ ì•ŠìŒ
            # MetadataManagerë¥¼ í†µí•œ ì•ˆì „í•œ ë©”íƒ€ë°ì´í„° ì ‘ê·¼
            existing_evaluator_data = MetadataManager.safe_get_metadata(state, "quality_evaluator") or {}
            existing_cumulative = existing_evaluator_data.get("cumulative_evaluation_results", {})

            # ì´ë¯¸ ì„±ê³µí•œ ì¿¼ë¦¬ ì‹ë³„
            previously_successful_queries = set()
            for qid, result_data in existing_cumulative.items():
                if result_data.get("is_relevant", False):
                    previously_successful_queries.add(qid)
            logger.info(f"[quality_evaluator_node] Previously successful queries: {len(previously_successful_queries)} - {list(previously_successful_queries)}")

            # ì¬í‰ê°€ê°€ í•„ìš”í•œ ì¿¼ë¦¬ë§Œ í•„í„°ë§
            queries_to_evaluate = []
            documents_to_evaluate = {}

            logger.info(f"[quality_evaluator_node] Debug - subqueries type: {type(subqueries)}, length: {len(subqueries)}")
            for i, query_dict in enumerate(subqueries):
                logger.info(f"[quality_evaluator_node] Debug - subquery {i}: {query_dict}")
                qid = query_dict.get("id", "")  # ğŸ”§ ìˆ˜ì •: 'subquery_id' â†’ 'id'
                logger.info(f"[quality_evaluator_node] Debug - extracted QID: '{qid}'")

                if qid and qid not in previously_successful_queries:
                    queries_to_evaluate.append(query_dict)
                    if qid in documents_by_query:
                        documents_to_evaluate[qid] = documents_by_query[qid]
                        logger.info(f"[quality_evaluator_node] Debug - Added QID {qid} to evaluation")
                    else:
                        logger.warning(f"[quality_evaluator_node] Debug - No documents for QID {qid}")
                else:
                    logger.info(f"[quality_evaluator_node] Debug - Skipped QID {qid} (empty or previously successful)")

            if queries_to_evaluate:
                query_ids = [q.get("id", "") for q in queries_to_evaluate]  # ğŸ”§ ìˆ˜ì •: 'subquery_id' â†’ 'id'
                logger.info(f"[quality_evaluator_node] Queries requiring evaluation: {len(queries_to_evaluate)} - {query_ids}")
                # ì„ ë³„ì  í‰ê°€ ë°°ì¹˜ ì¤€ë¹„
                evaluation_batch = _prepare_evaluation_batch(queries_to_evaluate, documents_to_evaluate)
            else:
                logger.info("[quality_evaluator_node] All queries previously successful, skipping evaluation")
                evaluation_batch = []

            # í‰ê°€í•  ì¿¼ë¦¬ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‹¤ì œ í‰ê°€ ìˆ˜í–‰
            if evaluation_batch:
                logger.info(f"[quality_evaluator_node] Prepared {len(evaluation_batch)} queries for evaluation")

                # ğŸ”§ GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ë™ì  ë™ì‹œì„± ì œí•œ
                max_concurrent = 1  # ê¸°ë³¸ê°’: ë³´ìˆ˜ì  ìˆœì°¨ ì‹¤í–‰
                try:
                    import torch
                    if torch.cuda.is_available():
                        # GPU 0ë§Œ ì‚¬ìš© (EXAONE ì „ìš©)
                        gpu0_free = torch.cuda.mem_get_info(0)[0] / 1e9
                        gpu0_allocated = torch.cuda.memory_allocated(0) / 1e9

                        # ë³´ìˆ˜ì ì¸ ë™ì‹œì„± ì •ì±… (ê¸´ ì…ë ¥ ê³ ë ¤)
                        if gpu0_free > 10.0:
                            max_concurrent = 5  # 10GB ì´ìƒ ì—¬ìœ : ìµœëŒ€ 5ê°œ ë³‘ë ¬
                        elif gpu0_free > 6.0:
                            max_concurrent = 3  # 6GB ì´ìƒ ì—¬ìœ : 3ê°œ ë³‘ë ¬
                        else:
                            max_concurrent = 2  # ê·¸ ì™¸: 2ê°œ ë³‘ë ¬

                        logger.info(
                            f"[quality_evaluator_node] ğŸ’¾ GPU 0: Free={gpu0_free:.2f}GB, "
                            f"Allocated={gpu0_allocated:.2f}GB â†’ max_concurrent={max_concurrent}"
                        )
                except ImportError:
                    logger.warning("[quality_evaluator_node] torch not available, using max_concurrent=1")
                    max_concurrent = 1

                # Quality Evaluator ì´ˆê¸°í™” (ê³µìœ  chatbot ì‚¬ìš© - ë¡œì»¬ EXAONE)
                evaluator = QualityEvaluator(
                    chatbot=chatbot,  # ê³µìœ  Chatbot ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© (GPU ë©”ëª¨ë¦¬ ì ˆì•½)
                    relevance_threshold=0.4,  # 0.5 â†’ 0.4: í”„ë¡¬í”„íŠ¸ "Moderate relevance" ì² í•™ê³¼ ì¼ì¹˜
                    confidence_threshold=0.6,
                    max_docs_per_query=3,  # 5 â†’ 3: í”„ë¡¬í”„íŠ¸ í¬ê¸° ê°ì†Œ
                    max_retries=3,
                    timeout_seconds=120.0  # 30ì´ˆ â†’ 120ì´ˆ: Multi-GPU ë¶„ì‚° ê³ ë ¤
                )

                # ë°°ì¹˜ í‰ê°€ ì‹¤í–‰ (GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ë™ì  ë³‘ë ¬ ì²˜ë¦¬)
                logger.info(f"[quality_evaluator_node] Starting batch evaluation with max_concurrent={max_concurrent}")
                evaluation_results, evaluation_stats = await evaluator.evaluate_batch_queries(
                    evaluation_batch,
                    max_concurrent=max_concurrent  # ë™ì ìœ¼ë¡œ ê²°ì •ëœ ë™ì‹œì„± ì œí•œ
                )
            else:
                # ëª¨ë“  ì¿¼ë¦¬ê°€ ì´ë¯¸ ì„±ê³µí•œ ê²½ìš° - ë¹ˆ ê²°ê³¼ë¡œ ì§„í–‰
                logger.info("[quality_evaluator_node] No new queries to evaluate - all previously successful")
                evaluation_results = {}
                # ë¹ˆ evaluation_stats ìƒì„± (None ëŒ€ì‹  ê¸°ë³¸ê°’ ì‚¬ìš©)
                from .quality_evaluator import EvaluationStats
                evaluation_stats = EvaluationStats(
                    total_queries=0,
                    total_documents_evaluated=0,
                    total_relevant_documents=0,
                    avg_relevance_score=0.0,
                    avg_confidence=0.0,
                    evaluation_time=0.0,
                    total_tokens_used=0,
                    api_calls_made=0,
                    cache_hits=0,
                    fallback_count=0
                )

            # ê¸°ì¡´ í‰ê°€ ê²°ê³¼ì™€ ë³‘í•© (ëˆ„ì  í‰ê°€ ì§€ì›)
            existing_evaluator_data = metadata.get("quality_evaluator", {})
            existing_results = existing_evaluator_data.get("evaluation_results", {})
            existing_cumulative = existing_evaluator_data.get("cumulative_evaluation_results", {})

            # í˜„ì¬ í‰ê°€ ê²°ê³¼ ì§ë ¬í™” (replanner í˜¸í™˜ì„±ì„ ìœ„í•œ í•„ë“œ ë§¤í•‘ í¬í•¨)
            current_serializable_results = {}
            for qid, result in evaluation_results.items():
                current_serializable_results[qid] = {
                    # ìƒˆë¡œìš´ êµ¬ì¡°
                    "query_id": result.query_id,
                    "query_text": result.query_text,
                    "is_relevant": result.is_relevant,
                    "relevance_score": result.relevance_score,
                    "document_evaluations": [asdict(doc_eval) for doc_eval in result.document_evaluations],
                    "evaluation_time": result.evaluation_time,
                    "confidence": result.confidence,
                    "reasoning": result.reasoning,
                    "total_documents": result.total_documents,
                    "relevant_documents": result.relevant_documents,
                    "avg_doc_score": result.avg_doc_score,
                    "max_doc_score": result.max_doc_score,
                    "tokens_used": result.tokens_used,
                    "error": result.error,
                    "fallback_used": result.fallback_used,
                    "evaluation_timestamp": datetime.now(timezone.utc).isoformat(),
                    "iteration_count": metadata.get("replanner", {}).get("iteration_count", 0),

                    # ğŸ”§ replanner í˜¸í™˜ì„±ì„ ìœ„í•œ backward compatibility í•„ë“œ
                    "confidence_score": result.confidence,  # replannerê°€ ê¸°ëŒ€í•˜ëŠ” í•„ë“œëª…
                    "evaluation_details": {  # replannerì—ê²Œ ìƒì„¸ í”¼ë“œë°± ì „ë‹¬
                        "reasoning": result.reasoning,
                        "improvement_suggestions": result.improvement_suggestions,
                            "query_effectiveness": result.query_effectiveness
                        }
                    }

            # ëˆ„ì  ê²°ê³¼ ìƒì„±: ê¸°ì¡´ ì„±ê³µ ê²°ê³¼ ë³´ì¡´ + ìƒˆë¡œìš´ í‰ê°€ ê²°ê³¼ ì¶”ê°€
            cumulative_results = _merge_evaluation_results(
            existing_cumulative or existing_results,
            current_serializable_results,
            metadata.get("replanner", {})
            )

            # ğŸ”§ ì¤‘ë³µ ì¿¼ë¦¬ í•„í„°ë§: 95% ì´ìƒ ìœ ì‚¬í•œ ì¿¼ë¦¬ ì¤‘ relevance_scoreê°€ ë‚®ì€ ê²ƒì„ ì œê±°
            cumulative_results = _filter_duplicate_queries(cumulative_results)

            # í˜„ì¬ ë°˜ë³µì˜ ì¿¼ë¦¬ë³„ ê´€ë ¨ì„± ìš”ì•½ ìƒì„±
            current_relevance_summary = {}
            for qid, result in evaluation_results.items():
                current_relevance_summary[qid] = {
                    "query_text": result.query_text,
                    "is_relevant": result.is_relevant,
                    "relevance_score": result.relevance_score,
                    "confidence": result.confidence,
                    "reasoning": result.reasoning
                }

            # ëˆ„ì  ê´€ë ¨ì„± ìš”ì•½ ìƒì„±
            cumulative_relevance_summary = {}
            for qid, result_data in cumulative_results.items():
                cumulative_relevance_summary[qid] = {
                    "query_text": result_data.get("query_text", ""),
                    "is_relevant": result_data.get("is_relevant", False),
                    "relevance_score": result_data.get("relevance_score", 0.0),
                    "confidence": result_data.get("confidence", 0.0),
                    "reasoning": result_data.get("reasoning", ""),
                    "evaluation_timestamp": result_data.get("evaluation_timestamp", ""),
                    "iteration_count": result_data.get("iteration_count", 0)
                }

            # ============================================
            # ìƒˆë¡œìš´ ì¿¼ë¦¬ ì„±ê³µ ì‹œì  ì¶”ì  ì‹œìŠ¤í…œ êµ¬ì¶•
            # ============================================
            current_iteration = metadata.get("replanner", {}).get("iteration_count", 0)
            search_timestamp, current_timestamp = _get_search_timestamps_from_state(state)

            # ê¸°ì¡´ ì¶”ì  ë°ì´í„° ë¡œë“œ
            existing_evaluator_data = metadata.get("quality_evaluator", {})
            existing_timeline = existing_evaluator_data.get("query_success_timeline", {})
            existing_status_history = existing_evaluator_data.get("query_status_by_iteration", {})

            # ì¿¼ë¦¬ ì„±ê³µ ì‹œì  ì¶”ì  ì—…ë°ì´íŠ¸
            query_success_timeline = _build_query_success_timeline(
            existing_timeline,
            current_serializable_results,
            current_iteration,
            current_timestamp,
            search_timestamp
            )

            # ë°˜ë³µë³„ ì¿¼ë¦¬ ìƒíƒœ ì´ë ¥ ì—…ë°ì´íŠ¸
            query_status_by_iteration = _build_query_status_by_iteration(
            existing_status_history,
            current_serializable_results,
            current_iteration
            )

            # ì„±ê³µ í†µê³„ ê³„ì‚°
            first_success_queries = [qid for qid, info in query_success_timeline.items()
                           if info.get("first_success", False) and info.get("iteration") == current_iteration]
            total_successful_queries = len([qid for qid, info in query_success_timeline.items()])

            logger.info(f"[quality_evaluator_node] Success tracking: {len(first_success_queries)} first successes, {total_successful_queries} total successful")

            # metadata.quality_evaluatorì— ì €ì¥ (ëˆ„ì  ì§€ì› + ìƒˆë¡œìš´ ì¶”ì  ì‹œìŠ¤í…œ)
            quality_evaluator_metadata = {
            "evaluation_results": current_serializable_results,  # í˜„ì¬ ë°˜ë³µ ê²°ê³¼
            "cumulative_evaluation_results": cumulative_results,  # ëˆ„ì  ê²°ê³¼
            "relevance_summary": current_relevance_summary,  # í˜„ì¬ ë°˜ë³µ ìš”ì•½
            "cumulative_relevance_summary": cumulative_relevance_summary,  # ëˆ„ì  ìš”ì•½
            "query_success_timeline": query_success_timeline,  # ğŸ†• ì¿¼ë¦¬ë³„ ì„±ê³µ ì‹œì  ì¶”ì 
            "query_status_by_iteration": query_status_by_iteration,  # ğŸ†• ë°˜ë³µë³„ ìƒíƒœ ì´ë ¥
            "success_statistics": {  # ğŸ†• ì„±ê³µ í†µê³„
            "first_success_this_iteration": len(first_success_queries),
            "total_successful_queries": total_successful_queries,
            "first_success_query_ids": first_success_queries,
            "current_iteration": current_iteration
            },
            "evaluation_stats": asdict(evaluation_stats),
            "config": {
            "relevance_threshold": evaluator.relevance_threshold,
            "confidence_threshold": evaluator.confidence_threshold,
            "max_docs_per_query": evaluator.max_docs_per_query,
            "model": "lgai/exaone-3-5-32b-instruct"
            },
            "evaluation_mode": "cumulative" if existing_cumulative or existing_results else "initial",
            "iteration_count": current_iteration,
            "timestamp": current_timestamp,
            "search_timestamp": search_timestamp,  # ğŸ†• ê²€ìƒ‰ ì‹œê° ê¸°ë¡
            "success": True
            }

            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ë°ì´í„° ë³´ì¡´)
            new_metadata = metadata.copy()
            new_metadata["quality_evaluator"] = quality_evaluator_metadata
            new_metadata["last_agent"] = "quality_evaluator"

            # ìš”ì•½ ë©”ì‹œì§€ ìƒì„±
            summary_content = _create_evaluation_summary(evaluation_results, evaluation_stats)

            evaluation_message = AIMessage(
            content=summary_content,
            additional_kwargs={
            "agent_name": "quality_evaluator",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "evaluation_stats": asdict(evaluation_stats),
            "relevant_queries": sum(1 for r in evaluation_results.values() if r.is_relevant),
            "total_queries": len(evaluation_results)
            }
            )

            new_messages = messages + [evaluation_message]

            logger.info(f"[quality_evaluator_node] Successfully evaluated {len(evaluation_results)} queries")
            logger.info(f"[quality_evaluator_node] Relevant queries: "
               f"{sum(1 for r in evaluation_results.values() if r.is_relevant)}/{len(evaluation_results)}")
            logger.info(f"[quality_evaluator_node] Average relevance score: {evaluation_stats.avg_relevance_score:.3f}")

            # ì—…ë°ì´íŠ¸ëœ state ë°˜í™˜
            return type(state)(
            messages=new_messages,
            context_messages=state.get("context_messages", []),
            metadata=new_metadata,
            current_agent="quality_evaluator",
            session_id=state.get("session_id", "")
            )

        except Exception as e:
            error_msg = f"Quality evaluation failed: {str(e)}"
            logger.error(f"[quality_evaluator_node] {error_msg}")
            return _create_error_state(state, messages, metadata, error_msg)

    return _quality_evaluator_node_impl

def _create_error_state(
    state: AgentState,
    messages: List[BaseMessage],
    metadata: Dict[str, Any],
    error_msg: str
) -> AgentState:
    """ì—ëŸ¬ ìƒíƒœ ìƒì„± í—¬í¼ í•¨ìˆ˜"""

    # ì—ëŸ¬ ë©”íƒ€ë°ì´í„° ì €ì¥
    new_metadata = metadata.copy()
    new_metadata["quality_evaluator"] = {
        "success": False,
        "error": error_msg,
        "timestamp": datetime.now(timezone.utc).isoformat()
    }
    new_metadata["last_agent"] = "quality_evaluator"

    # ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±
    error_message = AIMessage(
        content=f"Quality Evaluation Error: {error_msg}",
        additional_kwargs={
            "agent_name": "quality_evaluator",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "error": True
        }
    )

    new_messages = messages + [error_message]

    return type(state)(
        messages=new_messages,
        context_messages=state.get("context_messages", []),
        metadata=new_metadata,
        current_agent="quality_evaluator",
        session_id=state.get("session_id", "")
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utility Functions for Result Access
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_evaluation_results_for_query(state: AgentState, query_id: str) -> Optional[QueryEvaluationResult]:
    """íŠ¹ì • ì¿¼ë¦¬ì˜ í‰ê°€ ê²°ê³¼ ì¶”ì¶œ

    Args:
        state: Agent state containing evaluation results
        query_id: Query identifier to look up

    Returns:
        Optional[QueryEvaluationResult]: Evaluation result or None if not found
    """
    metadata = state.get("metadata", {})
    evaluator_data = metadata.get("quality_evaluator", {})

    if not evaluator_data.get("success", False):
        return None

    evaluation_results = evaluator_data.get("evaluation_results", {})
    result_data = evaluation_results.get(query_id)

    if not result_data:
        return None

    # DocumentEvaluation ê°ì²´ ë³µì›
    doc_evaluations = []
    for doc_data in result_data.get("document_evaluations", []):
        doc_eval = DocumentEvaluation(**doc_data)
        doc_evaluations.append(doc_eval)

    # QueryEvaluationResult ê°ì²´ ë³µì›
    return QueryEvaluationResult(
        query_id=result_data["query_id"],
        query_text=result_data["query_text"],
        is_relevant=result_data["is_relevant"],
        relevance_score=result_data["relevance_score"],
        document_evaluations=doc_evaluations,
        evaluation_time=result_data["evaluation_time"],
        confidence=result_data["confidence"],
        reasoning=result_data["reasoning"],
        total_documents=result_data["total_documents"],
        relevant_documents=result_data["relevant_documents"],
        avg_doc_score=result_data["avg_doc_score"],
        max_doc_score=result_data["max_doc_score"],
        tokens_used=result_data["tokens_used"],
        error=result_data.get("error"),
        fallback_used=result_data.get("fallback_used", False)
    )

def get_relevance_summary(state: AgentState) -> Dict[str, Dict[str, Any]]:
    """ëª¨ë“  ì¿¼ë¦¬ì˜ ê´€ë ¨ì„± ìš”ì•½ ì •ë³´ ì¶”ì¶œ

    Args:
        state: Agent state containing evaluation results

    Returns:
        Dict[str, Dict[str, Any]]: Query IDë³„ ê´€ë ¨ì„± ìš”ì•½
    """
    # MetadataManagerë¥¼ í†µí•œ ì•ˆì „í•œ ë©”íƒ€ë°ì´í„° ì ‘ê·¼
    evaluator_data = MetadataManager.safe_get_metadata(state, "quality_evaluator") or {}

    if not evaluator_data.get("success", False):
        return {}

    return evaluator_data.get("relevance_summary", {})

def format_evaluation_results(evaluator_metadata: Dict[str, Any]) -> str:
    """í‰ê°€ ê²°ê³¼ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ í˜•íƒœë¡œ í¬ë§·íŒ…

    Args:
        evaluator_metadata: Metadata from quality_evaluator namespace

    Returns:
        str: Formatted evaluation results string
    """
    if not evaluator_metadata.get("success", False):
        return f"Quality evaluation failed: {evaluator_metadata.get('error', 'Unknown error')}"

    evaluation_results = evaluator_metadata.get("evaluation_results", {})
    evaluation_stats = evaluator_metadata.get("evaluation_stats", {})

    lines = ["=== Quality Evaluation Results ===\n"]

    # ì „ì²´ í†µê³„
    relevant_count = sum(1 for result in evaluation_results.values()
                        if result.get("is_relevant", False))
    total_count = len(evaluation_results)

    lines.append(f"Overall Statistics:")
    lines.append(f"  Relevant Queries: {relevant_count}/{total_count} ({relevant_count/total_count*100:.1f}%)")
    lines.append(f"  Average Relevance Score: {evaluation_stats.get('avg_relevance_score', 0.0):.3f}")
    lines.append(f"  Average Confidence: {evaluation_stats.get('avg_confidence', 0.0):.3f}")
    lines.append(f"  Evaluation Time: {evaluation_stats.get('evaluation_time', 0.0):.2f}s")
    lines.append("")

    # ì¿¼ë¦¬ë³„ ê²°ê³¼
    for qid, result_data in evaluation_results.items():
        query_text = result_data.get("query_text", "")
        is_relevant = result_data.get("is_relevant", False)
        relevance_score = result_data.get("relevance_score", 0.0)
        confidence = result_data.get("confidence", 0.0)
        reasoning = result_data.get("reasoning", "")

        status_emoji = "âœ…" if is_relevant else "âŒ"
        lines.append(f"{status_emoji} Query {qid}: {query_text}")
        lines.append(f"  Relevance: {relevance_score:.3f} (Confidence: {confidence:.3f})")
        lines.append(f"  Reasoning: {reasoning}")
        lines.append("")

    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
    lines.append("=== Performance Metrics ===")
    lines.append(f"Total Tokens Used: {evaluation_stats.get('total_tokens_used', 0)}")
    lines.append(f"API Calls Made: {evaluation_stats.get('api_calls_made', 0)}")
    lines.append(f"Cache Hits: {evaluation_stats.get('cache_hits', 0)}")
    if evaluation_stats.get('fallback_count', 0) > 0:
        lines.append(f"Fallback Evaluations: {evaluation_stats.get('fallback_count', 0)}")

    return "\n".join(lines)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Retry Routing Logic - ë³„ë„ ëª¨ë“ˆì—ì„œ ì„í¬íŠ¸ë¨
# (routing_decision.py, loop_prevention.py ì°¸ì¡°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _update_retry_metadata(state: AgentState, retry_info: Dict[str, Any]) -> None:
    """ì¬ì‹œë„ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (ìƒíƒœ ë³€ê²½ ì—†ì´ ì •ë³´ë§Œ ì €ì¥)"""

    # Note: ì´ í•¨ìˆ˜ëŠ” stateë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ì§€ ì•Šê³ ,
    # ì •ë³´ë§Œ ë¡œê¹…í•©ë‹ˆë‹¤. ì‹¤ì œ state ì—…ë°ì´íŠ¸ëŠ” routeì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    logger.info(f"[_update_retry_metadata] Retry metadata: {retry_info}")

    # í–¥í›„ í•„ìš”ì‹œ temporary storageë‚˜ logging í™•ì¥ ê°€ëŠ¥
    pass

def _ensure_backward_compatibility_fields(result_data: Dict[str, Any]) -> Dict[str, Any]:
    """í‰ê°€ ê²°ê³¼ì— replanner í˜¸í™˜ì„± í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì¶”ê°€

    Args:
        result_data: í‰ê°€ ê²°ê³¼ ë°ì´í„°

    Returns:
        Dict[str, Any]: í˜¸í™˜ì„± í•„ë“œê°€ ì¶”ê°€ëœ ê²°ê³¼ ë°ì´í„°
    """
    if not result_data:
        return result_data

    # ê¸°ì¡´ ê²°ê³¼ë¥¼ ë³µì‚¬
    enhanced_result = result_data.copy()

    # backward compatibility í•„ë“œê°€ ì—†ìœ¼ë©´ ì¶”ê°€
    if "confidence_score" not in enhanced_result:
        enhanced_result["confidence_score"] = enhanced_result.get("confidence", 0.0)

    if "evaluation_details" not in enhanced_result:
        # ìƒˆ í˜•ì‹: ë”•ì…”ë„ˆë¦¬ë¡œ ìƒì„¸ ì •ë³´ ì œê³µ
        enhanced_result["evaluation_details"] = {
            "reasoning": enhanced_result.get("reasoning", ""),
            "improvement_suggestions": enhanced_result.get("improvement_suggestions", ""),
            "query_effectiveness": enhanced_result.get("query_effectiveness", "")
        }

    return enhanced_result

def _merge_evaluation_results(
    existing_results: Dict[str, Any],
    new_results: Dict[str, Any],
    replanner_data: Dict[str, Any]
) -> Dict[str, Any]:
    """ê¸°ì¡´ í‰ê°€ ê²°ê³¼ì™€ ìƒˆë¡œìš´ í‰ê°€ ê²°ê³¼ë¥¼ ë³‘í•© (ì„±ê³µ ë³´ì¡´ ì›ì¹™)

    Args:
        existing_results: ê¸°ì¡´ í‰ê°€ ê²°ê³¼
        new_results: ìƒˆë¡œìš´ í‰ê°€ ê²°ê³¼
        replanner_data: ì¬ê³„íšì ì •ë³´

    Returns:
        Dict[str, Any]: ë³‘í•©ëœ ëˆ„ì  í‰ê°€ ê²°ê³¼
    """

    if not existing_results:
        return new_results.copy()

    if not new_results:
        # ğŸ”§ ê¸°ì¡´ ê²°ê³¼ì—ë„ í˜¸í™˜ì„± í•„ë“œ ì¶”ê°€
        return {qid: _ensure_backward_compatibility_fields(result)
                for qid, result in existing_results.items()}

    # ğŸ”§ ê¸°ì¡´ ê²°ê³¼ì— í˜¸í™˜ì„± í•„ë“œ ì¶”ê°€
    merged_results = {qid: _ensure_backward_compatibility_fields(result)
                     for qid, result in existing_results.items()}

    # ì¬êµ¬ì„±ëœ ì¿¼ë¦¬ ID ëª©ë¡ (ì´ë“¤ì€ ìƒˆë¡œìš´ í‰ê°€ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸)
    reconstructed_query_ids = set()
    if replanner_data.get("status") in ["completed", "queries_consumed"]:
        reconstructions = replanner_data.get("reconstructions", [])
        reconstructed_query_ids = {recon.get("subquery_id", "") for recon in reconstructions if recon.get("subquery_id")}  # ğŸ”§ ì•ˆì „í•œ í‚¤ ì ‘ê·¼

    logger.info(f"[_merge_evaluation_results] Merging results: "
               f"existing={len(existing_results)}, new={len(new_results)}, "
               f"reconstructed={len(reconstructed_query_ids)}")

    # ë³‘í•© ë¡œì§
    for query_id, new_result in new_results.items():
        existing_result = merged_results.get(query_id)

        if query_id in reconstructed_query_ids:
            # ì¬êµ¬ì„±ëœ ì¿¼ë¦¬: í•­ìƒ ìƒˆë¡œìš´ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
            merged_results[query_id] = new_result
            logger.debug(f"[_merge_evaluation_results] Updated reconstructed query: {query_id}")

        elif not existing_result:
            # ê¸°ì¡´ ê²°ê³¼ê°€ ì—†ëŠ” ìƒˆë¡œìš´ ì¿¼ë¦¬: ì¶”ê°€
            merged_results[query_id] = new_result
            logger.debug(f"[_merge_evaluation_results] Added new query: {query_id}")

        else:
            # ê¸°ì¡´ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°: ì„±ê³µ ë³´ì¡´ ì›ì¹™ ì ìš©
            existing_relevant = existing_result.get("is_relevant", False)
            new_relevant = new_result.get("is_relevant", False)

            if existing_relevant and not new_relevant:
                # ê¸°ì¡´ ì„±ê³µ â†’ ìƒˆë¡œ ì‹¤íŒ¨: ê¸°ì¡´ ì„±ê³µ ê²°ê³¼ ë³´ì¡´
                logger.debug(f"[_merge_evaluation_results] Preserved successful result: {query_id}")
                # existing_resultë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì—…ë°ì´íŠ¸ ì•ˆ í•¨)

            elif not existing_relevant and new_relevant:
                # ê¸°ì¡´ ì‹¤íŒ¨ â†’ ìƒˆë¡œ ì„±ê³µ: ìƒˆë¡œìš´ ì„±ê³µ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
                merged_results[query_id] = new_result
                logger.debug(f"[_merge_evaluation_results] Updated to successful result: {query_id}")

            elif existing_relevant and new_relevant:
                # ë‘˜ ë‹¤ ì„±ê³µ: ë” ë†’ì€ ì ìˆ˜ì˜ ê²°ê³¼ ì‚¬ìš©
                existing_score = existing_result.get("relevance_score", 0.0)
                new_score = new_result.get("relevance_score", 0.0)

                if new_score > existing_score:
                    merged_results[query_id] = new_result
                    logger.debug(f"[_merge_evaluation_results] Updated to higher score: {query_id} "
                               f"({existing_score:.3f} â†’ {new_score:.3f})")
                else:
                    logger.debug(f"[_merge_evaluation_results] Kept existing higher score: {query_id}")

            else:
                # ë‘˜ ë‹¤ ì‹¤íŒ¨: ë” ë†’ì€ ì‹ ë¢°ë„ì˜ ê²°ê³¼ ì‚¬ìš©
                existing_confidence = existing_result.get("confidence", 0.0)
                new_confidence = new_result.get("confidence", 0.0)

                if new_confidence > existing_confidence:
                    merged_results[query_id] = new_result
                    logger.debug(f"[_merge_evaluation_results] Updated to higher confidence: {query_id}")
                else:
                    logger.debug(f"[_merge_evaluation_results] Kept existing result: {query_id}")

    logger.info(f"[_merge_evaluation_results] Merge complete: {len(merged_results)} total results")
    return merged_results

def _determine_retry_necessity(
    non_relevant_queries: List[Dict[str, Any]],
    total_queries: int,
    evaluation_stats: Dict[str, Any]  # í–¥í›„ ì‚¬ìš©ì„ ìœ„í•´ ìœ ì§€
) -> Dict[str, Any]:
    """ì¬ì‹œë„ í•„ìš”ì„± íŒë‹¨ ë¡œì§

    Args:
        non_relevant_queries: ê´€ë ¨ì„±ì´ ë‚®ì€ ì¿¼ë¦¬ë“¤
        total_queries: ì „ì²´ ì¿¼ë¦¬ ìˆ˜
        evaluation_stats: í‰ê°€ í†µê³„

    Returns:
        Dict[str, Any]: ì¬ì‹œë„ ê²°ì • ì •ë³´
    """
    if not non_relevant_queries:
        return {
            "should_retry": False,
            "reason": "All queries are relevant",
            "strategy": None
        }

    non_relevant_count = len(non_relevant_queries)
    non_relevant_ratio = non_relevant_count / total_queries

    # ì¬ì‹œë„ ê²°ì • ê¸°ì¤€ë“¤ (ì¤‘ì•™ ì§‘ì¤‘ì‹ ì„¤ì •ì—ì„œ ë¡œë“œ)
    RETRY_THRESHOLD_RATIO = 1.0 - _qe_cfg.relevance_threshold  # ë¹„ê´€ë ¨ ë¹„ìœ¨ ì„ê³„ê°’
    RETRY_MIN_QUERIES = 2  # ìµœì†Œ ë¹„ê´€ë ¨ ì¿¼ë¦¬ ìˆ˜ (ê³ ì •ê°’)
    LOW_CONFIDENCE_THRESHOLD = _qe_cfg.confidence_threshold  # ë‚®ì€ ì‹ ë¢°ë„ ê¸°ì¤€

    # ê¸°ì¤€ 1: ë¹„ê´€ë ¨ ì¿¼ë¦¬ ë¹„ìœ¨ì´ ë†’ì„ ë•Œ
    if non_relevant_ratio >= RETRY_THRESHOLD_RATIO and non_relevant_count >= RETRY_MIN_QUERIES:
        return {
            "should_retry": True,
            "reason": f"High non-relevant ratio: {non_relevant_ratio:.1%} ({non_relevant_count}/{total_queries})",
            "strategy": "regenerate_all_non_relevant"
        }

    # ê¸°ì¤€ 2: ë¹„ê´€ë ¨ ì¿¼ë¦¬ë“¤ì˜ ì‹ ë¢°ë„ê°€ ë‚®ì„ ë•Œ (í‰ê°€ê°€ ë¶ˆí™•ì‹¤)
    low_confidence_queries = [
        q for q in non_relevant_queries
        if q.get("confidence", 1.0) < LOW_CONFIDENCE_THRESHOLD
    ]

    if len(low_confidence_queries) >= 2:
        return {
            "should_retry": True,
            "reason": f"Low confidence in {len(low_confidence_queries)} non-relevant evaluations",
            "strategy": "regenerate_low_confidence"
        }

    # ê¸°ì¤€ 3: í•µì‹¬ ì•µì»¤ ì¿¼ë¦¬(Q1)ê°€ ë¹„ê´€ë ¨ì„±ì¼ ë•Œ
    anchor_non_relevant = any(
        q.get("query_id", "").startswith("Q1") for q in non_relevant_queries
    )

    if anchor_non_relevant:
        return {
            "should_retry": True,
            "reason": "Anchor query (Q1) is non-relevant - critical for search quality",
            "strategy": "regenerate_anchor_focused"
        }

    # ì¬ì‹œë„ ë¶ˆí•„ìš”
    return {
        "should_retry": False,
        "reason": f"Acceptable non-relevant ratio: {non_relevant_ratio:.1%} ({non_relevant_count}/{total_queries})",
        "strategy": None
    }

def get_retry_target_queries(state: AgentState) -> List[Dict[str, Any]]:
    """ì¬ì‹œë„ ëŒ€ìƒ ì¿¼ë¦¬ ëª©ë¡ ë°˜í™˜

    Args:
        state: retry_routing ì •ë³´ê°€ í¬í•¨ëœ agent state

    Returns:
        List[Dict[str, Any]]: ì¬ì‹œë„í•  ì¿¼ë¦¬ë“¤ì˜ ì •ë³´
    """
    metadata = state.get("metadata", {})
    retry_data = metadata.get("retry_routing", {})

    return retry_data.get("retry_queries", [])

def clear_retry_metadata(state: AgentState) -> AgentState:
    """ì¬ì‹œë„ ê´€ë ¨ ë©”íƒ€ë°ì´í„° ì •ë¦¬

    Args:
        state: ì •ë¦¬í•  agent state

    Returns:
        AgentState: ì¬ì‹œë„ ë©”íƒ€ë°ì´í„°ê°€ ì œê±°ëœ state
    """
    metadata = state.get("metadata", {})
    new_metadata = metadata.copy()

    # retry ê´€ë ¨ ë©”íƒ€ë°ì´í„° ì œê±°
    if "retry_routing" in new_metadata:
        del new_metadata["retry_routing"]

    return type(state)(
        messages=state.get("messages", []),
        context_messages=state.get("context_messages", []),
        metadata=new_metadata,
        current_agent=state.get("current_agent", ""),
        session_id=state.get("session_id", "")
    )

# ============================================
# ìš°ì•„í•œ ì¢…ë£Œ ë° ë¶€ë¶„ ê²°ê³¼ ì²˜ë¦¬
# ============================================

def get_loop_prevention_summary(state: AgentState) -> Dict[str, Any]:
    """ë£¨í”„ ë°©ì§€ ì‹œìŠ¤í…œ ìƒíƒœ ìš”ì•½ ë°˜í™˜

    Args:
        state: AgentState

    Returns:
        Dict[str, Any]: ë£¨í”„ ë°©ì§€ ìƒíƒœ ìš”ì•½
    """
    metadata = state.get("metadata", {})
    loop_data = metadata.get("loop_prevention", {})
    last_decision = metadata.get("last_routing_decision", {})

    return {
        "global_iterations": loop_data.get("global_iteration_count", 0),
        "routing_history": loop_data.get("routing_history", [])[-10:],  # ìµœê·¼ 10ê°œ
        "forced_terminations": loop_data.get("forced_terminations", 0),
        "last_route": last_decision.get("route", "unknown"),
        "last_reason": last_decision.get("reason", ""),
        "termination_risk": "high" if loop_data.get("global_iteration_count", 0) >= 20 else "low"
    }

def create_graceful_termination_message(
    state: AgentState,
    termination_reason: str,
    available_data: Dict[str, Any]
) -> str:
    """ìš°ì•„í•œ ì¢…ë£Œ ì‹œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•  ë©”ì‹œì§€ ìƒì„±

    Args:
        state: AgentState
        termination_reason: ì¢…ë£Œ ì´ìœ 
        available_data: ì‚¬ìš© ê°€ëŠ¥í•œ ë¶€ë¶„ ê²°ê³¼ ë°ì´í„°

    Returns:
        str: ì¢…ë£Œ ë©”ì‹œì§€
    """
    loop_summary = get_loop_prevention_summary(state)

    # í‰ê°€ ê²°ê³¼ ìš”ì•½
    metadata = state.get("metadata", {})
    evaluator_data = metadata.get("quality_evaluator", {})
    cumulative_results = evaluator_data.get("cumulative_evaluation_results", {})

    successful_count = sum(
        1 for result in cumulative_results.values()
        if result.get("is_relevant", False)
    )
    total_count = len(cumulative_results)

    message_parts = [
        "ğŸ”„ **Processing Terminated** (Loop Prevention)",
        f"**Reason**: {termination_reason}",
        f"**Iterations**: {loop_summary['global_iterations']}/25",
        "",
        "ğŸ“Š **Available Results**:",
        f"- Successful queries: {successful_count}/{total_count}",
        f"- Success rate: {successful_count/total_count*100:.1f}%" if total_count > 0 else "- No queries processed",
        "",
    ]

    # ì„±ê³µí•œ ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´ ë¶€ë¶„ ê²°ê³¼ ì œê³µ
    if successful_count > 0:
        message_parts.extend([
            "âœ… **Providing partial results based on successful queries**",
            "Note: Answer quality may be limited due to incomplete processing.",
            ""
        ])
    else:
        message_parts.extend([
            "âš ï¸ **No successful queries found**",
            "Unable to provide reliable results. Please try reformulating your question.",
            ""
        ])

    # ë””ë²„ê¹… ì •ë³´ (ì„ íƒì )
    if loop_summary["routing_history"]:
        message_parts.extend([
            f"ğŸ” **Debug Info**: Recent routes: {' â†’ '.join(loop_summary['routing_history'])}",
            ""
        ])

    return "\n".join(message_parts)

def has_sufficient_partial_results(state: AgentState, minimum_success_rate: float = 0.3) -> bool:
    """ë¶€ë¶„ ê²°ê³¼ê°€ ë‹µë³€ ìƒì„±ì— ì¶©ë¶„í•œì§€ í™•ì¸

    Args:
        state: AgentState
        minimum_success_rate: ìµœì†Œ ì„±ê³µë¥  (ê¸°ë³¸ê°’: 30%)

    Returns:
        bool: ë¶€ë¶„ ê²°ê³¼ë¡œ ë‹µë³€ ìƒì„± ê°€ëŠ¥ ì—¬ë¶€
    """
    metadata = state.get("metadata", {})
    evaluator_data = metadata.get("quality_evaluator", {})
    cumulative_results = evaluator_data.get("cumulative_evaluation_results", {})

    if not cumulative_results:
        return False

    successful_count = sum(
        1 for result in cumulative_results.values()
        if result.get("is_relevant", False)
    )
    total_count = len(cumulative_results)

    success_rate = successful_count / total_count if total_count > 0 else 0
    return success_rate >= minimum_success_rate and successful_count >= 1


def _filter_duplicate_queries(
    evaluation_results: Dict[str, Dict[str, Any]]
) -> Dict[str, Dict[str, Any]]:
    """Filter duplicate queries based on 95%+ similarity, keeping highest relevance score.

    Strategy:
    1. Compare all query pairs using SequenceMatcher (95%+ similarity threshold)
    2. Among duplicates, keep query with HIGHER relevance_score
    3. If relevance_score is equal, keep first query (arbitrary but deterministic)
    4. Mark lower-scored duplicates as is_relevant=False with failure reason

    Args:
        evaluation_results: Dict[query_id, result_dict] containing evaluation data

    Returns:
        Dict[query_id, result_dict] with duplicates filtered
    """
    from difflib import SequenceMatcher
    from ...logger.logger import get_logger

    logger = get_logger(__name__)

    if not evaluation_results:
        logger.info("[_filter_duplicate_queries] No results to filter")
        return evaluation_results

    # Step 1: Extract query texts for comparison
    query_texts = {}
    for qid, result in evaluation_results.items():
        query_text = result.get("query_text", "")
        query_texts[qid] = query_text

    # Step 2: Find all duplicate pairs (95%+ similarity)
    duplicate_groups = []
    processed_qids = set()
    qids = sorted(query_texts.keys())  # Deterministic ordering

    for i, qid1 in enumerate(qids):
        if qid1 in processed_qids:
            continue

        group = [qid1]
        text1 = query_texts[qid1]

        for qid2 in qids[i+1:]:
            if qid2 in processed_qids:
                continue

            text2 = query_texts[qid2]

            # Calculate similarity
            similarity = SequenceMatcher(None, text1, text2).ratio()

            if similarity >= 0.95:
                group.append(qid2)
                processed_qids.add(qid2)

        if len(group) > 1:
            duplicate_groups.append(group)
            for qid in group:
                processed_qids.add(qid)

    # Step 3: Process each duplicate group
    total_duplicates_found = 0
    queries_marked_failed = 0

    for group in duplicate_groups:
        logger.info(f"[_filter_duplicate_queries] Found duplicate group: {group}")

        # Extract relevance scores
        scores = {}
        for qid in group:
            result = evaluation_results[qid]
            scores[qid] = result.get("relevance_score", 0.0)

        # Find query with highest relevance score
        max_score = max(scores.values())
        winners = [qid for qid, score in scores.items() if score == max_score]

        # If multiple queries have same max score, keep first (deterministic)
        winner_qid = winners[0]
        losers = [qid for qid in group if qid != winner_qid]

        logger.info(f"[_filter_duplicate_queries] Winner: {winner_qid} (score={max_score:.3f})")
        logger.info(f"[_filter_duplicate_queries] Marking as failed: {losers}")

        # Mark losers as failed
        for loser_qid in losers:
            result = evaluation_results[loser_qid]

            result["is_relevant"] = False
            result["failure_reason"] = (
                f"Duplicate query filtered: similar to {winner_qid} "
                f"with higher relevance score ({max_score:.3f} vs {scores[loser_qid]:.3f})"
            )

            queries_marked_failed += 1

        total_duplicates_found += len(group) - 1

    # Step 4: Logging summary
    logger.info(f"[_filter_duplicate_queries] Filtering complete:")
    logger.info(f"  - Duplicate groups found: {len(duplicate_groups)}")
    logger.info(f"  - Total duplicates detected: {total_duplicates_found}")
    logger.info(f"  - Queries marked as failed: {queries_marked_failed}")
    logger.info(f"  - Queries retained: {len(evaluation_results) - queries_marked_failed}")

    return evaluation_results

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Module Exports
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

__all__ = [
    "quality_evaluator_node",
    "should_retry_routing",
    "get_evaluation_results_for_query",
    "get_relevance_summary",
    "format_evaluation_results",
    "get_retry_target_queries",
    "clear_retry_metadata",
    # Loop prevention functions
    "get_loop_prevention_summary",
    "create_graceful_termination_message",
    "has_sufficient_partial_results"
]