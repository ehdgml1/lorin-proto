"""
í† í° ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ

ë‹¤ì–‘í•œ LLM ëª¨ë¸ì˜ í† í° ê³„ì‚° ë° ì œí•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

í† í°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜„í™©:
- OpenAI (GPT): tiktoken ê³µì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (ì •í™•)
- Google Gemini: ê³µì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ (GPT-4 ê¸°ì¤€ ì¶”ì •)
- Anthropic Claude: ê³µì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ (GPT-4 ê¸°ì¤€ ì¶”ì •)

ì£¼ì˜: Geminiì™€ ClaudeëŠ” ì‹¤ì œ í† í° ìˆ˜ì™€ ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì •í™•í•œ í† í° ìˆ˜ê°€ í•„ìš”í•œ ê²½ìš° ê° ëª¨ë¸ì˜ APIë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì„¸ìš”.
"""

import tiktoken
from ..logger.logger import get_logger

logger = get_logger(__name__)


class ModelLimits:
    """ëª¨ë¸ë³„ í† í° ì œí•œ ì •ë³´"""
    
    # Gemini ëª¨ë¸
    GEMINI_2_5_FLASH = {
        "max_input_tokens": 1_000_000,  # 1M í† í°
        "max_output_tokens": 65_000,    # 65K í† í° (64K + ì—¬ìœ )
        "context_window": 1_000_000,    # 1M í† í°
        "model_name": "gemini-2.5-flash",
        "release_date": "2025-04-17",
        "knowledge_cutoff": "2025-01"
    }
    
    GEMINI_2_5_PRO = {
        "max_input_tokens": 1_000_000,  # 1M í† í° (ê³§ 2Mìœ¼ë¡œ í™•ì¥ ì˜ˆì •)
        "max_output_tokens": 64_000,    # 64K í† í°
        "context_window": 1_000_000,    # 1M í† í° (ê³§ 2Mìœ¼ë¡œ í™•ì¥ ì˜ˆì •)
        "model_name": "gemini-2.5-pro",
        "release_date": "2025-03-25",
        "knowledge_cutoff": "2025-01"
    }
    
    GEMINI_2_0_FLASH = {
        "max_input_tokens": 1_000_000,  # 1M í† í°
        "max_output_tokens": 8_192,     # 8K í† í°
        "context_window": 1_000_000,    # 1M í† í°
        "model_name": "gemini-2.0-flash",
        "release_date": "2024-12-11",
        "knowledge_cutoff": "2024-08"
    }
    
    GEMINI_1_5_FLASH = {
        "max_input_tokens": 1_000_000,  # 1M í† í°
        "max_output_tokens": 8_192,     # 8K í† í°
        "context_window": 1_000_000,    # 1M í† í°
        "model_name": "gemini-1.5-flash",
        "release_date": "2024-05-14",
        "knowledge_cutoff": "2024-08"
    }
    
    GEMINI_1_5_PRO = {
        "max_input_tokens": 1_000_000,  # 1M í† í°
        "max_output_tokens": 8_192,     # 8K í† í°
        "context_window": 1_000_000,    # 1M í† í°
        "model_name": "gemini-1.5-pro",
        "release_date": "2024-02-15",
        "knowledge_cutoff": "2024-08"
    }
    
    # OpenAI ëª¨ë¸
    GPT_4_TURBO = {
        "max_input_tokens": 128_000,
        "max_output_tokens": 4096,
        "context_window": 128_000,
        "model_name": "gpt-4-turbo"
    }
    
    GPT_4O = {
        "max_input_tokens": 128_000,
        "max_output_tokens": 16384,
        "context_window": 128_000,
        "model_name": "gpt-4o"
    }
    
    # Claude ëª¨ë¸
    CLAUDE_3_5_SONNET = {
        "max_input_tokens": 200_000,
        "max_output_tokens": 8192,
        "context_window": 200_000,
        "model_name": "claude-3-5-sonnet-20241022"
    }
    
    @classmethod
    def get_model_limits(cls, model_name: str) -> dict:
        """ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ì œí•œ ì •ë³´ ì¡°íšŒ"""
        model_map = {
            # Gemini 2.5 ëª¨ë¸
            "gemini-2.5-flash": cls.GEMINI_2_5_FLASH,
            "gemini-2.5-flash-exp": cls.GEMINI_2_5_FLASH,
            "gemini-2.5-pro": cls.GEMINI_2_5_PRO,
            "gemini-2.5-pro-exp": cls.GEMINI_2_5_PRO,
            # Gemini 2.0 ëª¨ë¸
            "gemini-2.0-flash": cls.GEMINI_2_0_FLASH,
            "gemini-2.0-flash-exp": cls.GEMINI_2_0_FLASH,
            # Gemini 1.5 ëª¨ë¸
            "gemini-1.5-flash": cls.GEMINI_1_5_FLASH,
            "gemini-1.5-pro": cls.GEMINI_1_5_PRO,
            "gemini-pro": cls.GEMINI_1_5_PRO,  # ê¸°ë³¸ê°’
            # OpenAI ëª¨ë¸
            "gpt-4-turbo": cls.GPT_4_TURBO,
            "gpt-4o": cls.GPT_4O,
            # Claude ëª¨ë¸
            "claude-3-5-sonnet": cls.CLAUDE_3_5_SONNET,
        }
        
        return model_map.get(model_name.lower(), {
            "max_input_tokens": 4096,
            "max_output_tokens": 2048,
            "context_window": 4096,
            "model_name": model_name
        })


class TokenCounter:
    """í† í° ê³„ì‚° ìœ í‹¸ë¦¬í‹°"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        self._encoders = {}
    
    def _get_encoder(self, model_name: str):
        """ëª¨ë¸ì— ë§ëŠ” í† í° ì¸ì½”ë” ê°€ì ¸ì˜¤ê¸°"""
        if model_name in self._encoders:
            return self._encoders[model_name]
        
        try:
            if "gpt" in model_name.lower():
                # OpenAI ëª¨ë¸ - ì •í™•í•œ í† í°í™” ì§€ì›
                if "gpt-4" in model_name.lower():
                    encoder = tiktoken.encoding_for_model("gpt-4")
                else:
                    encoder = tiktoken.encoding_for_model("gpt-3.5-turbo")
            else:
                # Gemini, Claude ë“± - ê³µì‹ í† í¬ë‚˜ì´ì € ì—†ìŒ
                # GPT-4ì™€ ìœ ì‚¬í•œ íŒ¨í„´ìœ¼ë¡œ ê·¼ì‚¬ì¹˜ ê³„ì‚°
                self.logger.info(f"{model_name}ì€ ê³µì‹ í† í¬ë‚˜ì´ì €ê°€ ì—†ì–´ GPT-4 ê¸°ì¤€ìœ¼ë¡œ ì¶”ì •í•©ë‹ˆë‹¤")
                encoder = tiktoken.encoding_for_model("gpt-4")
            
            self._encoders[model_name] = encoder
            
            return encoder
            
        except Exception as e:
            self.logger.warning(f"í† í° ì¸ì½”ë” ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ ì¸ì½”ë” ì‚¬ìš©")
            # ê¸°ë³¸ ì¸ì½”ë” ì‚¬ìš© (cl100k_base = GPT-4 í† í¬ë‚˜ì´ì €)
            encoder = tiktoken.get_encoding("cl100k_base")
            self._encoders[model_name] = encoder

            return encoder
    
    def count_tokens(self, text: str, model_name: str = "gemini-2.5-flash") -> int:
        """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
        try:
            encoder = self._get_encoder(model_name)
            tokens = encoder.encode(text)

            return len(tokens)

        except Exception as e:
            self.logger.error(f"í† í° ê³„ì‚° ì‹¤íŒ¨: {e}")

            # ëŒ€ëµì ì¸ ê³„ì‚° (1 í† í° â‰ˆ 4ê¸€ì)
            return len(text) // 4
    
    def estimate_tokens_simple(self, text: str) -> int:
        """ê°„ë‹¨í•œ í† í° ì¶”ì • (tiktoken ì—†ì´)"""
        # ì˜ì–´: 1 í† í° â‰ˆ 4ê¸€ì
        # í•œêµ­ì–´: 1 í† í° â‰ˆ 1-2ê¸€ì
        korean_chars = sum(1 for char in text if ord(char) >= 0xAC00 and ord(char) <= 0xD7AF)
        other_chars = len(text) - korean_chars
        
        # í•œêµ­ì–´ëŠ” 1.5ê¸€ìë‹¹ 1í† í°, ì˜ì–´ëŠ” 4ê¸€ìë‹¹ 1í† í°ìœ¼ë¡œ ì¶”ì •
        estimated_tokens = korean_chars // 1.5 + other_chars // 4

        return int(estimated_tokens)
    
    def check_token_limits(self, text: str, model_name: str = "gemini-2.5-flash") -> dict:
        """í† í° ì œí•œ í™•ì¸"""
        token_count = self.count_tokens(text, model_name)
        limits = ModelLimits.get_model_limits(model_name)
        
        return {
            "token_count": token_count,
            "model_limits": limits,
            "input_ok": token_count <= limits.get("max_input_tokens", 4096),
            "context_usage_percent": (token_count / limits.get("context_window", 4096)) * 100,
            "remaining_tokens": limits.get("max_input_tokens", 4096) - token_count
        }
    
    def split_text_by_tokens(self, text: str, max_tokens: int, model_name: str = "gemini-2.5-flash") -> list[str]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í° ì œí•œì— ë§ê²Œ ë¶„í• """
        encoder = self._get_encoder(model_name)
        tokens = encoder.encode(text)
        
        chunks = []
        for i in range(0, len(tokens), max_tokens):
            chunk_tokens = tokens[i:i + max_tokens]
            chunk_text = encoder.decode(chunk_tokens)
            chunks.append(chunk_text)
        
        return chunks


def create_token_analyzer():
    """í† í° ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return TokenCounter()


def analyze_text_tokens(text: str, model_name: str = "gemini-2.5-flash") -> dict:
    """í…ìŠ¤íŠ¸ í† í° ë¶„ì„ (í¸ì˜ í•¨ìˆ˜)"""
    counter = TokenCounter()

    return counter.check_token_limits(text, model_name)


def get_gemini_limits(model_name: str = "gemini-2.5-flash") -> dict:
    """Gemini ëª¨ë¸ì˜ ì œí•œ ì •ë³´ ë°˜í™˜"""
    # ëª¨ë¸ëª…ì„ ì •ê·œí™” (ì†Œë¬¸ì, í•˜ì´í”ˆ í†µì¼)
    model_name = model_name.lower().replace("_", "-")
    
    # ëª¨ë¸ë³„ ì œí•œ ì •ë³´ ë§¤í•‘
    model_limits = {
        "gemini-2.5-flash": ModelLimits.GEMINI_2_5_FLASH,
        "gemini-2.5-pro": ModelLimits.GEMINI_2_5_PRO,
        "gemini-2.0-flash": ModelLimits.GEMINI_2_0_FLASH,
        "gemini-1.5-flash": ModelLimits.GEMINI_1_5_FLASH,
        "gemini-1.5-pro": ModelLimits.GEMINI_1_5_PRO,
    }
    
    # ëª¨ë¸ëª…ì´ ì¼ì¹˜í•˜ë©´ í•´ë‹¹ ì œí•œ ì •ë³´ ë°˜í™˜
    for model_key, limits in model_limits.items():
        if model_key in model_name:
            return limits
    
    # ê¸°ë³¸ê°’ìœ¼ë¡œ Gemini 2.5 Flash ë°˜í™˜
    logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” Gemini ëª¨ë¸: {model_name}. ê¸°ë³¸ê°’(Gemini 2.5 Flash) ì‚¬ìš©")

    return ModelLimits.GEMINI_2_5_FLASH


# ì‚¬ìš© ì˜ˆì œ í•¨ìˆ˜ë“¤
def demo_token_counting():
    """í† í° ê³„ì‚° ë°ëª¨"""
    counter = TokenCounter()
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë“¤
    texts = [
        "ì•ˆë…•í•˜ì„¸ìš”! Gemini 2.0 Flash ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
        "Hello! I'm using Gemini 2.0 Flash model for my AI applications.",
        "ì´ê²ƒì€ ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. " * 100,  # ê¸´ í…ìŠ¤íŠ¸
    ]
    
    print("ğŸ” í† í° ê³„ì‚° ë°ëª¨")
    print("=" * 60)
    
    for i, text in enumerate(texts, 1):
        print(f"\n{i}. í…ìŠ¤íŠ¸: {text[:50]}...")
        
        # í† í° ê³„ì‚°
        result = counter.check_token_limits(text, "gemini-2.0-flash")
        
        print(f"   ğŸ“Š í† í° ìˆ˜: {result['token_count']:,}")
        print(f"   ğŸ“ˆ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©ë¥ : {result['context_usage_percent']:.2f}%")
        print(f"   âœ… ì…ë ¥ ê°€ëŠ¥: {'ì˜ˆ' if result['input_ok'] else 'ì•„ë‹ˆì˜¤'}")
        print(f"   ğŸ”„ ë‚¨ì€ í† í°: {result['remaining_tokens']:,}")


if __name__ == "__main__":
    demo_token_counting()