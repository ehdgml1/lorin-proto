# LORIN/agent/replanner.py
"""
Replanner Module - Selective Subquery Reconstruction
====================================================
ì¬ê³„íš ëª¨ë“ˆ: ì‹¤íŒ¨í•œ ì„œë¸Œì¿¼ë¦¬ë§Œ ì„ íƒì ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ëŠ” ì‹œìŠ¤í…œ

ì£¼ìš” ê¸°ëŠ¥:
- ì‹¤íŒ¨í•œ ì„œë¸Œì¿¼ë¦¬ ë¶„ì„ ë° ì‹¤íŒ¨ ì›ì¸ ì§„ë‹¨
- ì„±ê³µí•œ ì„œë¸Œì¿¼ë¦¬ëŠ” ë³´ì¡´í•˜ê³  ì‹¤íŒ¨í•œ ê²ƒë§Œ ì¬êµ¬ì„±
- ì˜ì¡´ì„± ê´€ê³„ ìœ ì§€ ë° ìˆ˜ë ´ ë³´ì¥
- ë¬´í•œ ë£¨í”„ ë°©ì§€ ë° í’ˆì§ˆ ê°œì„ 
"""

import json
import logging
import asyncio
from typing import Dict, List, Optional, Set, Tuple, Any
from dataclasses import dataclass, field

from ...logger.logger import get_logger
from ..state import AgentState
from ..schema import (
    MetadataManager,
    create_replanner_metadata,
    get_subqueries
)
from ...prompt.template_loader import PromptTemplateLoader


@dataclass
class FailureAnalysis:
    """ì‹¤íŒ¨ ë¶„ì„ ê²°ê³¼ - LLMì´ ììœ ë¡­ê²Œ ë¶„ì„"""
    subquery_id: str
    original_query: str
    failure_type: str  # LLMì´ ììœ ë¡­ê²Œ ì‘ì„± (Enum ì œê±°)
    confidence_score: float
    failure_reason: str
    suggested_improvements: List[str]
    context_hints: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "subquery_id": self.subquery_id,
            "original_query": self.original_query,
            "failure_type": self.failure_type,  # ì´ë¯¸ strì´ë¯€ë¡œ .value ë¶ˆí•„ìš”
            "confidence_score": self.confidence_score,
            "failure_reason": self.failure_reason,
            "suggested_improvements": self.suggested_improvements,
            "context_hints": self.context_hints
        }


@dataclass
class ReconstructionResult:
    """ì¬êµ¬ì„± ê²°ê³¼"""
    subquery_id: str
    original_query: str
    reconstructed_query: str
    improvement_rationale: str
    confidence_score: float
    iteration_count: int
    failure_type: Optional[str] = None
    reconstruction_strategy: Optional[str] = None
    strategy_applied: Optional[str] = None
    improvements_applied: Optional[List[str]] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "subquery_id": self.subquery_id,
            "original_query": self.original_query,
            "reconstructed_query": self.reconstructed_query,
            "improvement_rationale": self.improvement_rationale,
            "confidence_score": self.confidence_score,
            "iteration_count": self.iteration_count,
            "failure_type": self.failure_type,
            "reconstruction_strategy": self.reconstruction_strategy,
            "strategy_applied": self.strategy_applied,
            "improvements_applied": self.improvements_applied or []
        }


class FailureAnalyzer:
    """ì‹¤íŒ¨ ë¶„ì„ê¸° - ì‹¤íŒ¨í•œ ì„œë¸Œì¿¼ë¦¬ì˜ ì›ì¸ì„ ë¶„ì„"""

    def __init__(self, chatbot, logger=None):
        self.chatbot = chatbot
        self.logger = logger or get_logger(__name__)

    def _get_user_question(self, state: AgentState) -> str:
        """ìƒíƒœì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ì¶œ"""
        from langchain_core.messages import HumanMessage
        messages = state.get("messages", [])
        for message in reversed(messages):
            if isinstance(message, HumanMessage):
                return message.content
        return "Unknown question"

    async def analyze_failures(
        self,
        state: AgentState,
        failed_subqueries: List[str]
    ) -> List[FailureAnalysis]:
        """ì‹¤íŒ¨í•œ ì„œë¸Œì¿¼ë¦¬ë“¤ì„ ë¶„ì„í•˜ì—¬ ì‹¤íŒ¨ ì›ì¸ì„ ì§„ë‹¨ (ë³‘ë ¬ ì²˜ë¦¬)"""

        # í’ˆì§ˆ í‰ê°€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        quality_data = MetadataManager.safe_get_metadata(state, "quality_evaluator")
        quality_results = quality_data.get("evaluation_results", {}) if quality_data else {}

        planner_data = MetadataManager.safe_get_metadata(state, "planner")
        planner_json = planner_data.get("last_plan_json", {}) if planner_data else {}

        # ë³‘ë ¬ ë¶„ì„ì„ ìœ„í•œ ë‚´ë¶€ í•¨ìˆ˜
        async def analyze_single(subquery_id: str) -> FailureAnalysis:
            """ë‹¨ì¼ ì„œë¸Œì¿¼ë¦¬ ë¶„ì„ (ë³‘ë ¬ ì‹¤í–‰ìš©)"""
            try:
                return await self._analyze_single_failure(
                    subquery_id=subquery_id,
                    state=state,
                    quality_results=quality_results,
                    planner_data=planner_data
                )
            except Exception as e:
                self.logger.error(f"Failed to analyze subquery {subquery_id}: {e}")
                return self._create_default_analysis(subquery_id, state)

        # ğŸ”§ GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ë™ì‹œì„± ì œí•œ (ë³‘ë ¬ ì²˜ë¦¬ ì•ˆì •ì„± í™•ë³´)
        max_concurrent = 5  # ê¸°ë³¸ ë™ì‹œ ì‹¤í–‰ ìˆ˜

        # GPU ë©”ëª¨ë¦¬ ì—¬ìœ  í™•ì¸ ë° ë™ì  ì¡°ì •
        import torch
        if torch.cuda.is_available():
            free_memory_gb = torch.cuda.mem_get_info(0)[0] / 1e9
            if free_memory_gb < 4.0:
                self.logger.warning(f"[replanner] Low GPU memory ({free_memory_gb:.1f}GB), reducing max_concurrent to 2")
                max_concurrent = 2
            elif free_memory_gb < 6.0:
                self.logger.warning(f"[replanner] Moderate GPU memory ({free_memory_gb:.1f}GB), limiting max_concurrent to 3")
                max_concurrent = 3
            self.logger.info(f"[replanner] Parallel analysis with max_concurrent={max_concurrent} (free_memory={free_memory_gb:.1f}GB)")

        # Chunk ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬
        all_analyses = []
        for chunk_idx in range(0, len(failed_subqueries), max_concurrent):
            chunk = failed_subqueries[chunk_idx:chunk_idx + max_concurrent]
            self.logger.info(f"[replanner] Analyzing chunk {chunk_idx//max_concurrent + 1}/{(len(failed_subqueries) + max_concurrent - 1)//max_concurrent} ({len(chunk)} queries)")

            tasks = [analyze_single(subquery_id) for subquery_id in chunk]
            chunk_analyses = await asyncio.gather(*tasks)
            all_analyses.extend(chunk_analyses)

            # ì²­í¬ ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available() and chunk_idx + max_concurrent < len(failed_subqueries):
                torch.cuda.empty_cache()

        return list(all_analyses)

    async def _analyze_single_failure(
        self,
        subquery_id: str,
        state: AgentState,
        quality_results: Dict,
        planner_data: Dict
    ) -> FailureAnalysis:
        """ë‹¨ì¼ ì„œë¸Œì¿¼ë¦¬ ì‹¤íŒ¨ ë¶„ì„"""

        # ì„œë¸Œì¿¼ë¦¬ ì •ë³´ ì¶”ì¶œ
        planner_json = planner_data.get("last_plan_json", {})
        subqueries_list = planner_json.get("subqueries", [])

        # ë¦¬ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (idë¥¼ í‚¤ë¡œ ì‚¬ìš©)
        subqueries = {sq.get("id"): sq for sq in subqueries_list if sq.get("id")}

        subquery_info = subqueries.get(subquery_id, {})
        original_query = subquery_info.get("text", "")

        # í’ˆì§ˆ í‰ê°€ ê²°ê³¼ ì¶”ì¶œ
        quality_info = quality_results.get(subquery_id, {})
        confidence_score = quality_info.get("confidence_score", 0.0)
        evaluation_details = quality_info.get("evaluation_details", "")

        # LLMì„ ì‚¬ìš©í•œ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
        analysis_prompt = self._build_failure_analysis_prompt(
            original_query=original_query,
            confidence_score=confidence_score,
            evaluation_details=evaluation_details,
            user_question=self._get_user_question(state),
            subquery_context=subquery_info
        )

        # ğŸ”„ Retry up to 3 times for LLM call + JSON parsing
        max_attempts = 3
        last_error = None

        for attempt in range(1, max_attempts + 1):
            try:
                self.logger.info(f"[FailureAnalyzer] Attempt {attempt}/{max_attempts}: Analyzing {subquery_id}")

                analysis_response = await self.chatbot.ask(analysis_prompt)

                # ì‘ë‹µ íŒŒì‹±
                failure_analysis = self._parse_analysis_response(
                    subquery_id=subquery_id,
                    original_query=original_query,
                    response=analysis_response,
                    confidence_score=confidence_score
                )

                # JSON íŒŒì‹±ì´ ì„±ê³µí–ˆëŠ”ì§€ í™•ì¸ (failure_analysisê°€ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ê°€ì§€ëŠ”ì§€)
                if failure_analysis and failure_analysis.failure_type != "analysis_unavailable":
                    self.logger.info(f"[FailureAnalyzer] Attempt {attempt}/{max_attempts}: Analysis successful âœ“")
                    return failure_analysis
                else:
                    self.logger.warning(f"[FailureAnalyzer] Attempt {attempt}/{max_attempts}: Got default analysis, retrying...")
                    if attempt < max_attempts:
                        continue

                return failure_analysis

            except Exception as e:
                last_error = str(e)
                self.logger.error(f"[FailureAnalyzer] Attempt {attempt}/{max_attempts}: Failed - {e}")
                if attempt < max_attempts:
                    self.logger.info(f"[FailureAnalyzer] Retrying analysis for {subquery_id}...")
                    continue

        # All attempts failed
        self.logger.error(f"[FailureAnalyzer] âœ— Failed to analyze {subquery_id} after {max_attempts} attempts: {last_error}")
        return self._create_default_analysis(subquery_id, state)

    def _build_failure_analysis_prompt(
        self,
        original_query: str,
        confidence_score: float,
        evaluation_details,  # str ë˜ëŠ” Dict[str, str]
        user_question: str,
        subquery_context: Dict
    ) -> str:
        """ì‹¤íŒ¨ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Jinja2 template ì‚¬ìš©)"""
        template_loader = PromptTemplateLoader()

        # evaluation_detailsê°€ ë”•ì…”ë„ˆë¦¬ì´ë©´ ê·¸ëŒ€ë¡œ ì „ë‹¬, ë¬¸ìì—´ì´ë©´ backward compatibility
        if isinstance(evaluation_details, dict):
            eval_details = evaluation_details
        else:
            # Backward compatibility: ë¬¸ìì—´ì´ë©´ reasoningë§Œ ìˆëŠ” ê²ƒìœ¼ë¡œ ì²˜ë¦¬
            eval_details = {
                "reasoning": evaluation_details,
                "improvement_suggestions": "",
                "query_effectiveness": ""
            }

        return template_loader.render_template(
            "replanner_analysis.j2",
            user_question=user_question,
            subquery_context=subquery_context,
            original_query=original_query,
            confidence_score=confidence_score,
            evaluation_details=eval_details
        )

    def _extract_json_from_response(self, response: str) -> Optional[dict]:
        """plannerì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìì—°ì–´ ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ"""
        if not response:
            return None

        import re

        # 1. ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ ì‹œë„
        json_block_match = re.search(r'```json\s*({.*?})\s*```', response, re.DOTALL)
        if json_block_match:
            try:
                return json.loads(json_block_match.group(1))
            except json.JSONDecodeError:
                pass

        # 2. ì²« ë²ˆì§¸ '{' ë¶€í„° ë§ˆì§€ë§‰ '}' ê¹Œì§€ ì¶”ì¶œ ì‹œë„
        try:
            start = response.find("{")
            end = response.rfind("}")
            if start == -1 or end == -1 or end <= start:
                return None
            blob = response[start : end + 1]
            return json.loads(blob)
        except json.JSONDecodeError:
            pass

        # 3. ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ JSON ê°ì²´ ì°¾ê¸°
        json_match = re.search(r'\{(?:[^{}]|{[^{}]*})*\}', response)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except json.JSONDecodeError:
                pass

        return None

    def _extract_info_with_regex(self, response: str) -> Dict[str, Any]:
        """ì‘ë‹µì—ì„œ êµ¬ì¡°í™”ëœ ë¶„ì„ ì •ë³´ ì¶”ì¶œ - í¸í–¥ ì—†ëŠ” fallback íŒŒì‹±"""
        import re

        # ê¸°ë³¸ê°’: ì¶”ì •í•˜ì§€ ì•Šê³  ì•Œ ìˆ˜ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬
        result = {
            "failure_type": "relevance_too_low",  # ìµœì†Œí•œì˜ ê¸°ë³¸ê°’
            "failure_reason": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ìƒì„¸ ë¶„ì„ ë¶ˆê°€",
            "suggested_improvements": [],
            "context_hints": [],
            "confidence": 0.2  # ë‚®ì€ ì‹ ë¢°ë„ë¡œ ì„¤ì •
        }

        # 1. ì‹¤íŒ¨ ìœ í˜• ì¶”ì¶œ - ì •í™•í•œ ë§¤ì¹­
        failure_type_patterns = [
            r'"failure_type":\s*"([^"]+)"',
            r'failure_type[:\s]*([a-z_]+)',
        ]

        valid_failure_types = ["relevance_too_low", "coverage_insufficient", "specificity_lacking",
                              "context_mismatch", "ambiguity_high", "domain_misalignment"]

        for pattern in failure_type_patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                extracted_type = match.group(1).strip().lower()
                if extracted_type in valid_failure_types:
                    result["failure_type"] = extracted_type
                    result["confidence"] = 0.6  # ì •í™•í•œ ë§¤ì¹­ì‹œ ì‹ ë¢°ë„ í–¥ìƒ
                    break

        # 2. ì‹¤íŒ¨ ì›ì¸ ì¶”ì¶œ - ì •í™•í•œ í•„ë“œëª… ë§¤ì¹­
        reason_patterns = [
            r'"failure_reason":\s*"([^"]+)"',
            r'failure_reason[:\s]*(?:"|\')?([^\n"\']+)(?:"|\')?',
            r'(?:ì‹¤íŒ¨|ì˜¤ë¥˜)\s*(?:ì›ì¸|ì´ìœ )[:\s]*(?:"|\')?([^\n"\']+)(?:"|\')?',
            r'Reason[:\s]*(?:"|\')?([^\n"\']+)(?:"|\')?',
        ]

        for pattern in reason_patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                extracted = match.group(1).strip()
                if len(extracted) >= 5:
                    result["failure_reason"] = extracted
                    break

        # 3. ê°œì„  ì œì•ˆ ì¶”ì¶œ - ì •í™•í•œ êµ¬ì¡° ë§¤ì¹­
        improvements_patterns = [
            r'"suggested_improvements":\s*\[(.*?)\]',
            r'suggested_improvements[:\s]*\[(.*?)\]',
            r'ê°œì„ .*ì œì•ˆ[:\s]*\[(.*?)\]',
        ]

        for pattern in improvements_patterns:
            match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
            if match:
                try:
                    items_text = match.group(1)
                    # JSON ë°°ì—´ íŒŒì‹±
                    items = []
                    for item in re.findall(r'"([^"]+)"', items_text):
                        if len(item.strip()) > 3:
                            items.append(item.strip())
                    if items:
                        result["suggested_improvements"] = items[:5]
                        break
                except:
                    continue

        # 4. ì»¨í…ìŠ¤íŠ¸ íŒíŠ¸ ì¶”ì¶œ - ì •í™•í•œ êµ¬ì¡° ë§¤ì¹­
        hints_patterns = [
            r'"context_hints":\s*\[(.*?)\]',
            r'context_hints[:\s]*\[(.*?)\]',
            r'ì»¨í…ìŠ¤íŠ¸.*íŒíŠ¸[:\s]*\[(.*?)\]',
        ]

        for pattern in hints_patterns:
            match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
            if match:
                try:
                    items_text = match.group(1)
                    # JSON ë°°ì—´ íŒŒì‹±
                    items = []
                    for item in re.findall(r'"([^"]+)"', items_text):
                        if len(item.strip()) > 2:
                            items.append(item.strip())
                    if items:
                        result["context_hints"] = items[:5]
                        break
                except:
                    continue

        # 5. ì‹ ë¢°ë„ ì¶”ì¶œ - ì •í™•í•œ í•„ë“œëª… ë§¤ì¹­
        confidence_patterns = [
            r'"confidence":\s*([0-9]*\.?[0-9]+)',
            r'"confidence_score":\s*([0-9]*\.?[0-9]+)',
            r'confidence[:\s]*([0-9]*\.?[0-9]+)',
            r'ì‹ ë¢°ë„[:\s]*([0-9]*\.?[0-9]+)',
        ]

        for pattern in confidence_patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                try:
                    conf_value = float(match.group(1))
                    if 0.0 <= conf_value <= 1.0:
                        result["confidence"] = conf_value
                        break
                except ValueError:
                    continue

        # 6. ë§ˆì§€ë§‰ fallback: êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ì—ì„œ ìµœì†Œí•œì˜ ì •ë³´ ì¶”ì¶œ
        # ê°œì„  ì œì•ˆì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„
        if not result["suggested_improvements"]:
            improvement_bullets = re.findall(r'[-â€¢]\s*([^\n]{10,100})', response)
            if improvement_bullets:
                # ê°œì„ ê³¼ ê´€ë ¨ëœ ë‚´ìš©ë§Œ í•„í„°ë§
                meaningful_improvements = [
                    item.strip() for item in improvement_bullets
                    if any(keyword in item.lower() for keyword in ['í‚¤ì›Œë“œ', 'ìš©ì–´', 'ê²€ìƒ‰', 'ê°œì„ ', 'ì¡°ì •', 'keyword', 'improve', 'enhance'])
                ]
                if meaningful_improvements:
                    result["suggested_improvements"] = meaningful_improvements[:3]

        # ì»¨í…ìŠ¤íŠ¸ íŒíŠ¸ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„
        if not result["context_hints"]:
            hint_bullets = re.findall(r'[-â€¢]\s*([^\n]{5,80})', response)
            if hint_bullets:
                # AOSP/Android ê´€ë ¨ ë‚´ìš©ë§Œ í•„í„°ë§
                meaningful_hints = [
                    item.strip() for item in hint_bullets
                    if any(keyword in item.lower() for keyword in ['android', 'aosp', 'logcat', 'íƒœê·¸', 'ì‹œê°„', 'í•„í„°', 'activity', 'service'])
                ]
                if meaningful_hints:
                    result["context_hints"] = meaningful_hints[:3]

        return result

    def _parse_analysis_response(
        self,
        subquery_id: str,
        original_query: str,
        response: str,
        confidence_score: float
    ) -> FailureAnalysis:
        """ë¶„ì„ ì‘ë‹µ íŒŒì‹± (plannerì™€ ë™ì¼í•œ ì•ˆì •ì ì¸ ë°©ì‹)"""

        # 1ì°¨: JSON ì¶”ì¶œ ì‹œë„
        analysis_data = self._extract_json_from_response(response)

        if analysis_data is None:
            # 2ì°¨: ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ
            self.logger.info(f"JSON extraction failed for {subquery_id}, using regex extraction")
            analysis_data = self._extract_info_with_regex(response)

        try:
            # LLMì´ ììœ ë¡­ê²Œ ì‘ì„±í•œ failure_type ì‚¬ìš© (Enum ë³€í™˜ ì œê±°)
            failure_type = analysis_data.get("failure_type", "analysis_unavailable")

            return FailureAnalysis(
                subquery_id=subquery_id,
                original_query=original_query,
                failure_type=failure_type,  # str ê·¸ëŒ€ë¡œ ì‚¬ìš©
                confidence_score=analysis_data.get("confidence", 0.5),
                failure_reason=analysis_data.get("failure_reason", "LLM ë¶„ì„ ì‹¤íŒ¨"),
                suggested_improvements=analysis_data.get("suggested_improvements", []),
                context_hints=analysis_data.get("context_hints", [])
            )

        except Exception as e:
            self.logger.error(f"Error creating FailureAnalysis for {subquery_id}: {e}")
            return self._create_default_analysis(subquery_id, {"text": original_query})

    def _create_default_analysis(self, subquery_id: str, subquery_info: Dict[str, Any]) -> FailureAnalysis:
        """ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ìƒì„± - LLM ë¶„ì„ ì‹¤íŒ¨ ì‹œ ìµœì†Œí•œì˜ ì •ë³´ë§Œ ì œê³µ"""

        original_query = subquery_info.get("text", f"ì„œë¸Œì¿¼ë¦¬ {subquery_id}")

        return FailureAnalysis(
            subquery_id=subquery_id,
            original_query=original_query,
            failure_type="llm_analysis_failed",  # í•˜ë“œì½”ë”© ì œê±°, ì‚¬ì‹¤ ê¸°ë°˜ ì •ë³´
            confidence_score=0.0,  # ë¶„ì„ ì‹¤íŒ¨ì´ë¯€ë¡œ ì‹ ë¢°ë„ 0
            failure_reason="LLM ë¶„ì„ ì‹¤íŒ¨ë¡œ ì¸í•´ ìƒì„¸ ë¶„ì„ ë¶ˆê°€",
            suggested_improvements=[],  # í•˜ë“œì½”ë”©ëœ ì œì•ˆ ì œê±°
            context_hints=[]  # í•˜ë“œì½”ë”©ëœ íŒíŠ¸ ì œê±°
        )


class QueryRecomposer:
    """ì¿¼ë¦¬ ì¬êµ¬ì„±ê¸° - ì‹¤íŒ¨ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì„œë¸Œì¿¼ë¦¬ë¥¼ ì¬êµ¬ì„±"""

    def __init__(self, chatbot, logger=None):
        self.chatbot = chatbot
        self.logger = logger or get_logger(__name__)

    def _extract_json_from_response(self, response: str) -> Optional[dict]:
        """LLM ì‘ë‹µì—ì„œ ì •í™•í•œ JSON ì¶”ì¶œ - í¸í–¥ ì—†ëŠ” íŒŒì‹±"""
        if not response:
            return None

        import re
        import json

        # 1. ê°€ì¥ ì¼ë°˜ì ì¸ íŒ¨í„´: ```jsonê³¼ ``` ì‚¬ì´ì˜ JSON
        json_code_patterns = [
            r'```json\s*(\{[\s\S]*?\})\s*```',
            r'```\s*(\{[\s\S]*?\})\s*```',
            r'```json\s*(\[[\s\S]*?\])\s*```',  # ë°°ì—´ í˜•íƒœë„ ì§€ì›
        ]

        for pattern in json_code_patterns:
            matches = re.finditer(pattern, response, re.MULTILINE)
            for match in matches:
                json_candidate = match.group(1)
                try:
                    return json.loads(json_candidate)
                except json.JSONDecodeError:
                    # JSON ë¬¸ë²• ì˜¤ë¥˜ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì •ë¦¬ ì‹œë„
                    cleaned = self._clean_json_string(json_candidate)
                    if cleaned:
                        try:
                            return json.loads(cleaned)
                        except json.JSONDecodeError:
                            continue

        # 2. ê· í˜•ì¡íŒ ì¤‘ê´„í˜¸ ë¸”ë¡ ì°¾ê¸° (ê°€ì¥ ì™„ì „í•œ JSON ìš°ì„ )
        json_candidates = self._find_balanced_json_blocks(response)

        # í¬ê¸° ìˆœìœ¼ë¡œ ì •ë ¬ (í° ê²ƒë¶€í„° ì‹œë„)
        json_candidates.sort(key=len, reverse=True)

        for candidate in json_candidates:
            try:
                parsed = json.loads(candidate)
                # ìœ íš¨í•œ êµ¬ì¡°ì¸ì§€ ê²€ì¦
                if self._is_valid_response_structure(parsed):
                    return parsed
            except json.JSONDecodeError:
                # JSON ì •ë¦¬ í›„ ì¬ì‹œë„
                cleaned = self._clean_json_string(candidate)
                if cleaned:
                    try:
                        parsed = json.loads(cleaned)
                        if self._is_valid_response_structure(parsed):
                            return parsed
                    except json.JSONDecodeError:
                        continue

        return None

    def _clean_json_string(self, json_str: str) -> Optional[str]:
        """JSON ë¬¸ìì—´ ì •ë¦¬ - ì¼ë°˜ì ì¸ LLM ì¶œë ¥ ë¬¸ì œ í•´ê²°"""
        import re

        if not json_str:
            return None

        # ì•ë’¤ ê³µë°± ì œê±°
        cleaned = json_str.strip()

        # ì¼ë°˜ì ì¸ ë§ˆí¬ë‹¤ìš´ ì•„í‹°íŒ©íŠ¸ ì œê±°
        cleaned = re.sub(r'^```[a-z]*\n?', '', cleaned)
        cleaned = re.sub(r'\n?```$', '', cleaned)

        # ì£¼ì„ ì œê±° (// ë˜ëŠ” /* */)
        cleaned = re.sub(r'//.*?$', '', cleaned, flags=re.MULTILINE)
        cleaned = re.sub(r'/\*.*?\*/', '', cleaned, flags=re.DOTALL)

        # í›„í–‰ ì‰¼í‘œ ì œê±°
        cleaned = re.sub(r',(\s*[}\]])', r'\1', cleaned)

        # ì˜ëª»ëœ ë”°ì˜´í‘œ ìˆ˜ì • (ê³±ìŠ¬ë”°ì˜´í‘œ ë“±)
        cleaned = cleaned.replace('"', '"').replace('"', '"')
        cleaned = cleaned.replace(''', "'").replace(''', "'")

        return cleaned.strip()

    def _find_balanced_json_blocks(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê· í˜•ì¡íŒ JSON ë¸”ë¡ë“¤ ì°¾ê¸°"""
        import re

        candidates = []

        # ëª¨ë“  { ìœ„ì¹˜ ì°¾ê¸°
        for match in re.finditer(r'\{', text):
            start_pos = match.start()
            brace_count = 0
            in_string = False
            escape_next = False

            for i, char in enumerate(text[start_pos:], start_pos):
                if escape_next:
                    escape_next = False
                    continue

                if char == '\\':
                    escape_next = True
                    continue

                if char == '"' and not escape_next:
                    in_string = not in_string
                    continue

                if not in_string:
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            candidate = text[start_pos:i+1]
                            if len(candidate) > 10:  # ìµœì†Œ í¬ê¸° í•„í„°
                                candidates.append(candidate)
                            break

        return candidates

    def _is_valid_response_structure(self, parsed_json: dict) -> bool:
        """ì‘ë‹µ êµ¬ì¡°ê°€ ìœ íš¨í•œì§€ ê²€ì¦"""
        if not isinstance(parsed_json, dict):
            return False

        # reconstruction ì‘ë‹µ êµ¬ì¡° ê²€ì¦ (ìƒˆë¡œìš´ í…œí”Œë¦¿ ê¸°ì¤€)
        if "reconstructed_queries" in parsed_json:
            queries = parsed_json["reconstructed_queries"]
            if isinstance(queries, list) and len(queries) > 0:
                # ì²« ë²ˆì§¸ í•­ëª©ì´ ì˜¬ë°”ë¥¸ êµ¬ì¡°ì¸ì§€ í™•ì¸
                first_query = queries[0]
                if isinstance(first_query, dict):
                    # ìƒˆë¡œìš´ í…œí”Œë¦¿ í•„ë“œ í™•ì¸
                    required_fields = ["original_id", "reconstructed_text"]
                    has_required = all(field in first_query for field in required_fields)

                    # í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦
                    reconstructed_text = first_query.get("reconstructed_text", "")
                    has_placeholders = any(placeholder in reconstructed_text for placeholder in [
                        "WRITE_IMPROVED_QUERY_HERE",
                        "APPLY_STRATEGY_SPECIFIC_IMPROVEMENTS_HERE",
                        "EXPLAIN_WHY_BETTER"
                    ])

                    return has_required and not has_placeholders

        # analysis ì‘ë‹µ êµ¬ì¡° ê²€ì¦
        required_analysis_fields = ["failure_type", "failure_reason", "suggested_improvements"]
        if any(field in parsed_json for field in required_analysis_fields):
            return True

        # ê¸°ë³¸ì ì¸ JSON ê°ì²´ë¼ë©´ í—ˆìš©
        return len(parsed_json) > 0

    def _get_user_question(self, state: AgentState) -> str:
        """ìƒíƒœì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ì¶œ"""
        from langchain_core.messages import HumanMessage
        messages = state.get("messages", [])
        for message in reversed(messages):
            if isinstance(message, HumanMessage):
                return message.content
        return "Unknown question"

    async def reconstruct_queries(
        self,
        state: AgentState,
        failure_analyses: List[FailureAnalysis],
        iteration_count: int = 1
    ) -> List[ReconstructionResult]:
        """ì‹¤íŒ¨ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì„œë¸Œì¿¼ë¦¬ë“¤ì„ ì¬êµ¬ì„±"""

        reconstructions = []

        # ì„±ê³µí•œ ì„œë¸Œì¿¼ë¦¬ ì •ë³´ ìˆ˜ì§‘ (ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©)
        successful_context = self._collect_successful_context(state)
        self.logger.info(f"[replanner] Using {len(successful_context.get('successful_queries', []))} successful queries as context")

        for analysis in failure_analyses:
            try:
                reconstruction = await self._reconstruct_single_query(
                    analysis=analysis,
                    state=state,
                    successful_context=successful_context,
                    iteration_count=iteration_count
                )
                reconstructions.append(reconstruction)

            except Exception as e:
                self.logger.error(f"Failed to reconstruct query {analysis.subquery_id}: {e}")
                # ê¸°ë³¸ ì¬êµ¬ì„± ê²°ê³¼ ìƒì„±
                reconstructions.append(self._create_default_reconstruction(analysis, iteration_count))

        return reconstructions

    def _collect_successful_context(self, state: AgentState) -> Dict[str, Any]:
        """ì„±ê³µí•œ ì„œë¸Œì¿¼ë¦¬ë“¤ì˜ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ìˆ˜ì§‘"""

        quality_data = MetadataManager.safe_get_metadata(state, "quality_evaluator")
        quality_results = quality_data.get("evaluation_results", {}) if quality_data else {}

        planner_data = MetadataManager.safe_get_metadata(state, "planner")
        planner_json = planner_data.get("last_plan_json", {}) if planner_data else {}

        successful_queries = []
        successful_patterns = []

        # ë¦¬ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        subqueries_list = planner_json.get("subqueries", [])
        subqueries_dict = {sq.get("id"): sq for sq in subqueries_list if sq.get("id")}

        for subquery_id, quality_info in quality_results.items():
            if quality_info.get("is_relevant", False):
                subquery_info = subqueries_dict.get(subquery_id, {})
                if subquery_info:
                    successful_queries.append({
                        "id": subquery_id,
                        "question": subquery_info.get("text", ""),
                        "score": quality_info.get("confidence_score", 0.0)
                    })

        # ì„±ê³µ íŒ¨í„´ ì¶”ì¶œ
        if successful_queries:
            high_scoring = [q for q in successful_queries if q["score"] > 0.7]
            successful_patterns = [q["question"] for q in high_scoring[:3]]

        return {
            "successful_queries": successful_queries,
            "successful_patterns": successful_patterns,
            "total_successful": len(successful_queries)
        }

    async def _reconstruct_single_query(
        self,
        analysis: FailureAnalysis,
        state: AgentState,
        successful_context: Dict[str, Any],
        iteration_count: int
    ) -> ReconstructionResult:
        """ë‹¨ì¼ ì„œë¸Œì¿¼ë¦¬ ì¬êµ¬ì„±"""

        # ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        reconstruction_prompt = self._build_reconstruction_prompt(
            analysis=analysis,
            user_question=self._get_user_question(state),
            successful_context=successful_context,
            iteration_count=iteration_count
        )

        # ğŸ”„ Retry up to 3 times for LLM call + JSON parsing
        max_attempts = 3
        last_error = None

        for attempt in range(1, max_attempts + 1):
            try:
                self.logger.info(f"[QueryRecomposer] Attempt {attempt}/{max_attempts}: Reconstructing {analysis.subquery_id}")

                reconstruction_response = await self.chatbot.ask(reconstruction_prompt)

                # ì‘ë‹µ íŒŒì‹±
                reconstruction_result = self._parse_reconstruction_response(
                    analysis=analysis,
                    response=reconstruction_response,
                    iteration_count=iteration_count
                )

                # JSON íŒŒì‹±ì´ ì„±ê³µí–ˆëŠ”ì§€ í™•ì¸ (ìœ íš¨í•œ ì¬êµ¬ì„± ì¿¼ë¦¬ê°€ ìˆëŠ”ì§€)
                if reconstruction_result and reconstruction_result.reconstructed_query != analysis.original_query:
                    self.logger.info(f"[QueryRecomposer] Attempt {attempt}/{max_attempts}: Reconstruction successful âœ“")
                    return reconstruction_result
                else:
                    self.logger.warning(f"[QueryRecomposer] Attempt {attempt}/{max_attempts}: Got same query, retrying...")
                    if attempt < max_attempts:
                        continue

                return reconstruction_result

            except Exception as e:
                last_error = str(e)
                self.logger.error(f"[QueryRecomposer] Attempt {attempt}/{max_attempts}: Failed - {e}")
                if attempt < max_attempts:
                    self.logger.info(f"[QueryRecomposer] Retrying reconstruction for {analysis.subquery_id}...")
                    continue

        # All attempts failed
        self.logger.error(f"[QueryRecomposer] âœ— Failed to reconstruct {analysis.subquery_id} after {max_attempts} attempts: {last_error}")
        return self._create_default_reconstruction(analysis, iteration_count)

    def _build_reconstruction_prompt(
        self,
        analysis: FailureAnalysis,
        user_question: str,
        successful_context: Dict[str, Any],
        iteration_count: int
    ) -> str:
        """ì¬êµ¬ì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Jinja2 template ì‚¬ìš©)"""
        template_loader = PromptTemplateLoader()

        # Convert single analysis to list format expected by template
        failure_analyses = [{
            "subquery_id": analysis.subquery_id,
            "original_query": analysis.original_query,
            "failure_type": analysis.failure_type,  # ì´ë¯¸ str
            "failure_reason": analysis.failure_reason,
            "suggested_improvements": analysis.suggested_improvements
        }]

        return template_loader.render_template(
            "replanner_reconstruction.j2",
            target_query=analysis.original_query,
            user_question=user_question,
            successful_context=successful_context,
            failure_analyses=failure_analyses
        )

    def _extract_reconstruction_info_with_regex(self, response: str, original_query: str) -> Dict[str, Any]:
        """ì‘ë‹µì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ - í¸í–¥ ì—†ëŠ” fallback íŒŒì‹±"""
        import re

        # ê¸°ë³¸ê°’: ì›ë³¸ ì¿¼ë¦¬ ìœ ì§€ (í¸í–¥ ì—†ìŒ)
        result = {
            "reconstructed_query": original_query,
            "improvement_rationale": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ì›ë³¸ ì¿¼ë¦¬ ìœ ì§€",
            "confidence": 0.3,
            "key_changes": [],
            "failure_type": None,
            "reconstruction_strategy": None,
            "strategy_applied": None,
            "improvements_applied": []
        }

        # ğŸ”§ í”Œë ˆì´ìŠ¤í™€ë” í…ìŠ¤íŠ¸ ê°ì§€ ë° ì²˜ë¦¬ (í™•ì¥ë¨)
        placeholder_patterns = [
            r'WRITE_YOUR_IMPROVED_QUERY_HERE',
            r'EXPLAIN_YOUR_IMPROVEMENTS_HERE',
            r'\[WRITE_IMPROVED_QUERY_HERE\]',
            r'APPLY_STRATEGY_SPECIFIC_IMPROVEMENTS_HERE',
            r'\[EXPLAIN_WHY_BETTER\]',
            r'STRATEGY_SPECIFIC_IMPROVEMENT_\d+',
            r'YOUR_SEMANTIC_KEYWORD_SEQUENCE',  # í…œí”Œë¦¿ í”Œë ˆì´ìŠ¤í™€ë” ì¶”ê°€
            r'YOUR_EXPLANATION',                # í…œí”Œë¦¿ í”Œë ˆì´ìŠ¤í™€ë” ì¶”ê°€
            r'YOUR_.*?_HERE',                   # í¬ê´„ì  YOUR_*_HERE íŒ¨í„´
            r'WRITE_.*?_HERE',                  # í¬ê´„ì  WRITE_*_HERE íŒ¨í„´
            r'\[YOUR_.*?\]',                    # ëŒ€ê´„í˜¸ í¬í•¨ YOUR íŒ¨í„´
        ]

        for pattern in placeholder_patterns:
            if re.search(pattern, response, re.IGNORECASE):
                self.logger.warning(f"Detected placeholder in LLM response: {pattern}")
                result["improvement_rationale"] = "LLM ì‘ë‹µì— í”Œë ˆì´ìŠ¤í™€ë” í¬í•¨, ì›ë³¸ ì¿¼ë¦¬ ìœ ì§€"
                result["confidence"] = 0.1  # ë§¤ìš° ë‚®ì€ ì‹ ë¢°ë„
                return result

        # 1. ì¬êµ¬ì„±ëœ ì¿¼ë¦¬ ì¶”ì¶œ - ìƒˆë¡œìš´ í…œí”Œë¦¿ í•„ë“œ ì§€ì›
        query_extraction_patterns = [
            # ìƒˆë¡œìš´ í…œí”Œë¦¿ í•„ë“œ (ìš°ì„ ìˆœìœ„)
            r'"reconstructed_text":\s*"([^"]+)"',
            # í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ í•„ë“œ
            r'"reconstructed_query":\s*"([^"]+)"',
            # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ íŒ¨í„´
            r'(?:ì¬êµ¬ì„±ëœ?|ê°œì„ ëœ?|ìƒˆë¡œìš´)\s*(?:ì¿¼ë¦¬|ì§ˆë¬¸)[:\s]*(?:"|\')?([^\n"\']+)(?:"|\')?',
            r'Reconstructed[:\s]*(?:"|\')?([^\n"\']+)(?:"|\')?',
            r'Improved[:\s]*(?:"|\')?([^\n"\']+)(?:"|\')?',
        ]

        for pattern in query_extraction_patterns:
            matches = re.finditer(pattern, response, re.IGNORECASE)
            for match in matches:
                extracted = match.group(1).strip()
                # ìœ íš¨ì„± ê²€ì¦: ìµœì†Œ ê¸¸ì´ì™€ ì›ë³¸ê³¼ ë‹¤ë¥¸ì§€ í™•ì¸
                if len(extracted) >= 5 and extracted != original_query:
                    result["reconstructed_query"] = extracted
                    result["confidence"] = 0.7  # ì‹¤ì œ ì¶”ì¶œ ì„±ê³µì‹œ ì‹ ë¢°ë„ í–¥ìƒ
                    break
            if result["reconstructed_query"] != original_query:
                break

        # 2. ê°œì„  ë…¼ë¦¬ ì¶”ì¶œ - ì •í™•í•œ í•„ë“œëª… ë§¤ì¹­
        rationale_patterns = [
            r'"improvement_rationale":\s*"([^"]+)"',
            r'"rationale":\s*"([^"]+)"',
            r'(?:ê°œì„ |í–¥ìƒ)\s*(?:ë…¼ë¦¬|ì´ìœ |ê·¼ê±°)[:\s]*(?:"|\')?([^\n"\']+)(?:"|\')?',
            r'Rationale[:\s]*(?:"|\')?([^\n"\']+)(?:"|\')?',
        ]

        for pattern in rationale_patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                extracted = match.group(1).strip()
                if len(extracted) >= 5:
                    result["improvement_rationale"] = extracted
                    break

        # 3. ì‹ ë¢°ë„ ì¶”ì¶œ - ì •í™•í•œ í•„ë“œëª… ë§¤ì¹­
        confidence_patterns = [
            r'"confidence":\s*([0-9]*\.?[0-9]+)',
            r'confidence[:\s]*([0-9]*\.?[0-9]+)',
            r'ì‹ ë¢°ë„[:\s]*([0-9]*\.?[0-9]+)',
        ]

        for pattern in confidence_patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                try:
                    conf_value = float(match.group(1))
                    if 0.0 <= conf_value <= 1.0:
                        result["confidence"] = conf_value
                        break
                except ValueError:
                    continue

        # 4. ê°œì„ ì‚¬í•­ ì¶”ì¶œ - ì •í™•í•œ êµ¬ì¡° ë§¤ì¹­
        improvements_patterns = [
            r'"improvements_applied":\s*\[(.*?)\]',
            r'"key_changes":\s*\[(.*?)\]',
            r'ê°œì„ .*ì‚¬í•­[:\s]*\[(.*?)\]',
        ]

        for pattern in improvements_patterns:
            match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
            if match:
                try:
                    items_text = match.group(1)
                    # ê°„ë‹¨í•œ ë°°ì—´ íŒŒì‹±
                    items = []
                    for item in re.findall(r'"([^"]+)"', items_text):
                        if len(item.strip()) > 2:
                            items.append(item.strip())
                    if items:
                        result["key_changes"] = items[:3]
                        break
                except:
                    continue

        # 5. ìƒˆë¡œìš´ í•„ë“œë“¤ ì¶”ì¶œ
        # failure_type ì¶”ì¶œ
        failure_type_patterns = [
            r'"failure_type":\s*"([^"]+)"',
            r'failure_type[:\s]*([a-z_]+)',
        ]

        for pattern in failure_type_patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                extracted = match.group(1).strip().strip('"\'')
                if extracted in ["relevance_too_low", "coverage_insufficient", "specificity_lacking",
                               "context_mismatch", "ambiguity_high", "domain_misalignment"]:
                    result["failure_type"] = extracted
                    break

        # reconstruction_strategy ì¶”ì¶œ
        strategy_patterns = [
            r'"reconstruction_strategy":\s*"([^"]+)"',
            r'reconstruction_strategy[:\s]*([a-z_]+)',
        ]

        for pattern in strategy_patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                extracted = match.group(1).strip().strip('"\'')
                if extracted in ["keyword_enhancement", "scope_expansion", "precision_increase",
                               "context_realignment", "disambiguation", "domain_realignment", "general_enhancement"]:
                    result["reconstruction_strategy"] = extracted
                    break

        # strategy_applied ì¶”ì¶œ
        strategy_applied_patterns = [
            r'"strategy_applied":\s*"([^"]+)"',
            r'strategy_applied[:\s]*(?:"|\')?([^\n"\']+)(?:"|\')?',
        ]

        for pattern in strategy_applied_patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                extracted = match.group(1).strip().strip('"\'')
                if len(extracted) >= 10:
                    result["strategy_applied"] = extracted
                    break

        # improvements_applied ë°°ì—´ ì¶”ì¶œ
        if not result["improvements_applied"]:
            improvements_array_patterns = [
                r'"improvements_applied":\s*\[(.*?)\]',
                r'improvements_applied[:\s]*\[(.*?)\]',
            ]

            for pattern in improvements_array_patterns:
                match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
                if match:
                    try:
                        items_text = match.group(1)
                        items = [item.strip().strip('"\'') for item in re.findall(r'"([^"]+)"', items_text)]
                        if items and all(len(item) > 3 for item in items):
                            result["improvements_applied"] = items[:3]
                            break
                    except:
                        continue

        # 6. ë§ˆì§€ë§‰ fallback: êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ì—ì„œ ë³€ê²½ì‚¬í•­ ì¶”ì¶œ
        if not result["key_changes"]:
            changes = re.findall(r'[-â€¢]\s*([^\n]{5,80})', response)
            if changes:
                # ì˜ë¯¸ìˆëŠ” ë³€ê²½ì‚¬í•­ë§Œ í•„í„°ë§
                meaningful_changes = [
                    change.strip() for change in changes
                    if any(keyword in change.lower() for keyword in ['ì¶”ê°€', 'ê°œì„ ', 'ë³€ê²½', 'add', 'improve', 'enhance'])
                ]
                if meaningful_changes:
                    result["key_changes"] = meaningful_changes[:3]

        # ê¸°ì¡´ key_changesë¥¼ improvements_appliedë¡œ ë³µì‚¬ (í˜¸í™˜ì„±)
        if not result["improvements_applied"] and result["key_changes"]:
            result["improvements_applied"] = result["key_changes"]

        return result

    def _parse_reconstruction_response(
        self,
        analysis: FailureAnalysis,
        response: str,
        iteration_count: int
    ) -> ReconstructionResult:
        """ì¬êµ¬ì„± ì‘ë‹µ íŒŒì‹± (plannerì™€ ë™ì¼í•œ ì•ˆì •ì ì¸ ë°©ì‹)"""

        # ë””ë²„ê¹…: LLM ì‘ë‹µ ë¡œê¹…
        self.logger.info(
            f"ğŸ¤– [{analysis.subquery_id}] LLM Reconstruction Response:\n"
            f"  Original Query: '{analysis.original_query}'\n"
            f"  LLM Response (first 500 chars): {response[:500]}"
        )

        # 1ì°¨: JSON ì¶”ì¶œ ì‹œë„
        reconstruction_data = self._extract_json_from_response(response)

        if reconstruction_data is None:
            # 2ì°¨: ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ
            self.logger.info(f"JSON extraction failed for reconstruction {analysis.subquery_id}, using regex extraction")
            reconstruction_data = self._extract_reconstruction_info_with_regex(response, analysis.original_query)
        else:
            # JSON íŒŒì‹± ì„±ê³µ - ë°°ì—´ êµ¬ì¡° ì²˜ë¦¬
            if "reconstructed_queries" in reconstruction_data:
                # LLMì´ ë°°ì—´ í˜•íƒœë¡œ ë°˜í™˜í•œ ê²½ìš° ì²« ë²ˆì§¸ í•­ëª© ì¶”ì¶œ
                queries_array = reconstruction_data.get("reconstructed_queries", [])
                if queries_array and isinstance(queries_array, list) and len(queries_array) > 0:
                    reconstruction_data = queries_array[0]
                    self.logger.info(f"âœ… [{analysis.subquery_id}] Extracted first item from reconstructed_queries array")

        # ë””ë²„ê¹…: íŒŒì‹± ê²°ê³¼ ë¡œê¹…
        reconstructed_text = reconstruction_data.get("reconstructed_text") or reconstruction_data.get("reconstructed_query", "")
        self.logger.info(
            f"ğŸ“ [{analysis.subquery_id}] Parsed Reconstruction Data:\n"
            f"  reconstructed_text: '{reconstructed_text}'\n"
            f"  Original for comparison: '{analysis.original_query}'\n"
            f"  Are they same? {reconstructed_text == analysis.original_query}"
        )

        try:
            return ReconstructionResult(
                subquery_id=analysis.subquery_id,
                original_query=analysis.original_query,
                reconstructed_query=reconstructed_text or analysis.original_query,
                improvement_rationale=reconstruction_data.get("improvement_rationale", "ì¬êµ¬ì„± ë…¼ë¦¬ ëˆ„ë½"),
                confidence_score=reconstruction_data.get("confidence", 0.5),
                iteration_count=iteration_count,
                failure_type=reconstruction_data.get("failure_type", analysis.failure_type),  # ì´ë¯¸ str
                reconstruction_strategy=reconstruction_data.get("reconstruction_strategy"),
                strategy_applied=reconstruction_data.get("strategy_applied"),
                improvements_applied=reconstruction_data.get("improvements_applied", [])
            )

        except Exception as e:
            self.logger.error(f"Error creating ReconstructionResult for {analysis.subquery_id}: {e}")
            return self._create_default_reconstruction(analysis, iteration_count)

    def _create_default_reconstruction(
        self,
        analysis: FailureAnalysis,
        iteration_count: int
    ) -> ReconstructionResult:
        """ê¸°ë³¸ ì¬êµ¬ì„± ê²°ê³¼ ìƒì„±"""

        # ì›ë³¸ ì¿¼ë¦¬ ìœ ì§€ (í¸í–¥ ì—†ëŠ” ê¸°ë³¸ê°’)
        original = analysis.original_query

        return ReconstructionResult(
            subquery_id=analysis.subquery_id,
            original_query=original,
            reconstructed_query=original,  # í¸í–¥ ì œê±°: ì›ë³¸ ìœ ì§€
            improvement_rationale="ì¬êµ¬ì„± ì‹¤íŒ¨ë¡œ ì›ë³¸ ì¿¼ë¦¬ ìœ ì§€",
            confidence_score=0.2,  # ë‚®ì€ ì‹ ë¢°ë„
            iteration_count=iteration_count,
            failure_type=analysis.failure_type,  # ì´ë¯¸ str
            reconstruction_strategy="general_enhancement",
            strategy_applied="ê¸°ë³¸ ì „ëµ - ì¬êµ¬ì„± ì‹¤íŒ¨",
            improvements_applied=[]
        )


class ConvergenceDetector:
    """ìˆ˜ë ´ ê°ì§€ê¸° - ë¬´í•œ ë£¨í”„ ë°©ì§€ ë° ìˆ˜ë ´ ë³´ì¥"""

    def __init__(self, max_iterations: int = 25, min_improvement_threshold: float = 0.01):
        self.max_iterations = max_iterations
        self.min_improvement_threshold = min_improvement_threshold
        self.logger = get_logger(__name__)

    def should_continue_iteration(
        self,
        current_iteration: int,
        iteration_history: List[Dict[str, Any]]
    ) -> Tuple[bool, str]:
        """ë°˜ë³µì„ ê³„ì†í• ì§€ ê²°ì •"""

        # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ í™•ì¸
        if current_iteration >= self.max_iterations:
            return False, f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ {self.max_iterations}ì— ë„ë‹¬"

        # ì´ì „ ë°˜ë³µê³¼ì˜ ê°œì„ ë„ í™•ì¸ (ì •ë³´ ë¡œê¹…ë§Œ, ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ)
        if len(iteration_history) >= 2:
            prev_scores = iteration_history[-2].get("average_score", 0.0)
            curr_scores = iteration_history[-1].get("average_score", 0.0)
            improvement = curr_scores - prev_scores

            # ğŸ”§ ê°œì„ ë„ê°€ ë‚®ì•„ë„ í’ˆì§ˆ í†µê³¼í•  ë•Œê¹Œì§€ ê³„ì† ì§„í–‰
            if improvement < self.min_improvement_threshold:
                self.logger.info(f"ê°œì„ ë„ {improvement:.3f}ì´ ì„ê³„ê°’ {self.min_improvement_threshold} ë¯¸ë§Œì´ì§€ë§Œ ê³„ì† ì§„í–‰")
                # return False ì œê±° - ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ

        # ì—°ì† ì‹¤íŒ¨ í™•ì¸ (ë§¤ìš° ê´€ëŒ€í•˜ê²Œ)
        if len(iteration_history) >= 12:
            recent_failures = [
                hist.get("failed_count", 0)
                for hist in iteration_history[-12:]
            ]
            if all(count > 0 for count in recent_failures):
                return False, "12íšŒ ì—°ì† ì‹¤íŒ¨ ì„œë¸Œì¿¼ë¦¬ ì¡´ì¬"

        return True, "ë°˜ë³µ ê³„ì† ê°€ëŠ¥"

    def analyze_convergence_pattern(
        self,
        iteration_history: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ìˆ˜ë ´ íŒ¨í„´ ë¶„ì„"""

        if not iteration_history:
            return {"pattern": "insufficient_data", "confidence": 0.0}

        # ì ìˆ˜ ê°œì„  íŒ¨í„´ ë¶„ì„
        scores = [hist.get("average_score", 0.0) for hist in iteration_history]

        if len(scores) < 2:
            return {"pattern": "insufficient_data", "confidence": 0.0}

        # ê°œì„  ì¶”ì„¸ ê³„ì‚°
        improvements = [scores[i] - scores[i-1] for i in range(1, len(scores))]

        # íŒ¨í„´ ë¶„ë¥˜
        if all(imp >= 0 for imp in improvements):
            pattern = "improving"
            confidence = min(0.9, sum(improvements) / len(improvements) * 2)
        elif all(imp <= 0 for imp in improvements[-2:]):
            pattern = "degrading"
            confidence = 0.7
        elif abs(improvements[-1]) < 0.05:
            pattern = "converged"
            confidence = 0.8
        else:
            pattern = "fluctuating"
            confidence = 0.5

        return {
            "pattern": pattern,
            "confidence": confidence,
            "latest_improvement": improvements[-1] if improvements else 0.0,
            "average_improvement": sum(improvements) / len(improvements) if improvements else 0.0
        }


async def replanning_node(chatbot, agent_name: str = "replanner"):
    """
    ì¬ê³„íš ë…¸ë“œ - ì‹¤íŒ¨í•œ ì„œë¸Œì¿¼ë¦¬ë§Œ ì„ íƒì ìœ¼ë¡œ ì¬êµ¬ì„±

    Args:
        chatbot: LLM ì¸ìŠ¤í„´ìŠ¤
        agent_name: ì—ì´ì „íŠ¸ ì´ë¦„

    Returns:
        ì¬ê³„íš ë…¸ë“œ í•¨ìˆ˜
    """
    logger = get_logger(__name__)

    # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    failure_analyzer = FailureAnalyzer(chatbot, logger)
    query_recomposer = QueryRecomposer(chatbot, logger)
    convergence_detector = ConvergenceDetector()

    async def replanning_node_impl(state: AgentState) -> AgentState:
        """ì¬ê³„íš ë…¸ë“œ êµ¬í˜„"""

        try:
            logger.info(f"[{agent_name}] ì¬ê³„íš í”„ë¡œì„¸ìŠ¤ ì‹œì‘")

            # ì‹¤íŒ¨í•œ ì„œë¸Œì¿¼ë¦¬ ì‹ë³„
            failed_subqueries = _identify_failed_subqueries(state)

            if not failed_subqueries:
                logger.info(f"[{agent_name}] ì‹¤íŒ¨í•œ ì„œë¸Œì¿¼ë¦¬ ì—†ìŒ, ì¬ê³„íš ë¶ˆí•„ìš”")
                return type(state)(
                    messages=state.get("messages", []),
                    context_messages=state.get("context_messages", []),
                    metadata=state.get("metadata", {}),
                    current_agent=agent_name,
                    session_id=state.get("session_id", "")
                )

            logger.info(f"[{agent_name}] ì‹¤íŒ¨í•œ ì„œë¸Œì¿¼ë¦¬ {len(failed_subqueries)}ê°œ ë°œê²¬: {failed_subqueries}")

            # ë°˜ë³µ ì •ë³´ ì´ˆê¸°í™”
            replanner_data = MetadataManager.safe_get_metadata(state, "replanner")
            iteration_count = (replanner_data.get("iteration_count", 0) if replanner_data else 0) + 1

            # ìˆ˜ë ´ ì¡°ê±´ í™•ì¸
            iteration_history = replanner_data.get("iteration_history", []) if replanner_data else []
            should_continue, reason = convergence_detector.should_continue_iteration(
                iteration_count, iteration_history
            )

            if not should_continue:
                logger.warning(f"[{agent_name}] ë°˜ë³µ ì¤‘ë‹¨: {reason}")
                _update_replanner_metadata(state, {
                    "status": "converged",
                    "convergence_reason": reason,
                    "iteration_count": iteration_count
                })

                # ğŸ”§ ìˆ˜ì •: ì—…ë°ì´íŠ¸ëœ metadataë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
                updated_metadata = state.get("metadata", {})

                return type(state)(
                    messages=state.get("messages", []),
                    context_messages=state.get("context_messages", []),
                    metadata=updated_metadata,  # â† ì—…ë°ì´íŠ¸ëœ metadata ì‚¬ìš©
                    current_agent=agent_name,
                    session_id=state.get("session_id", "")
                )

            # 1ë‹¨ê³„: ì‹¤íŒ¨ ë¶„ì„
            logger.info(f"[{agent_name}] 1ë‹¨ê³„: ì‹¤íŒ¨ ë¶„ì„ ìˆ˜í–‰")
            failure_analyses = await failure_analyzer.analyze_failures(state, failed_subqueries)

            # 2ë‹¨ê³„: ì¿¼ë¦¬ ì¬êµ¬ì„±
            logger.info(f"[{agent_name}] 2ë‹¨ê³„: ì¿¼ë¦¬ ì¬êµ¬ì„± ìˆ˜í–‰")
            reconstructions = await query_recomposer.reconstruct_queries(
                state, failure_analyses, iteration_count
            )

            # 3ë‹¨ê³„: ì¬êµ¬ì„±ëœ ì„œë¸Œì¿¼ë¦¬ë¡œ í”Œë˜ë„ˆ ë°ì´í„° ì—…ë°ì´íŠ¸
            logger.info(f"[{agent_name}] 3ë‹¨ê³„: í”Œë˜ë„ˆ ë°ì´í„° ì—…ë°ì´íŠ¸")
            _update_planner_with_reconstructions(state, reconstructions)

            # 4ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            _update_replanner_metadata(state, {
                "status": "completed",
                "iteration_count": iteration_count,
                "failed_count": len(failed_subqueries),
                "reconstructed_count": len(reconstructions),
                "failure_analyses": [analysis.to_dict() for analysis in failure_analyses],
                "reconstructions": [recon.to_dict() for recon in reconstructions]
            })

            # ë°˜ë³µ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            _update_iteration_history(state, failed_subqueries, reconstructions)

            logger.info(f"[{agent_name}] ì¬ê³„íš ì™„ë£Œ: {len(reconstructions)}ê°œ ì¿¼ë¦¬ ì¬êµ¬ì„±")

            # ğŸ”§ ìˆ˜ì •: ì—…ë°ì´íŠ¸ëœ metadataë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
            updated_metadata = state.get("metadata", {})

            return type(state)(
                messages=state.get("messages", []),
                context_messages=state.get("context_messages", []),
                metadata=updated_metadata,  # â† ì—…ë°ì´íŠ¸ëœ metadata ì‚¬ìš©
                current_agent=agent_name,
                session_id=state.get("session_id", "")
            )

        except Exception as e:
            logger.error(f"[{agent_name}] ì¬ê³„íš í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            _update_replanner_metadata(state, {
                "status": "error",
                "error_message": str(e),
                "iteration_count": iteration_count
            })

            # ğŸ”§ ìˆ˜ì •: ì—…ë°ì´íŠ¸ëœ metadataë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
            updated_metadata = state.get("metadata", {})

            return type(state)(
                messages=state.get("messages", []),
                context_messages=state.get("context_messages", []),
                metadata=updated_metadata,  # â† ì—…ë°ì´íŠ¸ëœ metadata ì‚¬ìš©
                current_agent=agent_name,
                session_id=state.get("session_id", "")
            )

    return replanning_node_impl


def _identify_failed_subqueries(state: AgentState) -> List[str]:
    """ì‹¤íŒ¨í•œ ì„œë¸Œì¿¼ë¦¬ ì‹ë³„"""

    quality_data = MetadataManager.safe_get_metadata(state, "quality_evaluator")
    if not quality_data:
        return []

    quality_results = quality_data.get("evaluation_results", {})
    failed_subqueries = []

    for subquery_id, quality_info in quality_results.items():
        if not quality_info.get("is_relevant", False):
            failed_subqueries.append(subquery_id)

    return failed_subqueries


def _measure_keyword_change_ratio(original: str, reconstructed: str) -> float:
    """í‚¤ì›Œë“œ ë³€ê²½ ë¹„ìœ¨ ì¸¡ì • (30% ì´ìƒ ë³€ê²½ ìš”êµ¬)

    Args:
        original: ì›ë³¸ ì¿¼ë¦¬
        reconstructed: ì¬êµ¬ì„±ëœ ì¿¼ë¦¬

    Returns:
        float: í‚¤ì›Œë“œ ë³€ê²½ ë¹„ìœ¨ (0.0-1.0, 1.0 = 100% ë³€ê²½)
    """
    original_keywords = set(original.lower().split())
    reconstructed_keywords = set(reconstructed.lower().split())

    if not original_keywords:
        return 1.0  # ì›ë³¸ì´ ë¹„ì–´ìˆìœ¼ë©´ ì™„ì „íˆ ë³€ê²½ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼

    # ëŒ€ì¹­ ì°¨ì§‘í•©: ì›ë³¸ì—ë§Œ ìˆê±°ë‚˜ ì¬êµ¬ì„±ì—ë§Œ ìˆëŠ” í‚¤ì›Œë“œ
    changed_keywords = original_keywords.symmetric_difference(reconstructed_keywords)
    change_ratio = len(changed_keywords) / len(original_keywords)

    return change_ratio


def _is_too_similar(text1: str, text2: str, threshold: float = 0.80) -> bool:
    """ë‘ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ìœ ì‚¬í•œì§€ ê²€ì¦ (ì¬êµ¬ì„± í’ˆì§ˆ ì²´í¬)

    Args:
        text1: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ (ì›ë³¸ ì¿¼ë¦¬)
        text2: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸ (ì¬êµ¬ì„±ëœ ì¿¼ë¦¬)
        threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0.80 = 80% ìœ ì‚¬) - ì‹¤ì§ˆì  ë³€í™” ìš”êµ¬

    Returns:
        True if texts are too similar (ì¬êµ¬ì„± ì‹¤íŒ¨), False otherwise
    """
    from difflib import SequenceMatcher

    # ì›ë³¸ì´ ë¹„ì–´ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ì¬êµ¬ì„± í—ˆìš©
    if not text1.strip():
        return False

    # ì •ê·œí™”: ì†Œë¬¸ì ë³€í™˜ ë° ê³µë°± ì •ë¦¬
    normalized_text1 = " ".join(text1.lower().split())
    normalized_text2 = " ".join(text2.lower().split())

    # ì™„ì „ ë™ì¼í•œ ê²½ìš°
    if normalized_text1 == normalized_text2:
        return True

    # 1. ë¬¸ìì—´ ìœ ì‚¬ë„ ê³„ì‚° (0.80 ì„ê³„ê°’)
    ratio = SequenceMatcher(None, normalized_text1, normalized_text2).ratio()

    # 2. í‚¤ì›Œë“œ ë³€ê²½ ë¹„ìœ¨ ê³„ì‚° (30% ì´ìƒ ìš”êµ¬)
    keyword_change_ratio = _measure_keyword_change_ratio(text1, text2)

    # ìœ ì‚¬ë„ 80% ì´ìƒ OR í‚¤ì›Œë“œ ë³€ê²½ 30% ë¯¸ë§Œì´ë©´ "too similar"
    is_similar = ratio >= threshold or keyword_change_ratio < 0.30

    return is_similar


def _update_planner_with_reconstructions(
    state: AgentState,
    reconstructions: List[ReconstructionResult]
) -> None:
    """ì¬êµ¬ì„±ëœ ì¿¼ë¦¬ë¡œ í”Œë˜ë„ˆ ë°ì´í„° ì—…ë°ì´íŠ¸ (í’ˆì§ˆ ê²€ì¦ í¬í•¨)"""

    logger = get_logger(__name__)
    planner_data = MetadataManager.safe_get_metadata(state, "planner")
    if not planner_data:
        return

    planner_json = planner_data.get("last_plan_json", {})
    subqueries_list = planner_json.get("subqueries", [])

    # ì¬êµ¬ì„± í’ˆì§ˆ í†µê³„
    unchanged_count = 0
    total_reconstructions = len(reconstructions)

    # ì¬êµ¬ì„± ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ë¹ ë¥¸ lookup)
    reconstruction_dict = {r.subquery_id: r for r in reconstructions}

    # ë¦¬ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ìˆœíšŒí•˜ë©° ì—…ë°ì´íŠ¸
    for subquery in subqueries_list:
        subquery_id = subquery.get("id")
        if subquery_id in reconstruction_dict:
            reconstruction = reconstruction_dict[subquery_id]
            old_text = subquery.get("text", "")
            new_text = reconstruction.reconstructed_query

            # ë””ë²„ê¹…: ì¬êµ¬ì„± ì „í›„ ì¿¼ë¦¬ ì¶œë ¥
            from difflib import SequenceMatcher
            similarity = SequenceMatcher(None, old_text.lower(), new_text.lower()).ratio()
            logger.info(
                f"ğŸ” [{subquery_id}] Reconstruction comparison:\n"
                f"  Original: '{old_text}'\n"
                f"  Reconstructed: '{new_text}'\n"
                f"  Similarity: {similarity:.2%}"
            )

            # ì¬êµ¬ì„± í’ˆì§ˆ ê²€ì¦
            if old_text == new_text or _is_too_similar(old_text, new_text):
                logger.warning(
                    f"âŒ [{subquery_id}] Reconstruction quality check failed - "
                    f"new query too similar to original (similarity: {similarity:.2%}). Skipping update."
                )
                unchanged_count += 1
                continue  # í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨ - ì—…ë°ì´íŠ¸ skip

            # í’ˆì§ˆ ê²€ì¦ í†µê³¼ - ì—…ë°ì´íŠ¸ ì§„í–‰
            logger.info(f"âœ… [{subquery_id}] Reconstruction quality OK - updating query")
            subquery["text"] = new_text
            subquery["reconstruction_info"] = {
                "iteration_count": reconstruction.iteration_count,
                "improvement_rationale": reconstruction.improvement_rationale,
                "confidence_score": reconstruction.confidence_score,
                "original_query": reconstruction.original_query
            }

    # ì¬êµ¬ì„± í’ˆì§ˆ ë©”íƒ€ë°ì´í„° ì„¤ì •
    reconstruction_quality = "high"
    if unchanged_count > 0:
        quality_ratio = unchanged_count / total_reconstructions
        if quality_ratio >= 0.5:  # 50% ì´ìƒ ì‹¤íŒ¨
            reconstruction_quality = "low"
            logger.error(
                f"ğŸš¨ Reconstruction quality too low: {unchanged_count}/{total_reconstructions} "
                f"({quality_ratio*100:.1f}%) failed similarity check"
            )
        elif quality_ratio >= 0.3:  # 30% ì´ìƒ ì‹¤íŒ¨
            reconstruction_quality = "medium"
            logger.warning(
                f"âš ï¸ Reconstruction quality medium: {unchanged_count}/{total_reconstructions} "
                f"({quality_ratio*100:.1f}%) failed similarity check"
            )

    # í”Œë˜ë„ˆ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    if "metadata" not in state:
        state["metadata"] = {}
    if "planner" not in state["metadata"]:
        state["metadata"]["planner"] = {}
    state["metadata"]["planner"]["last_plan_json"] = planner_json

    # ì¬êµ¬ì„± í’ˆì§ˆ ì •ë³´ ì €ì¥
    if "replanner" not in state["metadata"]:
        state["metadata"]["replanner"] = {}
    state["metadata"]["replanner"]["reconstruction_quality"] = reconstruction_quality
    state["metadata"]["replanner"]["reconstruction_stats"] = {
        "total": total_reconstructions,
        "successful": total_reconstructions - unchanged_count,
        "failed": unchanged_count,
        "success_rate": (total_reconstructions - unchanged_count) / total_reconstructions if total_reconstructions > 0 else 0
    }


def _update_replanner_metadata(state: AgentState, update_data: Dict[str, Any]) -> None:
    """ì¬ê³„íšì ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""

    if "metadata" not in state:
        state["metadata"] = {}
    if "replanner" not in state["metadata"]:
        state["metadata"]["replanner"] = {}

    state["metadata"]["replanner"].update(update_data)


def _update_iteration_history(
    state: AgentState,
    failed_subqueries: List[str],
    reconstructions: List[ReconstructionResult]
) -> None:
    """ë°˜ë³µ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""

    # í˜„ì¬ ë°˜ë³µ ì •ë³´ êµ¬ì„±
    current_iteration = {
        "iteration": MetadataManager.safe_get_metadata(state, "replanner", "iteration_count") or 1,
        "failed_count": len(failed_subqueries),
        "reconstructed_count": len(reconstructions),
        "average_score": sum(r.confidence_score for r in reconstructions) / len(reconstructions) if reconstructions else 0.0,
        "timestamp": "current"  # ì‹¤ì œ êµ¬í˜„ì‹œ datetime ì‚¬ìš©
    }

    # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    if "metadata" not in state:
        state["metadata"] = {}
    if "replanner" not in state["metadata"]:
        state["metadata"]["replanner"] = {}

    iteration_history = state["metadata"]["replanner"].get("iteration_history", [])
    iteration_history.append(current_iteration)

    # ìµœëŒ€ 10ê°œ íˆìŠ¤í† ë¦¬ ìœ ì§€
    if len(iteration_history) > 10:
        iteration_history = iteration_history[-10:]

    state["metadata"]["replanner"]["iteration_history"] = iteration_history
