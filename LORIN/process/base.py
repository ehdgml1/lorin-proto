"""
Process Base Module - Data Processing and Workflow Management
============================================================

Agentì™€ LLMì„ í†µí•©í•˜ì—¬ ë³µì¡í•œ ë°ì´í„° ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°ë¥¼ ê´€ë¦¬í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.
ë‹¤ì–‘í•œ ì²˜ë¦¬ íŒ¨í„´ì„ ì§€ì›í•˜ë©° í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” í´ë˜ìŠ¤
----------
- None

"""

from ..logger.logger import get_logger
from ..llm import Chatbot
from ..agent.graph import create_agent_graph
from ..agent.state import create_initial_state, get_last_message, format_agent_state
from ..process.route import initialize_graph

from langgraph.graph import StateGraph
from langchain_core.messages import HumanMessage, AIMessage
from datetime import datetime, timezone

from typing import Optional

async def _single_turn_conversation(app: StateGraph, questions: list[str], config: Optional[dict] = None) -> str:
    """
    25.08.17. ì´ì„±í›ˆ

    - ì‹±ê¸€í„´ ëŒ€í™” ë©”ì„œë“œì…ë‹ˆë‹¤.
    - ì§ˆë¬¸ë³„ë¡œ ìƒˆë¡œìš´ ìƒíƒœë¥¼ ìƒì„±í•˜ì—¬ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    - ìµœì¢… ë‹µë³€ì€ messages ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ë©ë‹ˆë‹¤.
    """
    logger = get_logger(__name__)
    logger.debug("Single-turn conversation begins")

    states = []

    for i, question in enumerate(questions, 1):
        # ì´ˆê¸° ìƒíƒœ ìƒì„± (ì‹¤í—˜ config í¬í•¨)
        metadata = {
            "question_id": i,
            "experiment_config": config or {}  # ì‹¤í—˜ ì„¤ì • ì „ë‹¬
        }
        current_state = create_initial_state(
            question,
            metadata=metadata
        )

        logger.info(f"[Q{i}] {question}")
        
        try:
            # ê·¸ë˜í”„ ì‹¤í–‰
            current_state = await app.ainvoke(current_state)
            
            # ê²°ê³¼ ì¶œë ¥
            last_message = get_last_message(current_state)
            logger.info(f"[A{i}] {last_message.content}")

            states.append(current_state)
   
        except Exception as e:
            logger.error(f"Question {i} processing failed: {e}")

    logger.debug("Single-turn conversation ends")

    return states


async def _multi_turn_conversation(app: StateGraph, questions: list[str], config: Optional[dict] = None) -> str:
    """
    25.08.17. ì´ì„±í›ˆ

    - ë©€í‹°í„´ ëŒ€í™” ë©”ì„œë“œì…ë‹ˆë‹¤.
    - ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ í•˜ë‚˜ì˜ ìƒíƒœë¥¼ ê³µìœ í•˜ì—¬ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    - ìµœì¢… ë‹µë³€ì€ messages ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ë©ë‹ˆë‹¤.
    """
    logger = get_logger(__name__)
    logger.debug("Multi-turn conversation begins")

    # ì´ˆê¸° ìƒíƒœ ìƒì„± (ì‹¤í—˜ config í¬í•¨)
    metadata = {
        "experiment_config": config or {}  # ì‹¤í—˜ ì„¤ì • ì „ë‹¬
    }
    current_state = create_initial_state(
        questions[0],
        metadata=metadata
    )

    for i, question in enumerate(questions, 1):
        logger.info(f"[Q{i}] {question}")

        # ì²« ë²ˆì§¸ê°€ ì•„ë‹ˆë©´ ìƒˆ ë©”ì‹œì§€ ì¶”ê°€
        if i > 1:
            # ì´ì „ ìƒíƒœì— ìƒˆ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            current_state["messages"].append(
                HumanMessage(
                    content=question,
                    kwargs={
                        "agent_name": "human",
                        "timestamp": datetime.now(timezone.utc).isoformat()
                    }
                )
            )
        
        try:
            # ê·¸ë˜í”„ ì‹¤í–‰
            current_state = await app.ainvoke(current_state)
            
            # ê²°ê³¼ ì¶œë ¥
            last_message = get_last_message(current_state)
            logger.info(f"[A{i}] {last_message.content}")

        except Exception as e:
            logger.error(f"Question {i} processing failed: {e}")

    logger.debug("Multi-turn conversation ends")

    return current_state
    

async def _llm_process(app: StateGraph, config: Optional[dict] = None, question: Optional[str] = None):
    """
    25.08.17. ì´ì„±í›ˆ

    - í˜„ì¬ ì†”ë£¨ì…˜ ë²„ì „ì—ì„œëŠ” ì‚¬ìš©ì ìƒí˜¸ì‘ìš©ì„ ê³ ë ¤í•˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.
    - LLM ì‚¬ìš©ì„ ìœ„í•´ì„œëŠ” questions ë¦¬ìŠ¤íŠ¸ì— ì§ˆë¬¸ì„ ì¶”ê°€í•˜ë©´ ë©ë‹ˆë‹¤.
    - í–¥í›„ íŒŒì¼ì„ ì½ì–´ì˜¤ê±°ë‚˜ APIë¥¼ í†µí•´ ì‚¬ìš©ì ìƒí˜¸ì‘ìš©ì„ í•˜ë„ë¡ ìˆ˜ì •ë  ì˜ˆì •ì…ë‹ˆë‹¤.
    - ë‹µë³€ ìƒì„± ê³¼ì •ì—ì„œ ì‹±ê¸€í„´ê³¼ ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    logger = get_logger(__name__)

    logger.debug("LLM process begins")

    # 1. ì§ˆë¬¸ ì¤€ë¹„
    # âœ¨ ë‘ ê°€ì§€ ì˜ë„(intent) ì§€ì›:
    # - debug: ë¬¸ì œ/ì—ëŸ¬ ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸ ë²”ìœ„ ì°¾ê¸°
    # - analysis: íŠ¹ì • í™œë™/ì´ë²¤íŠ¸ê°€ ë°œìƒí•œ ë¡œê·¸ ìœ„ì¹˜ ì°¾ê¸°

    if question:
        # ğŸ”¬ ì‹¤í—˜ ëª¨ë“œ: ì™¸ë¶€ì—ì„œ ì§ˆë¬¸ ì œê³µ (experiment_runner.pyì—ì„œ ì‚¬ìš©)
        questions = [question]
        logger.info(f"Using provided question: {question[:100]}...")
    else:
        # ê¸°ë³¸ ëª¨ë“œ: í•˜ë“œì½”ë”©ëœ ì§ˆë¬¸ ì‚¬ìš© (í…ŒìŠ¤íŠ¸/ê°œë°œìš©)
        questions = [
            "The data passed between processes is too large and this is causing a failure. Please tell me which log range I should check for debugging."
            #"When does the Android system boot process start the DropBoxManager service?"
        ]
        logger.info("Using default hardcoded question (development mode)")
    
    # 2. ë‹µë³€ ìƒì„±
    conversation_type = "single"

    if conversation_type == "single":
        states = await _single_turn_conversation(app, questions, config)
    elif conversation_type == "multi":
        state = await _multi_turn_conversation(app, questions, config)
        states = [state]
    else:
        logger.warning("Invalid conversation type")
        states = await _single_turn_conversation(app, questions, config)

    # 3. ë‹µë³€ ì²˜ë¦¬
    # 25.08.17. ì´ì„±í›ˆ: ë””ë²„ê¹…ì„ ìœ„í•œ ì½”ë“œì…ë‹ˆë‹¤.
    # ë””ë²„ê¹…ì„ ìœ„í•´ ëª¨ë“  stateë¥¼ ì¶œë ¥í•˜ê³  ì‹¶ë‹¤ë©´ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
    # í–¥í›„ ë‹¤ë¥¸ ì½”ë“œë¡œ ëŒ€ì²´ë  ì˜ˆì •ì…ë‹ˆë‹¤.
    for state in states:
        logger.info(f'{format_agent_state(state)}')

    logger.debug("LLM process ends")
    
    return states


async def main_process(
    chatbot: Chatbot,
    vectorstore,
    *,
    sparse_store=None,                 # â† ì¶”ê°€
    corpus_path: Optional[str] = None,  # â† ì¶”ê°€
    config: Optional[dict] = None,      # â† ì‹¤í—˜ ì„¤ì •
    question: Optional[str] = None      # â† ì‹¤í—˜ìš© ì§ˆë¬¸
):
    logger = get_logger(__name__)
    logger.debug("Main process begins")

    if config:
        logger.info(f"Main process running with config: {config}")
    if question:
        logger.info(f"Main process running with question: {question[:100]}...")

    graph = create_agent_graph()
    logger.debug("1.Gragh created")

    # ê¸°ì¡´: graph = initialize_graph(graph, chatbot, vectorstore)
    graph = await initialize_graph(
        graph,
        chatbot,
        vectorstore,
        corpus_path=corpus_path,   # â† ì¶”ê°€
        sparse_store=sparse_store,  # â† ì¶”ê°€
        config=config               # â† ì‹¤í—˜ ì„¤ì • ì „ë‹¬
    )
    logger.debug("2.Graph initialized")

    app = graph.compile()
    logger.debug("3.Graph compiled")

    states = await _llm_process(app, config, question)  # â† question ì „ë‹¬
    logger.debug("4.LLM process ends")
    return states