#!/usr/bin/env python3
"""
FAISS ê²€ìƒ‰ ì—”ì§„ - BGE-multilingual-gemma2 ê¸°ë°˜
ì¿¼ë¦¬ë¥¼ ì…ë ¥ë°›ì•„ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import os
import sys
import json
import pandas as pd
import torch
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from colorama import init, Fore, Back, Style
import argparse

# Colorama ì´ˆê¸°í™” (Windows ì§€ì›)
init(autoreset=True)

# â”€â”€ í™˜ê²½ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
# Tokenizer parallelism ê²½ê³  ë°©ì§€
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# â”€â”€ ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    rank: int
    score: float
    content: str
    metadata: Dict[str, Any]
    line_range: List[int]  # [line_start, line_end]
    time_start: str
    time_end: str

# â”€â”€ FAISS ê²€ìƒ‰ ì—”ì§„ í´ë˜ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class FAISSSearchEngine:
    """FAISS ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„"""
    
    def __init__(
        self,
        index_path: str,
        model_name: str = "BAAI/bge-multilingual-gemma2",
        device: str = None,
        use_fp16: bool = True,
        max_length: int = 2048,
        quantization: str = "none"  # "4bit", "8bit", "none"
    ):
        """
        Args:
            index_path: FAISS ì¸ë±ìŠ¤ ê²½ë¡œ
            model_name: ì„ë² ë”© ëª¨ë¸ ì´ë¦„
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (Noneì´ë©´ ìë™ ì„ íƒ)
            use_fp16: FP16 ì‚¬ìš© ì—¬ë¶€ (quantization="none"ì¼ ë•Œë§Œ ì‚¬ìš©)
            max_length: í† í¬ë‚˜ì´ì € ìµœëŒ€ ê¸¸ì´
            quantization: ì–‘ìí™” ëª¨ë“œ ("4bit": ìµœëŒ€ ì ˆì•½/ëŠë¦¼, "8bit": ê· í˜•/ë¹ ë¦„, "none": ì–‘ìí™” ì—†ìŒ)
        """
        self.index_path = index_path
        self.model_name = model_name
        self.use_fp16 = use_fp16
        self.max_length = max_length
        self.quantization = quantization

        # ë””ë°”ì´ìŠ¤ ì„¤ì • (BGE ì„ë² ë”©ì„ GPU 0ë¡œ - EXAONEì€ Multi-GPU ë¶„ì‚°)
        if device is None:
            # GPU 0 ì‚¬ìš© (EXAONEì€ GPU 0+1ì— ë¶„ì‚°)
            self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)

        print(f"{Fore.CYAN}ë””ë°”ì´ìŠ¤: {self.device} (BGE ì„ë² ë”© ëª¨ë¸)")
        
        # ëª¨ë¸ê³¼ ì¸ë±ìŠ¤ ë¡œë“œ
        self._load_model()
        self._load_index()
    
    def _load_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì„ íƒì  ì–‘ìí™” ì§€ì›)"""
        from transformers import AutoTokenizer, AutoModel

        print(f"{Fore.YELLOW}ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}...")

        # ë¡œë”© ì „ GPU ë©”ëª¨ë¦¬ ìƒíƒœ
        if torch.cuda.is_available():
            device_idx = int(str(self.device).split(':')[-1]) if ':' in str(self.device) else 0
            initial_allocated = torch.cuda.memory_allocated(device_idx) / 1e9
            print(f"{Fore.CYAN}ë¡œë”© ì „ GPU {device_idx} ë©”ëª¨ë¦¬: {initial_allocated:.2f}GB")

        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)

        # ì–‘ìí™” ëª¨ë“œì— ë”°ë¥¸ ì„¤ì •
        if self.quantization == "4bit":
            # ğŸ”§ 4-bit ì–‘ìí™”: ìµœëŒ€ ë©”ëª¨ë¦¬ ì ˆì•½ (9GB â†’ 2-3GB), ë¡œë”© ëŠë¦¼
            print(f"{Fore.CYAN}âœ“ 4-bit ì–‘ìí™” í™œì„±í™” (ìµœëŒ€ ë©”ëª¨ë¦¬ ì ˆì•½, ë¡œë”© ëŠë¦¼)")

            # device_mapì„ ë‹¨ìˆœ ë¬¸ìì—´ë¡œ ì§€ì • (bitsandbytes í˜¸í™˜)
            device_idx = int(str(self.device).split(':')[-1]) if ':' in str(self.device) else 0

            # BitsAndBytesConfigëŠ” load_in_4bitì™€ ì¶©ëŒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì§ì ‘ í”Œë˜ê·¸ ì‚¬ìš©
            self.model = AutoModel.from_pretrained(
                self.model_name,
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.float16,
                device_map={"": device_idx},
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            quant_info = "4-bit quantized"

        elif self.quantization == "8bit":
            # ğŸ”§ 8-bit ì–‘ìí™”: ê· í˜•ì¡íŒ ì„ íƒ (9GB â†’ 4-5GB), ë¡œë”© ë¹ ë¦„ (ì¶”ì²œ!)
            print(f"{Fore.CYAN}âœ“ 8-bit ì–‘ìí™” í™œì„±í™” (ê· í˜• ëª¨ë“œ, ë¡œë”© ë¹ ë¦„) âš¡")

            # device_mapì„ ë‹¨ìˆœ ë¬¸ìì—´ë¡œ ì§€ì • (bitsandbytes í˜¸í™˜)
            device_idx = int(str(self.device).split(':')[-1]) if ':' in str(self.device) else 0

            self.model = AutoModel.from_pretrained(
                self.model_name,
                load_in_8bit=True,  # ì§ì ‘ í”Œë˜ê·¸ ì‚¬ìš© (BitsAndBytesConfig ì—†ì´)
                device_map={"": device_idx},  # GPU ì¸ë±ìŠ¤ë¡œ ì§€ì •
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            quant_info = "8-bit quantized"

        else:
            # ì–‘ìí™” ì—†ìŒ: ì›ë³¸ ëª¨ë¸ (9-10GB), ë¡œë”© ê°€ì¥ ë¹ ë¦„
            print(f"{Fore.CYAN}ì–‘ìí™” ì—†ìŒ (ì›ë³¸ ëª¨ë¸, FP16: ~9GB)")

            self.model = AutoModel.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if self.use_fp16 else torch.float32,
                trust_remote_code=True
            )
            self.model.to(self.device)
            quant_info = "FP16" if self.use_fp16 else "FP32"

        self.model.eval()

        # BGE ëª¨ë¸ìš© ì¿¼ë¦¬ prefix
        self.query_instruction = "Represent this sentence for searching relevant passages: "

        print(f"{Fore.GREEN}âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({quant_info})")

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
        if torch.cuda.is_available():
            device_idx = int(str(self.device).split(':')[-1]) if ':' in str(self.device) else 0
            allocated = torch.cuda.memory_allocated(device_idx) / 1e9
            reserved = torch.cuda.memory_reserved(device_idx) / 1e9
            memory_increase = allocated - initial_allocated
            print(f"{Fore.CYAN}ë¡œë”© í›„ GPU {device_idx} ë©”ëª¨ë¦¬ - Allocated: {allocated:.2f}GB (+{memory_increase:.2f}GB), Reserved: {reserved:.2f}GB")

            # ì–‘ìí™” íš¨ê³¼ ê²€ì¦
            if self.quantization in ["4bit", "8bit"]:
                expected_mem = 2.5 if self.quantization == "4bit" else 4.5
                if memory_increase > expected_mem + 1.5:  # 1.5GB ë§ˆì§„
                    print(f"{Fore.RED}âš ï¸  ê²½ê³ : ì–‘ìí™”ê°€ ì œëŒ€ë¡œ ì ìš©ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
                    print(f"{Fore.RED}   ì˜ˆìƒ: ~{expected_mem:.1f}GB, ì‹¤ì œ: {memory_increase:.2f}GB")
                else:
                    print(f"{Fore.GREEN}âœ“ ì–‘ìí™” ì„±ê³µ: {memory_increase:.2f}GB (ì˜ˆìƒ ë²”ìœ„)")
    
    def _load_index(self):
        """FAISS ì¸ë±ìŠ¤ ë¡œë“œ"""
        from langchain_community.vectorstores import FAISS
        from langchain.embeddings.base import Embeddings
        
        # ì»¤ìŠ¤í…€ ì„ë² ë”© í´ë˜ìŠ¤
        class CustomEmbeddings(Embeddings):
            def __init__(self, engine):
                self.engine = engine
            
            def embed_documents(self, texts: List[str]) -> List[List[float]]:
                return [self.engine._encode_text(text, add_instruction=False) for text in texts]
            
            def embed_query(self, text: str) -> List[float]:
                return self.engine._encode_text(text, add_instruction=True)
        
        print(f"{Fore.YELLOW}ì¸ë±ìŠ¤ ë¡œë”© ì¤‘: {self.index_path}...")

        embeddings = CustomEmbeddings(self)
        self.vectorstore = FAISS.load_local(
            self.index_path,
            embeddings,
            allow_dangerous_deserialization=True
        )

        print(f"{Fore.GREEN}âœ“ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")

        # âœ¨ ë¡œë“œëœ ì¸ë±ìŠ¤ì˜ ë©”íƒ€ë°ì´í„° ê²€ì¦
        print(f"\n{Fore.CYAN}=== ë¡œë“œëœ ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ê²€ì¦ ===")
        try:
            # ìƒ˜í”Œ ë¬¸ì„œ ì¶”ì¶œ (ì²« ë²ˆì§¸ ë¬¸ì„œ)
            sample_docs = self.vectorstore.similarity_search("test", k=1)
            if sample_docs:
                sample_metadata = sample_docs[0].metadata

                # Temporal metadata í•„ë“œ í™•ì¸
                has_line_number = 'line_number' in sample_metadata
                has_total_lines = 'total_lines' in sample_metadata
                has_relative_position = 'relative_position' in sample_metadata
                has_range = 'Range' in sample_metadata

                print(f"{Fore.CYAN}ë©”íƒ€ë°ì´í„° í•„ë“œ ìƒíƒœ:")
                print(f"  - line_number: {Fore.GREEN + 'âœ“' if has_line_number else Fore.RED + 'âœ— MISSING'}")
                print(f"  - total_lines: {Fore.GREEN + 'âœ“' if has_total_lines else Fore.RED + 'âœ— MISSING'}")
                print(f"  - relative_position: {Fore.GREEN + 'âœ“' if has_relative_position else Fore.RED + 'âœ— MISSING'}")
                print(f"  - Range: {Fore.GREEN + 'âœ“' if has_range else Fore.RED + 'âœ— MISSING'}")

                # ì‹¤ì œ ê°’ ì¶œë ¥
                if has_relative_position:
                    print(f"\n{Fore.CYAN}ìƒ˜í”Œ temporal metadata ê°’:")
                    print(f"  - line_number: {sample_metadata.get('line_number', 'N/A')}")
                    print(f"  - total_lines: {sample_metadata.get('total_lines', 'N/A')}")
                    print(f"  - relative_position: {sample_metadata.get('relative_position', 0.0):.4f}")
                    print(f"  - Range: {sample_metadata.get('Range', 'N/A')}")
                    print(f"{Fore.GREEN}âœ“ Temporal filtering ì‚¬ìš© ê°€ëŠ¥")
                else:
                    print(f"\n{Fore.RED}âš ï¸ WARNING: Temporal metadata í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤!")
                    print(f"{Fore.RED}   â†’ Temporal filteringì´ ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    print(f"{Fore.YELLOW}   â†’ FAISS ì¸ë±ìŠ¤ë¥¼ ì¬êµ¬ì¶•í•˜ì„¸ìš”.")
                    print(f"\n{Fore.CYAN}Available fields: {list(sample_metadata.keys())}")
            else:
                print(f"{Fore.RED}âš ï¸ ìƒ˜í”Œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"{Fore.RED}âš ï¸ ë©”íƒ€ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"{Fore.CYAN}{'='*60}\n")

    def reload_index(self, new_index_path: str):
        """
        ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œ (BGE ëª¨ë¸ì€ ìœ ì§€)

        Args:
            new_index_path: ìƒˆ FAISS ì¸ë±ìŠ¤ ê²½ë¡œ
        """
        from langchain_community.vectorstores import FAISS
        from langchain.embeddings.base import Embeddings

        # ì»¤ìŠ¤í…€ ì„ë² ë”© í´ë˜ìŠ¤
        class CustomEmbeddings(Embeddings):
            def __init__(self, engine):
                self.engine = engine

            def embed_documents(self, texts: List[str]) -> List[List[float]]:
                return [self.engine._encode_text(text, add_instruction=False) for text in texts]

            def embed_query(self, text: str) -> List[float]:
                return self.engine._encode_text(text, add_instruction=True)

        print(f"{Fore.YELLOW}ğŸ”„ ì¸ë±ìŠ¤ êµì²´ ì¤‘: {new_index_path}...")

        # ì´ì „ ì¸ë±ìŠ¤ ë©”ëª¨ë¦¬ í•´ì œ
        if hasattr(self, 'vectorstore'):
            del self.vectorstore
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        self.index_path = new_index_path
        embeddings = CustomEmbeddings(self)
        self.vectorstore = FAISS.load_local(
            new_index_path,
            embeddings,
            allow_dangerous_deserialization=True
        )

        print(f"{Fore.GREEN}âœ“ ì¸ë±ìŠ¤ êµì²´ ì™„ë£Œ")

    def _encode_text(self, text: str, add_instruction: bool = False) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        if add_instruction:
            text = self.query_instruction + text
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            text,
            padding=True,
            truncation=True,
            max_length=self.max_length,
            return_tensors='pt'
        ).to(self.device)
        
        # ì¸ì½”ë”©
        with torch.no_grad():
            outputs = self.model(**inputs)
            embeddings = outputs[0].mean(dim=1)  # Mean pooling
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
        
        return embeddings[0].cpu().numpy().tolist()
    
    def search(
        self,
        query: str,
        k: int = 5,
        score_threshold: Optional[float] = None,
        filter: Optional[Dict[str, Any]] = None
    ) -> List[SearchResult]:
        """
        ê²€ìƒ‰ ì‹¤í–‰ (PRE-FILTERING ì§€ì›)

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            score_threshold: ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ë” ìœ ì‚¬)
            filter: FAISS ë©”íƒ€ë°ì´í„° í•„í„° (PRE-FILTERING)
                   ì˜ˆ: {"relative_position": {"$gte": 0.5, "$lte": 0.7}}

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # âœ¨ PRE-FILTERING: FAISS searches only within filter range BEFORE semantic search
        # However, LangChain FAISS uses POST-FILTERING (fetch_k â†’ filter in Python)
        # So we need to set fetch_k high enough to get all candidates before filtering
        fetch_k = max(100, k * 3) if filter else k  # Fetch more when filtering

        results_with_scores = self.vectorstore.similarity_search_with_score(
            query,
            k=k,
            filter=filter,  # â† Filter is applied AFTER fetching fetch_k results
            fetch_k=fetch_k  # â† Fetch more results before filtering
        )

        # ê²°ê³¼ íŒŒì‹±
        search_results = []
        for rank, (doc, score) in enumerate(results_with_scores, 1):
            # ì ìˆ˜ í•„í„°ë§
            if score_threshold is not None and score > score_threshold:
                continue
            
            # SearchResult ê°ì²´ ìƒì„±
            result = SearchResult(
                rank=rank,
                score=float(score),
                content=doc.page_content,
                metadata=doc.metadata,
                line_range=doc.metadata.get('Range', [0, 0]),
                time_start=doc.metadata.get('TimeStart', ''),
                time_end=doc.metadata.get('TimeEnd', '')
            )
            search_results.append(result)
        
        return search_results
    
    def search_with_reranking(
        self,
        query: str,
        k: int = 20,
        top_n: int = 5
    ) -> List[SearchResult]:
        """
        ì¬ìˆœìœ„í™”ë¥¼ í¬í•¨í•œ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
            top_n: ì¬ìˆœìœ„í™” í›„ ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        """
        # ì´ˆê¸° ê²€ìƒ‰
        initial_results = self.search(query, k=k)

        # ì—¬ê¸°ì— ì¬ìˆœìœ„í™” ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
        # ì˜ˆ: Cross-encoder, BM25 ë“±

        return initial_results[:top_n]

# â”€â”€ ê²°ê³¼ ì¶œë ¥ í•¨ìˆ˜ë“¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def print_result(result: SearchResult, verbose: bool = False):
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
    print(f"\n{Fore.CYAN}{'='*80}")
    print(f"{Fore.YELLOW}[ìˆœìœ„ {result.rank}] {Fore.GREEN}ìœ ì‚¬ë„: {result.score:.4f}")
    print(f"{Fore.MAGENTA}ë¼ì¸ë²”ìœ„: {result.line_range}")
    print(f"{Fore.MAGENTA}ì‹œê°„: {result.time_start} - {result.time_end}")
    print(f"{Fore.CYAN}{'-'*80}")
    
    # ë‚´ìš© ì¶œë ¥ (ê¸¸ì´ ì œí•œ)
    content = result.content.strip()
    if not verbose and len(content) > 500:
        content = content[:500] + "..."
    
    # ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥
    lines = content.split('\n')
    for line in lines[:10] if not verbose else lines:  # verboseê°€ ì•„ë‹ˆë©´ 10ì¤„ê¹Œì§€ë§Œ
        print(f"{Fore.WHITE}{line}")
    
    if not verbose and len(lines) > 10:
        print(f"{Fore.YELLOW}... ({len(lines)-10}ì¤„ ë” ìˆìŒ)")

def save_results(results: List[SearchResult], output_path: str, format: str = "json"):
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    if format == "json":
        data = []
        for r in results:
            data.append({
                "rank": r.rank,
                "score": r.score,
                "content": r.content,
                "line_range": r.line_range,
                "time_start": r.time_start,
                "time_end": r.time_end,
                "metadata": r.metadata
            })
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    elif format == "csv":
        df = pd.DataFrame([{
            "rank": r.rank,
            "score": r.score,
            "content": r.content[:200],  # CSVëŠ” ë‚´ìš© ì œí•œ
            "line_range": r.line_range[0],
            "time_start": r.time_start,
            "time_end": r.time_end
        } for r in results])
        df.to_csv(output_path, index=False, encoding='utf-8')
    
    elif format == "txt":
        with open(output_path, 'w', encoding='utf-8') as f:
            for r in results:
                f.write(f"Rank {r.rank} (Score: {r.score:.4f})\n")
                if r.line_range:
                    f.write(f"Lines: {r.line_range[0]} - {r.line_range[1]}\n")
                f.write(f"Time: {r.time_start} - {r.time_end}\n")
                f.write(f"Content:\n{r.content}\n")
                f.write("="*80 + "\n\n")
    
    print(f"{Fore.GREEN}âœ“ ê²°ê³¼ ì €ì¥: {output_path}")

# â”€â”€ ì¸í„°ë™í‹°ë¸Œ ê²€ìƒ‰ ëª¨ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def interactive_search(engine: FAISSSearchEngine):
    """ì¸í„°ë™í‹°ë¸Œ ê²€ìƒ‰ ëª¨ë“œ"""
    print(f"\n{Fore.CYAN}{'='*80}")
    print(f"{Fore.YELLOW}ì¸í„°ë™í‹°ë¸Œ ê²€ìƒ‰ ëª¨ë“œ")
    print(f"{Fore.WHITE}ëª…ë ¹ì–´: /quit (ì¢…ë£Œ), /save (ê²°ê³¼ ì €ì¥), /clear (í™”ë©´ ì •ë¦¬)")
    print(f"       /top N (ìƒìœ„ Nê°œ ê²°ê³¼), /threshold F (ìœ ì‚¬ë„ ì„ê³„ê°’)")
    print(f"{Fore.CYAN}{'='*80}\n")
    
    # ì„¤ì •
    top_k = 5
    threshold = None
    last_results = None
    
    while True:
        try:
            # ì¿¼ë¦¬ ì…ë ¥
            query = input(f"\n{Fore.GREEN}ê²€ìƒ‰ì–´ ì…ë ¥: {Style.RESET_ALL}").strip()
            
            # ëª…ë ¹ì–´ ì²˜ë¦¬
            if query.startswith('/'):
                parts = query.split()
                cmd = parts[0].lower()
                
                if cmd == '/quit':
                    print(f"{Fore.YELLOW}ê²€ìƒ‰ ì¢…ë£Œ")
                    break
                
                elif cmd == '/clear':
                    os.system('cls' if os.name == 'nt' else 'clear')
                    continue
                
                elif cmd == '/save' and last_results:
                    filename = input("ì €ì¥í•  íŒŒì¼ëª… (ì˜ˆ: results.json): ").strip()
                    if filename:
                        format = filename.split('.')[-1] if '.' in filename else 'json'
                        save_results(last_results, filename, format)
                    continue
                
                elif cmd == '/top' and len(parts) > 1:
                    try:
                        top_k = int(parts[1])
                        print(f"{Fore.YELLOW}ìƒìœ„ {top_k}ê°œ ê²°ê³¼ í‘œì‹œ")
                    except ValueError:
                        print(f"{Fore.RED}ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”")
                    continue
                
                elif cmd == '/threshold' and len(parts) > 1:
                    try:
                        threshold = float(parts[1])
                        print(f"{Fore.YELLOW}ìœ ì‚¬ë„ ì„ê³„ê°’: {threshold}")
                    except ValueError:
                        print(f"{Fore.RED}ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”")
                    continue
                
                else:
                    print(f"{Fore.RED}ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {cmd}")
                    continue
            
            # ë¹ˆ ì¿¼ë¦¬ í™•ì¸
            if not query:
                continue
            
            # ê²€ìƒ‰ ì‹¤í–‰
            print(f"\n{Fore.YELLOW}ê²€ìƒ‰ ì¤‘...")
            results = engine.search(query, k=top_k, score_threshold=threshold)
            last_results = results
            
            # ê²°ê³¼ ì¶œë ¥
            if results:
                print(f"{Fore.GREEN}ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
                for result in results:
                    print_result(result, verbose=False)
            else:
                print(f"{Fore.RED}ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        except KeyboardInterrupt:
            print(f"\n{Fore.YELLOW}ê²€ìƒ‰ ì¢…ë£Œ")
            break
        except Exception as e:
            print(f"{Fore.RED}ì˜¤ë¥˜ ë°œìƒ: {e}")

# â”€â”€ ë°°ì¹˜ ê²€ìƒ‰ ëª¨ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def batch_search(engine: FAISSSearchEngine, queries_file: str, output_dir: str):
    """ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ì¼ê´„ ì²˜ë¦¬"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ì¿¼ë¦¬ íŒŒì¼ ì½ê¸°
    with open(queries_file, 'r', encoding='utf-8') as f:
        queries = [line.strip() for line in f if line.strip()]
    
    print(f"{Fore.CYAN}ë°°ì¹˜ ê²€ìƒ‰: {len(queries)}ê°œ ì¿¼ë¦¬")
    
    all_results = {}
    for i, query in enumerate(queries, 1):
        print(f"\n[{i}/{len(queries)}] ê²€ìƒ‰: {query}")
        
        results = engine.search(query, k=5)
        all_results[query] = results
        
        # ê°œë³„ ê²°ê³¼ ì €ì¥
        query_safe = "".join(c for c in query if c.isalnum() or c in (' ', '-', '_'))[:50]
        output_file = output_dir / f"result_{i:03d}_{query_safe}.json"
        save_results(results, str(output_file), format="json")
    
    # ì „ì²´ ìš”ì•½ ì €ì¥
    summary_file = output_dir / "summary.json"
    summary_data = {
        query: [{
            "rank": r.rank,
            "score": r.score,
            "content_preview": r.content[:100]
        } for r in results[:3]]  # ìƒìœ„ 3ê°œë§Œ
        for query, results in all_results.items()
    }
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, ensure_ascii=False, indent=2)
    
    print(f"\n{Fore.GREEN}âœ“ ë°°ì¹˜ ê²€ìƒ‰ ì™„ë£Œ: {output_dir}")

# â”€â”€ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="FAISS ê²€ìƒ‰ ì—”ì§„")
    parser.add_argument(
        "--index-path",
        type=str,
        help="FAISS ì¸ë±ìŠ¤ ê²½ë¡œ"
    )
    parser.add_argument(
        "--query",
        type=str,
        help="ê²€ìƒ‰ ì¿¼ë¦¬ (ë‹¨ì¼ ê²€ìƒ‰)"
    )
    parser.add_argument(
        "--interactive",
        action="store_true",
        help="ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ"
    )
    parser.add_argument(
        "--batch",
        type=str,
        help="ë°°ì¹˜ ê²€ìƒ‰ìš© ì¿¼ë¦¬ íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=5,
        help="ë°˜í™˜í•  ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)"
    )
    parser.add_argument(
        "--threshold",
        type=float,
        help="ìœ ì‚¬ë„ ì„ê³„ê°’"
    )
    parser.add_argument(
        "--output",
        type=str,
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ"
    )
    parser.add_argument(
        "--format",
        choices=["json", "csv", "txt"],
        default="json",
        help="ì¶œë ¥ í˜•ì‹ (ê¸°ë³¸ê°’: json)"
    )
    parser.add_argument(
        "--device",
        type=str,
        help="ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (cuda:0, cuda:1, cpu)"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ì¶œë ¥"
    )
    
    args = parser.parse_args()
    
    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    if not args.index_path:
        if os.name == 'nt':  # Windows
            args.index_path = r"C:\Users\soilf\lorin-proto\LORIN\make_faiss\log_faiss_index_bge_gemma2"
        else:  # Linux
            args.index_path = "/home/bigdata/lorin-proto/LORIN/make_faiss/log_faiss_index_bge_gemma2"
    
    # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
    print(f"{Fore.CYAN}FAISS ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”...")
    engine = FAISSSearchEngine(
        index_path=args.index_path,
        device=args.device
    )
    
    # ì‹¤í–‰ ëª¨ë“œ ì„ íƒ
    if args.interactive:
        # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
        interactive_search(engine)
    
    elif args.batch:
        # ë°°ì¹˜ ëª¨ë“œ
        output_dir = args.output or "batch_results"
        batch_search(engine, args.batch, output_dir)
    
    elif args.query:
        # ë‹¨ì¼ ì¿¼ë¦¬ ëª¨ë“œ
        print(f"\n{Fore.YELLOW}ê²€ìƒ‰: {args.query}")
        results = engine.search(
            args.query,
            k=args.top_k,
            score_threshold=args.threshold
        )
        
        # ê²°ê³¼ ì¶œë ¥
        if results:
            print(f"{Fore.GREEN}ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ\n")
            for result in results:
                print_result(result, verbose=args.verbose)
            
            # íŒŒì¼ ì €ì¥
            if args.output:
                save_results(results, args.output, args.format)
        else:
            print(f"{Fore.RED}ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    else:
        # ê¸°ë³¸: ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
        interactive_search(engine)

if __name__ == "__main__":
    main()