"""
Quality Evaluator Module - EXAONE-based Relevance Assessment
=============================================================

LORIN ì‹œìŠ¤í…œì˜ ì„œë¸Œì¿¼ë¦¬ì™€ ê²€ìƒ‰ ë¬¸ì„œ ê°„ ê´€ë ¨ì„±ì„ EXAONE ëª¨ë¸ë¡œ í‰ê°€í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

ì£¼ìš” íŠ¹ì§•:
- EXAONE 3.5-32B ëª¨ë¸ ê¸°ë°˜ ê´€ë ¨ì„± í‰ê°€
- Android ë¡œê·¸ ë°ì´í„° íŠ¹í™” í‰ê°€ í”„ë¡¬í”„íŠ¸
- ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ í† í° íš¨ìœ¨ì„± ìµœì í™”
- êµ¬ì¡°í™”ëœ JSON ì¶œë ¥ê³¼ ì‹ ë¢°ë„ ì¸¡ì •
- ì—ëŸ¬ í•¸ë“¤ë§ ë° í´ë°± ë©”ì»¤ë‹ˆì¦˜
- LangGraph State í˜¸í™˜ì„±

ì•„í‚¤í…ì²˜:
- QueryEvaluationResult: ì¿¼ë¦¬ë³„ ì¢…í•© í‰ê°€ ê²°ê³¼
- DocumentEvaluation: ë¬¸ì„œë³„ ì„¸ë¶€ í‰ê°€ ì •ë³´
- QualityEvaluator: ë©”ì¸ í‰ê°€ ì—”ì§„
- quality_evaluator_node: LangGraph í†µí•© ë…¸ë“œ

í†µí•©:
- ì…ë ¥: state.metadata.faiss_retriever ë°ì´í„°
- ì¶œë ¥: state.metadata.quality_evaluator ë„¤ì„ìŠ¤í˜ì´ìŠ¤
- ì˜ì¡´ì„±: chatbot.pyì˜ EXAONE ëª¨ë¸
"""

from __future__ import annotations
import json
import time
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timezone
import logging
import torch

from langchain_core.messages import AIMessage, BaseMessage

from ...llm.chatbot import Chatbot, LLMProvider
from ...make_faiss.search_engine import SearchResult
from ...logger.logger import get_logger
from ..state import AgentState
from ...prompt.template_loader import PromptTemplateLoader

logger = get_logger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Classes for Evaluation Results
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class DocumentEvaluation:
    """ê°œë³„ ë¬¸ì„œì— ëŒ€í•œ ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼"""
    doc_rank: int
    content_preview: str  # ì²˜ìŒ 200ì
    relevance_score: float  # 0.0-1.0
    is_relevant: bool  # True/False íŒë‹¨
    reasoning: str  # í‰ê°€ ê·¼ê±°
    confidence: float  # í‰ê°€ ì‹ ë¢°ë„ 0.0-1.0
    tokens_analyzed: int  # ë¶„ì„ëœ í† í° ìˆ˜

@dataclass
class QueryEvaluationResult:
    """ì„œë¸Œì¿¼ë¦¬ì— ëŒ€í•œ ì¢…í•© í‰ê°€ ê²°ê³¼"""
    query_id: str
    query_text: str
    is_relevant: bool  # ìµœì¢… ê´€ë ¨ì„± íŒë‹¨
    relevance_score: float  # 0.0-1.0 ì¢…í•© ì ìˆ˜
    document_evaluations: List[DocumentEvaluation]
    evaluation_time: float  # í‰ê°€ ì†Œìš” ì‹œê°„
    confidence: float  # ì¢…í•© ì‹ ë¢°ë„
    reasoning: str  # ì¢…í•© í‰ê°€ ê·¼ê±°

    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
    total_documents: int
    relevant_documents: int
    avg_doc_score: float
    max_doc_score: float
    tokens_used: int

    # replannerë¥¼ ìœ„í•œ ìƒì„¸ í”¼ë“œë°± (default value í•„ë“œëŠ” ë§ˆì§€ë§‰ì—)
    improvement_suggestions: str = ""  # ê°œì„  ì œì•ˆ
    query_effectiveness: str = ""  # ì¿¼ë¦¬ íš¨ê³¼ì„± í‰ê°€

    # ì—ëŸ¬ ì •ë³´
    error: Optional[str] = None
    fallback_used: bool = False

@dataclass
class EvaluationStats:
    """ì „ì²´ í‰ê°€ ê³¼ì •ì˜ í†µê³„ ì •ë³´"""
    total_queries: int
    successful_evaluations: int
    failed_evaluations: int
    total_documents_evaluated: int
    total_relevant_documents: int

    avg_relevance_score: float
    avg_confidence: float
    avg_evaluation_time: float

    total_tokens_used: int
    api_calls_made: int
    cache_hits: int
    fallback_count: int

    evaluation_time: float
    timestamp: str

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Korean Log Analysis Prompt Templates
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class EvaluationPrompts:
    """EXAONE ëª¨ë¸ì„ ìœ„í•œ í•œêµ­ì–´ ë¡œê·¸ ë¶„ì„ íŠ¹í™” í”„ë¡¬í”„íŠ¸"""

    @staticmethod
    def get_relevance_evaluation_prompt(
        query: str,
        documents: List[SearchResult],
        max_docs: int = 5
    ) -> str:
        """ê´€ë ¨ì„± í‰ê°€ë¥¼ ìœ„í•œ ë©”ì¸ í”„ë¡¬í”„íŠ¸ ìƒì„± (Jinja2 template ì‚¬ìš©)"""
        template_loader = PromptTemplateLoader()

        return template_loader.render_template(
            "quality_evaluation.j2",
            query=query,
            documents=documents[:max_docs],
            max_docs=max_docs
        )

    @staticmethod
    def get_simple_relevance_prompt(query: str, content: str) -> str:
        """ë‹¨ì¼ ë¬¸ì„œ ê°„ë‹¨ í‰ê°€ìš© í”„ë¡¬í”„íŠ¸"""
        return f"""ë‹¤ìŒ ì¿¼ë¦¬ì™€ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ 0-100 ì ìˆ˜ë¡œ í‰ê°€í•˜ì„¸ìš”.

ì¿¼ë¦¬: {query}
ë¬¸ì„œ: {content[:200]}...

ì ìˆ˜ë§Œ ìˆ«ìë¡œ ë‹µí•˜ì„¸ìš” (0-100):"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Quality Evaluator Implementation
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class QualityEvaluator:
    """EXAONE ê¸°ë°˜ ê´€ë ¨ì„± í‰ê°€ ì—”ì§„"""

    def __init__(
        self,
        chatbot: Optional[Chatbot] = None,
        relevance_threshold: float = 0.5,
        confidence_threshold: float = 0.6,
        max_docs_per_query: int = 3,  # 5 â†’ 3 (í”„ë¡¬í”„íŠ¸ í¬ê¸° ê°ì†Œ)
        max_retries: int = 3,
        timeout_seconds: float = 120.0  # 30ì´ˆ â†’ 120ì´ˆ (Multi-GPU ë¶„ì‚° ê³ ë ¤)
    ):
        """
        Args:
            chatbot: EXAONE ì±—ë´‡ ì¸ìŠ¤í„´ìŠ¤ (Noneì´ë©´ ìë™ ìƒì„±)
            relevance_threshold: ê´€ë ¨ì„± ì„ê³„ê°’ (>=ì´ë©´ ê´€ë ¨ì„± ìˆìŒ)
            confidence_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
            max_docs_per_query: ì¿¼ë¦¬ë‹¹ ìµœëŒ€ ë¶„ì„ ë¬¸ì„œ ìˆ˜
            max_retries: API í˜¸ì¶œ ì¬ì‹œë„ íšŸìˆ˜
            timeout_seconds: í‰ê°€ íƒ€ì„ì•„ì›ƒ
        """
        self.relevance_threshold = relevance_threshold
        self.confidence_threshold = confidence_threshold
        self.max_docs_per_query = max_docs_per_query
        self.max_retries = max_retries
        self.timeout_seconds = timeout_seconds

        # ì±—ë´‡ ì´ˆê¸°í™” (í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜)
        if chatbot is None:
            from ...utils import create_chatbot_from_env
            self.chatbot = create_chatbot_from_env(
                temperature=0.1  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
                # max_tokensëŠ” ê¸°ë³¸ê°’(4096) ì‚¬ìš©
            )
        else:
            self.chatbot = chatbot

        # ì„±ëŠ¥ ì¶”ì 
        self.total_api_calls = 0
        self.total_tokens_used = 0
        self.cache_hits = 0
        self.fallback_count = 0

        # ê°„ë‹¨í•œ ìºì‹œ (ì¿¼ë¦¬-ê²°ê³¼ í•´ì‹œ)
        self.evaluation_cache: Dict[str, QueryEvaluationResult] = {}

        logger.info(f"[QualityEvaluator] Initialized with threshold={relevance_threshold}")

    def _generate_cache_key(self, query: str, doc_hashes: List[str]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        import hashlib
        cache_input = f"{query}:{':'.join(sorted(doc_hashes))}"
        return hashlib.md5(cache_input.encode('utf-8')).hexdigest()

    def _get_document_hash(self, doc: SearchResult) -> str:
        """ë¬¸ì„œ í•´ì‹œ ìƒì„±"""
        import hashlib
        content_key = f"{doc.content[:100]}:{doc.score}"
        return hashlib.md5(content_key.encode('utf-8')).hexdigest()[:8]

    async def _call_exaone_with_retry(self, prompt: str) -> str:
        """ì¬ì‹œë„ ë¡œì§ì´ ìˆëŠ” EXAONE í˜¸ì¶œ - Exponential backoff ì ìš©"""
        import time
        last_error = None
        # ì¬ì‹œë„ íšŸìˆ˜ 3â†’2ë¡œ ê°ì†Œ (ì„±ëŠ¥ ìµœì í™”)
        optimized_retries = min(self.max_retries, 2)

        for attempt in range(optimized_retries):
            call_start_time = time.time()
            try:
                self.total_api_calls += 1

                # ğŸ“Š LLM í˜¸ì¶œ ì‹œì‘ ë¡œê·¸
                prompt_length = len(prompt)
                logger.info(f"[QualityEvaluator] ğŸš€ LLM call #{self.total_api_calls} started (attempt {attempt + 1}/{optimized_retries}, prompt: {prompt_length} chars, timeout: {self.timeout_seconds}s)")

                # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ (í˜¸ì¶œ ì „)
                if torch.cuda.is_available():
                    free_memory_before = torch.cuda.mem_get_info(0)[0] / 1e9
                    allocated_memory_before = torch.cuda.memory_allocated(0) / 1e9
                    logger.debug(f"[QualityEvaluator] ğŸ’¾ Pre-call GPU: Free={free_memory_before:.2f}GB, Allocated={allocated_memory_before:.2f}GB")

                    # ë©”ëª¨ë¦¬ ë¶€ì¡± ê²½ê³ 
                    if free_memory_before < 2.0:
                        logger.warning(f"[QualityEvaluator] âš ï¸ Low GPU memory before LLM call: {free_memory_before:.2f}GB (OOM risk)")

                # íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ í˜¸ì¶œ
                response = await asyncio.wait_for(
                    self.chatbot.ask(prompt),
                    timeout=self.timeout_seconds
                )

                call_duration = time.time() - call_start_time

                # í† í° ê³„ì‚° (ê·¼ì‚¬ì¹˜)
                estimated_tokens = len(prompt.split()) + len(response.split())
                self.total_tokens_used += estimated_tokens

                # ğŸ“Š LLM í˜¸ì¶œ ì„±ê³µ ë¡œê·¸
                response_length = len(response)
                logger.info(f"[QualityEvaluator] âœ… LLM call #{self.total_api_calls} succeeded in {call_duration:.2f}s (response: {response_length} chars, ~{estimated_tokens} tokens)")

                # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ (í˜¸ì¶œ í›„)
                if torch.cuda.is_available():
                    free_memory_after = torch.cuda.mem_get_info(0)[0] / 1e9
                    allocated_memory_after = torch.cuda.memory_allocated(0) / 1e9
                    memory_delta = allocated_memory_after - allocated_memory_before
                    logger.debug(f"[QualityEvaluator] ğŸ’¾ Post-call GPU: Free={free_memory_after:.2f}GB, Allocated={allocated_memory_after:.2f}GB (Î”{memory_delta:+.2f}GB)")

                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (CUDA OOM ë°©ì§€)
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    logger.debug(f"[QualityEvaluator] ğŸ§¹ GPU cache cleared")

                return response

            except asyncio.TimeoutError:
                call_duration = time.time() - call_start_time
                last_error = f"Timeout after {self.timeout_seconds}s"
                logger.warning(f"[QualityEvaluator] â±ï¸ LLM call #{self.total_api_calls} TIMEOUT after {call_duration:.2f}s (attempt {attempt + 1}/{optimized_retries})")

                # GPU ìƒíƒœ í™•ì¸ (íƒ€ì„ì•„ì›ƒ ì‹œ)
                if torch.cuda.is_available():
                    free_memory = torch.cuda.mem_get_info(0)[0] / 1e9
                    logger.warning(f"[QualityEvaluator] ğŸ’¾ GPU memory at timeout: {free_memory:.2f}GB free")

            except RuntimeError as e:
                call_duration = time.time() - call_start_time
                # GPU OOM ì—ëŸ¬ íŠ¹ë³„ ì²˜ë¦¬
                if "out of memory" in str(e).lower() or "oom" in str(e).lower():
                    if torch.cuda.is_available():
                        allocated = torch.cuda.memory_allocated(0) / 1e9
                        reserved = torch.cuda.memory_reserved(0) / 1e9
                        logger.error(f"[QualityEvaluator] ğŸ’¥ GPU OOM during LLM call #{self.total_api_calls} (attempt {attempt + 1})")
                        logger.error(f"[QualityEvaluator] ğŸ’¾ GPU Memory: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB")
                    last_error = f"GPU Out of Memory: {str(e)}"
                else:
                    logger.error(f"[QualityEvaluator] ğŸ’¥ RuntimeError in LLM call #{self.total_api_calls}: {e}")
                    last_error = f"RuntimeError: {str(e)}"

            except Exception as e:
                call_duration = time.time() - call_start_time
                last_error = str(e)
                logger.warning(f"[QualityEvaluator] âŒ LLM call #{self.total_api_calls} failed after {call_duration:.2f}s (attempt {attempt + 1}/{optimized_retries}): {type(e).__name__}: {e}")

            # ì¬ì‹œë„ ì „ ê³µê²©ì ì¸ ë©”ëª¨ë¦¬ ì •ë¦¬
            if attempt < optimized_retries - 1:
                logger.info(f"[QualityEvaluator] ğŸ§¹ Performing aggressive memory cleanup before retry...")

                # ê³µê²©ì ì¸ ë©”ëª¨ë¦¬ ì •ë¦¬
                import gc
                if torch.cuda.is_available():
                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (í˜¸ì¶œ ìˆœì„œ ì¤‘ìš”)
                    torch.cuda.empty_cache()      # 1. ìºì‹œ ë¹„ìš°ê¸°
                    torch.cuda.synchronize()      # 2. ëª¨ë“  GPU ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
                    gc.collect()                   # 3. Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜

                    # ì •ë¦¬ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
                    free_after_cleanup = torch.cuda.mem_get_info(0)[0] / 1e9
                    allocated_after_cleanup = torch.cuda.memory_allocated(0) / 1e9
                    logger.info(f"[QualityEvaluator] ğŸ’¾ After cleanup: Free={free_after_cleanup:.2f}GB, Allocated={allocated_after_cleanup:.2f}GB")

                # Exponential backoff: 2s, 4s, 8s... (1s â†’ 2së¡œ ì¦ê°€, ë©”ëª¨ë¦¬ í•´ì œ ì‹œê°„ í™•ë³´)
                backoff_time = 2 ** (attempt + 1)
                logger.info(f"[QualityEvaluator] â³ Retrying in {backoff_time}s (allowing memory to stabilize)...")
                await asyncio.sleep(backoff_time)

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        logger.error(f"[QualityEvaluator] ğŸ’¥ LLM call failed after {optimized_retries} attempts: {last_error}")
        raise Exception(f"EXAONE call failed after {optimized_retries} attempts: {last_error}")

    def _parse_evaluation_response(self, response: str) -> Dict[str, Any]:
        """EXAONE ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹± (í–¥ìƒëœ ì—ëŸ¬ ì²˜ë¦¬)"""
        try:
            # ğŸ”§ ê°•í™”ëœ JSON ë¸”ë¡ ì¶”ì¶œ
            json_str = self._extract_json_block(response)

            # ë¨¼ì € ì›ë³¸ JSON íŒŒì‹± ì‹œë„ (ê°€ì¥ ì¼ë°˜ì ì¸ ê²½ìš°)
            try:
                result = json.loads(json_str)
                logger.debug("[QualityEvaluator] Direct JSON parsing successful")
                return result
            except json.JSONDecodeError as parse_error:
                # ì›ë³¸ íŒŒì‹± ì‹¤íŒ¨ì‹œì—ë§Œ ì •ë¦¬ í›„ ì¬ì‹œë„
                logger.debug(f"[QualityEvaluator] Direct parsing failed: {parse_error}")
                logger.debug("[QualityEvaluator] Applying JSON sanitization")

                # ì •ë¦¬ëœ JSONìœ¼ë¡œ ì¬ì‹œë„
                sanitized_json = self._sanitize_json_string(json_str)
                return json.loads(sanitized_json)

        except json.JSONDecodeError as e:
            logger.error(f"[QualityEvaluator] JSON parsing failed: {e}")
            logger.error(f"[QualityEvaluator] Raw response: {response}")
            logger.error(f"[QualityEvaluator] Extracted JSON string: {json_str}")
            raise ValueError(f"Invalid JSON response: {e}")

    def _extract_json_block(self, response: str) -> str:
        """ì‘ë‹µì—ì„œ JSON ë¸”ë¡ì„ ì¶”ì¶œí•˜ëŠ” ê°•í™”ëœ ë¡œì§"""
        import re

        # 1. ```json ì½”ë“œ ë¸”ë¡ ìš°ì„  ì‹œë„
        json_pattern = r'```json\s*(\{[\s\S]*?\})\s*```'
        match = re.search(json_pattern, response, re.MULTILINE | re.DOTALL)
        if match:
            return match.group(1).strip()

        # 2. ```ì—†ëŠ” ì½”ë“œ ë¸”ë¡ ì‹œë„
        code_pattern = r'```\s*(\{[\s\S]*?\})\s*```'
        match = re.search(code_pattern, response, re.MULTILINE | re.DOTALL)
        if match:
            return match.group(1).strip()

        # 3. ì§ì ‘ JSON ê°ì²´ ì¶”ì¶œ (ê°€ì¥ í° ì™„ì „í•œ ê°ì²´)
        if "{" in response and "}" in response:
            brace_positions = []
            brace_count = 0
            start_pos = -1

            for i, char in enumerate(response):
                if char == '{':
                    if brace_count == 0:
                        start_pos = i
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0 and start_pos != -1:
                        # ì™„ì „í•œ JSON ê°ì²´ ë°œê²¬
                        candidate = response[start_pos:i+1]
                        brace_positions.append((len(candidate), candidate))

            # ê°€ì¥ í° JSON ê°ì²´ ë°˜í™˜
            if brace_positions:
                brace_positions.sort(key=lambda x: x[0], reverse=True)
                return brace_positions[0][1].strip()

        # 4. ì „ì²´ ì‘ë‹µì´ JSONì¸ ê²½ìš°
        response = response.strip()
        if response.startswith('{') and response.endswith('}'):
            return response

        # 5. ì‹¤íŒ¨í•œ ê²½ìš° ì „ì²´ ì‘ë‹µ ë°˜í™˜
        return response

    def _sanitize_json_string(self, json_str: str) -> str:
        """JSON ë¬¸ìì—´ì—ì„œ ìœ íš¨í•˜ì§€ ì•Šì€ escape ë¬¸ìì™€ ì œì–´ ë¬¸ìë¥¼ ì •ë¦¬"""
        import re

        # 0. JSON ì£¼ì„ ì œê±° (LLMì´ ìƒì„±í•˜ëŠ” ì£¼ì„ ë¬¸ì œ í•´ê²°)
        json_str = re.sub(r'//.*?$', '', json_str, flags=re.MULTILINE)
        json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)

        # 1. JSON ë¬¸ìì—´ ê°’ ë‚´ì˜ ì œì–´ ë¬¸ìì™€ escape ì²˜ë¦¬
        def fix_string_content(match):
            string_content = match.group(1)

            # ì œì–´ ë¬¸ìë¥¼ escape ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
            control_char_map = {
                '\n': '\\n',
                '\r': '\\r',
                '\t': '\\t',
                '\b': '\\b',
                '\f': '\\f',
            }

            # ì œì–´ ë¬¸ì ì²˜ë¦¬ (ì´ë¯¸ escapeëœ ê²ƒì€ ì œì™¸)
            result = []
            i = 0
            while i < len(string_content):
                char = string_content[i]

                # ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ escapeëœ ë¬¸ìëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                if char == '\\' and i + 1 < len(string_content):
                    next_char = string_content[i + 1]
                    if next_char in '"\\bfnrt/u':
                        result.append(char)
                        result.append(next_char)
                        i += 2
                        continue
                    else:
                        # ìœ íš¨í•˜ì§€ ì•Šì€ escapeëŠ” ë°±ìŠ¬ë˜ì‹œë¥¼ escape
                        result.append('\\\\')
                        i += 1
                        continue

                # ì œì–´ ë¬¸ìë¥¼ escape ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
                if char in control_char_map:
                    result.append(control_char_map[char])
                # ASCII ì œì–´ ë¬¸ì (0x00-0x1F, 0x7F-0x9F) ì²˜ë¦¬
                elif ord(char) < 0x20 or (0x7F <= ord(char) <= 0x9F):
                    result.append(f'\\u{ord(char):04x}')
                else:
                    result.append(char)

                i += 1

            return f'"{"".join(result)}"'

        # JSON ë¬¸ìì—´ ê°’ë“¤ì„ ì°¾ì•„ì„œ ì²˜ë¦¬ (ê°œì„ ëœ íŒ¨í„´)
        # ì´ì¤‘ ë”°ì˜´í‘œ ë‚´ì˜ ë‚´ìš©ì„ ë§¤ì¹­í•˜ë˜, escapeëœ ë”°ì˜´í‘œëŠ” ë¬´ì‹œ
        json_str = re.sub(r'"((?:[^"\\]|\\.)*)?"', fix_string_content, json_str)

        return json_str

    def _fallback_evaluation(
        self,
        query: str,
        documents: List[SearchResult]
    ) -> QueryEvaluationResult:
        """í´ë°± í‰ê°€ ë©”ì»¤ë‹ˆì¦˜ (LLM í‰ê°€ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì œê³µ)"""
        logger.warning(f"[QualityEvaluator] Using fallback evaluation for query: {query}")
        self.fallback_count += 1

        # LLM íŒë‹¨ ìœ„ì„ - ë‹¨ìˆœ ê¸°ë³¸ê°’ë§Œ ì œê³µ (ë¬¸ì„œë³„ í‰ê°€ ì œê±°)
        doc_evaluations = []  # ë¹ˆ ë¦¬ìŠ¤íŠ¸
        avg_score = 0.1  # ë‚®ì€ ê¸°ë³¸ê°’ (LLM í‰ê°€ ì‹¤íŒ¨ ì‹œ)
        max_score = 0.1
        relevant_count = 0

        return QueryEvaluationResult(
            query_id="fallback",
            query_text=query,
            is_relevant=avg_score >= self.relevance_threshold,
            relevance_score=avg_score,
            document_evaluations=doc_evaluations,  # ë¹ˆ ë¦¬ìŠ¤íŠ¸
            evaluation_time=0.1,
            confidence=0.5,
            reasoning="LLM í‰ê°€ ì‹¤íŒ¨ë¡œ í´ë°± ì‚¬ìš© - ë‚®ì€ ì‹ ë¢°ë„ ê¸°ë³¸ê°’ ì ìš©",
            total_documents=len(documents),
            relevant_documents=relevant_count,  # 0
            avg_doc_score=avg_score,  # 0.1
            max_doc_score=max_score,  # 0.1
            tokens_used=0,
            improvement_suggestions="",  # í´ë°± ì‹œ ë¹ˆ ê°’
            query_effectiveness="",  # í´ë°± ì‹œ ë¹ˆ ê°’
            fallback_used=True
        )

    async def evaluate_query_document_relevance(
        self,
        subquery: Dict[str, Any],
        documents: List[SearchResult]
    ) -> QueryEvaluationResult:
        """ì„œë¸Œì¿¼ë¦¬ì™€ ë¬¸ì„œë“¤ ê°„ì˜ ê´€ë ¨ì„± í‰ê°€ (ë©”ì¸ í•¨ìˆ˜)"""

        query_id = subquery.get("id", "unknown")
        query_text = subquery.get("question", subquery.get("text", "")).strip()

        # ë””ë²„ê¹…: í‰ê°€í•˜ëŠ” ì¿¼ë¦¬ ì •ë³´ í™•ì¸
        logger.info(f"ğŸ¯ [QualityEvaluator] Evaluating query:")
        logger.info(f"  - Query ID: {query_id}")
        logger.info(f"  - Query text (full): '{query_text}'")
        logger.info(f"  - Query length: {len(query_text)}")
        logger.info(f"  - Documents to evaluate: {len(documents)}")

        if not query_text:
            return QueryEvaluationResult(
                query_id=query_id,
                query_text=query_text,
                is_relevant=False,
                relevance_score=0.0,
                document_evaluations=[],
                evaluation_time=0.0,
                confidence=0.0,
                reasoning="ë¹ˆ ì¿¼ë¦¬",
                total_documents=0,
                relevant_documents=0,
                avg_doc_score=0.0,
                max_doc_score=0.0,
                tokens_used=0,
                improvement_suggestions="",
                query_effectiveness="",
                error="Empty query text"
            )

        if not documents:
            return QueryEvaluationResult(
                query_id=query_id,
                query_text=query_text,
                is_relevant=False,
                relevance_score=0.0,
                document_evaluations=[],
                evaluation_time=0.0,
                confidence=0.0,
                reasoning="ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ",
                total_documents=0,
                relevant_documents=0,
                avg_doc_score=0.0,
                max_doc_score=0.0,
                tokens_used=0,
                improvement_suggestions="",
                query_effectiveness="",
                error="No documents provided"
            )

        start_time = time.time()

        # ìºì‹œ í™•ì¸
        doc_hashes = [self._get_document_hash(doc) for doc in documents]
        cache_key = self._generate_cache_key(query_text, doc_hashes)

        if cache_key in self.evaluation_cache:
            cached_result = self.evaluation_cache[cache_key]
            cached_result.query_id = query_id  # ID ì—…ë°ì´íŠ¸
            self.cache_hits += 1
            logger.debug(f"[QualityEvaluator] Cache hit for {query_id}")
            return cached_result

        # ë¬¸ì„œ ìˆ˜ ì œí•œ
        docs_to_evaluate = documents[:self.max_docs_per_query]

        # ğŸ”„ Retry up to 3 times for LLM call + JSON parsing
        max_attempts = 3
        evaluation_data = None
        last_error = None

        for attempt in range(1, max_attempts + 1):
            try:
                logger.info(f"[QualityEvaluator] Attempt {attempt}/{max_attempts}: Evaluating {query_id}")

                # EXAONE í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompt = EvaluationPrompts.get_relevance_evaluation_prompt(
                    query_text, docs_to_evaluate, self.max_docs_per_query
                )

                # EXAONE í˜¸ì¶œ
                response = await self._call_exaone_with_retry(prompt)

                # ì‘ë‹µ íŒŒì‹±
                evaluation_data = self._parse_evaluation_response(response)

                # JSON íŒŒì‹± ì„±ê³µ
                logger.info(f"[QualityEvaluator] Attempt {attempt}/{max_attempts}: JSON parsing successful âœ“")
                break  # Success - exit retry loop

            except Exception as e:
                last_error = str(e)
                logger.error(f"[QualityEvaluator] Attempt {attempt}/{max_attempts}: Failed - {e}")
                if attempt < max_attempts:
                    logger.info(f"[QualityEvaluator] Retrying evaluation for {query_id}...")
                    continue

        # Process result if successful
        if evaluation_data:
            try:
                logger.info(f"[QualityEvaluator] âœ“ Successfully parsed JSON after {attempt} attempt(s)")

                # ê²°ê³¼ êµ¬ì¡°í™”
                result = self._structure_evaluation_result(
                    query_id, query_text, docs_to_evaluate, evaluation_data, start_time
                )

                # ìºì‹œ ì €ì¥
                if len(self.evaluation_cache) < 100:  # ìºì‹œ í¬ê¸° ì œí•œ
                    self.evaluation_cache[cache_key] = result

                logger.info(f"[QualityEvaluator] Completed evaluation for {query_id}: "
                           f"relevant={result.is_relevant}, score={result.relevance_score:.3f}")

                return result

            except Exception as e:
                last_error = f"Result structuring failed: {str(e)}"
                logger.error(f"[QualityEvaluator] {last_error}")

        # All attempts failed
        logger.error(f"[QualityEvaluator] âœ— Failed to evaluate {query_id} after {max_attempts} attempts: {last_error}")

        try:
            # í´ë°± í‰ê°€ ì‹œë„
            fallback_result = self._fallback_evaluation(query_text, docs_to_evaluate)
            fallback_result.query_id = query_id
            fallback_result.evaluation_time = time.time() - start_time
            fallback_result.error = f"EXAONE evaluation failed: {last_error}"
            return fallback_result

        except Exception as fallback_error:
            logger.error(f"[QualityEvaluator] Fallback evaluation also failed: {fallback_error}")

            return QueryEvaluationResult(
                query_id=query_id,
                query_text=query_text,
                is_relevant=False,
                relevance_score=0.0,
                document_evaluations=[],
                evaluation_time=time.time() - start_time,
                confidence=0.0,
                reasoning="í‰ê°€ ì‹¤íŒ¨",
                total_documents=len(docs_to_evaluate),
                relevant_documents=0,
                avg_doc_score=0.0,
                max_doc_score=0.0,
                tokens_used=self.total_tokens_used,
                improvement_suggestions="",
                query_effectiveness="",
                error=f"Both EXAONE and fallback evaluation failed: {last_error}"
                )

    def _structure_evaluation_result(
        self,
        query_id: str,
        query_text: str,
        documents: List[SearchResult],
        evaluation_data: Dict[str, Any],
        start_time: float
    ) -> QueryEvaluationResult:
        """EXAONE ì‘ë‹µì„ QueryEvaluationResultë¡œ êµ¬ì¡°í™” (ë¬¸ì„œë³„ í‰ê°€ ì œê±°)"""

        overall = evaluation_data.get("overall_assessment", {})

        # ë¬¸ì„œë³„ í‰ê°€ëŠ” ë” ì´ìƒ ìƒì„±í•˜ì§€ ì•ŠìŒ (overallë§Œ ì‚¬ìš©)
        doc_evaluations = []
        avg_doc_score = 0.0
        max_doc_score = 0.0
        relevant_count = 0

        evaluation_time = time.time() - start_time

        # Threshold-based relevance decision (override EXAONE's decision)
        relevance_score = overall.get("relevance_score", 0.0)
        is_relevant_by_threshold = relevance_score >= self.relevance_threshold

        return QueryEvaluationResult(
            query_id=query_id,
            query_text=query_text,
            is_relevant=is_relevant_by_threshold,  # Use threshold-based decision
            relevance_score=relevance_score,
            document_evaluations=doc_evaluations,  # ë¹ˆ ë¦¬ìŠ¤íŠ¸
            evaluation_time=evaluation_time,
            confidence=overall.get("confidence", 0.5),
            reasoning=overall.get("reasoning", ""),
            total_documents=len(documents),
            relevant_documents=relevant_count,  # 0
            avg_doc_score=avg_doc_score,  # 0.0
            max_doc_score=max_doc_score,  # 0.0
            tokens_used=self.total_tokens_used,
            improvement_suggestions=overall.get("improvement_suggestions", ""),  # replanner ê°œì„  ì œì•ˆ
            query_effectiveness=overall.get("query_effectiveness", "")  # ì¿¼ë¦¬ íš¨ê³¼ì„± í‰ê°€
        )

    async def evaluate_batch_queries(
        self,
        subqueries_with_docs: List[Tuple[Dict[str, Any], List[SearchResult]]],
        max_concurrent: int = 5  # 2 â†’ 5ë¡œ ì¦ê°€ (API ëª¨ë¸ìš©)
    ) -> Tuple[Dict[str, QueryEvaluationResult], EvaluationStats]:
        """ë°°ì¹˜ ì¿¼ë¦¬ í‰ê°€ (ë³‘ë ¬ ì²˜ë¦¬ - asyncio.gather ì‚¬ìš©)

        Args:
            subqueries_with_docs: í‰ê°€í•  (ì„œë¸Œì¿¼ë¦¬, ë¬¸ì„œë“¤) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            max_concurrent: ë™ì‹œ ì‹¤í–‰ ì œí•œ (ê¸°ë³¸: 5ê°œ)
                           - API ëª¨ë¸ (Gemini/GPT/Llama): 5ê°œ ë³‘ë ¬ (ë¹ ë¦„)
                           - ë¡œì»¬ ëª¨ë¸ (EXAONE): 1ê°œ ìˆœì°¨ (ë©”ëª¨ë¦¬ ì•ˆì „)
        """

        # ë³‘ë ¬ ì‹¤í–‰ ì „ ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬ (ì•ˆì „ì„± í™•ë³´)
        if torch.cuda.is_available():
            import gc
            torch.cuda.synchronize()
            gc.collect()
            torch.cuda.empty_cache()
            logger.info("[QualityEvaluator] Pre-parallel memory cleanup completed")

        # ë©”ëª¨ë¦¬ ì•ˆì „ì„± ì²´í¬ ë° ë™ì  max_concurrent ì¡°ì •
        free_memory_gb = 0
        if torch.cuda.is_available():
            free_memory_gb = torch.cuda.mem_get_info(0)[0] / 1e9

        # LLM providerì— ë”°ë¼ ë³‘ë ¬ë„ ì¡°ì •
        if self.chatbot.provider.value in ["gemini", "gpt", "llama"]:
            # API ê¸°ë°˜ ëª¨ë¸: ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™” (GPU ë©”ëª¨ë¦¬ ë¶ˆí•„ìš”)
            max_concurrent = min(max_concurrent, 5)  # ìµœëŒ€ 5ê°œê¹Œì§€ ë³‘ë ¬
            logger.info(f"[QualityEvaluator] Parallel execution mode for API model: max_concurrent={max_concurrent}")
        else:
            # ë¡œì»¬ ëª¨ë¸ (EXAONE): ë©”ëª¨ë¦¬ ì•ˆì „ì„ ìœ„í•´ ìˆœì°¨ ì‹¤í–‰
            max_concurrent = 1
            logger.info(f"[QualityEvaluator] Sequential execution mode for local model (max_concurrent=1)")

        logger.info(f"[QualityEvaluator] Starting PARALLEL evaluation of {len(subqueries_with_docs)} queries (max_concurrent={max_concurrent}, free_memory={free_memory_gb:.1f}GB)")
        start_time = time.time()

        results = {}
        successful_count = 0
        failed_count = 0
        total_docs = 0
        total_relevant_docs = 0
        all_relevance_scores = []
        all_confidences = []
        all_eval_times = []

        # ë³‘ë ¬ í‰ê°€ ì‹¤í–‰ (ì²­í¬ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ë©”ëª¨ë¦¬ ì•ˆì „ì„± í™•ë³´)
        total_queries = len(subqueries_with_docs)

        for chunk_idx in range(0, total_queries, max_concurrent):
            # í˜„ì¬ ì²­í¬ ì¶”ì¶œ
            chunk = subqueries_with_docs[chunk_idx:chunk_idx + max_concurrent]
            chunk_size = len(chunk)

            logger.info(f"[QualityEvaluator] Processing chunk {chunk_idx//max_concurrent + 1} "
                       f"with {chunk_size} queries in parallel")

            # ë³‘ë ¬ ì‹¤í–‰í•  íƒœìŠ¤í¬ ìƒì„±
            tasks = []
            for subquery, documents in chunk:
                task = self.evaluate_query_document_relevance(subquery, documents)
                tasks.append(task)

            # ë³‘ë ¬ ì‹¤í–‰ (ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ë‹¤ë¥¸ íƒœìŠ¤í¬ëŠ” ê³„ì† ì‹¤í–‰)
            chunk_results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ ì§‘ê³„
            for idx, result in enumerate(chunk_results):
                query_id = chunk[idx][0].get("id", f"query_{chunk_idx + idx}")

                # ì—ëŸ¬ ì²´í¬
                if isinstance(result, Exception):
                    logger.error(f"[QualityEvaluator] Parallel evaluation failed for {query_id}: {result}")
                    failed_count += 1
                    continue

                # ì •ìƒ ê²°ê³¼ ì²˜ë¦¬
                if result.error:
                    failed_count += 1
                else:
                    successful_count += 1

                results[result.query_id] = result
                total_docs += result.total_documents
                total_relevant_docs += result.relevant_documents
                all_relevance_scores.append(result.relevance_score)
                all_confidences.append(result.confidence)
                all_eval_times.append(result.evaluation_time)

            # ê° ì²­í¬ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ë‹¤ìŒ ì²­í¬ë¥¼ ìœ„í•œ ê³µê°„ í™•ë³´)
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.debug(f"[QualityEvaluator] Memory cleanup after chunk {chunk_idx//max_concurrent + 1}")

        total_time = time.time() - start_time

        # í†µê³„ ê³„ì‚°
        stats = EvaluationStats(
            total_queries=len(subqueries_with_docs),
            successful_evaluations=successful_count,
            failed_evaluations=failed_count,
            total_documents_evaluated=total_docs,
            total_relevant_documents=total_relevant_docs,
            avg_relevance_score=sum(all_relevance_scores) / len(all_relevance_scores) if all_relevance_scores else 0.0,
            avg_confidence=sum(all_confidences) / len(all_confidences) if all_confidences else 0.0,
            avg_evaluation_time=sum(all_eval_times) / len(all_eval_times) if all_eval_times else 0.0,
            total_tokens_used=self.total_tokens_used,
            api_calls_made=self.total_api_calls,
            cache_hits=self.cache_hits,
            fallback_count=self.fallback_count,
            evaluation_time=total_time,
            timestamp=datetime.now(timezone.utc).isoformat()
        )

        logger.info(f"[QualityEvaluator] Batch evaluation completed in {total_time:.2f}s")
        logger.info(f"[QualityEvaluator] Success: {successful_count}/{len(subqueries_with_docs)}, "
                   f"Tokens: {self.total_tokens_used}, Cache hits: {self.cache_hits}")

        return results, stats

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Module Exports
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

__all__ = [
    "DocumentEvaluation",
    "QueryEvaluationResult",
    "EvaluationStats",
    "EvaluationPrompts",
    "QualityEvaluator"
]